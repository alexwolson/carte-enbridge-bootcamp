{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 2-2\n",
    "\n",
    "Transforming customer service using AI\n",
    "\n",
    "In this lab, we will use HuggingFace's transformers library to build a customer service chatbot. We will use the DistilBERT model to build a chatbot that can answer questions about the customer service policy of a company.\n",
    "\n",
    "For this notebook to run in a reasonable time, it's essential that you enable GPU acceleration. To do this, go to Runtime > Change runtime type, and select GPU as the hardware accelerator."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a263f540c83733e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if GPU is enabled\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f'Running on a Mac with Metal Performance Shaders (MPS).')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead (NOT RECOMMENDED!)')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d53f3d3fe1899ab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we will import the necessary libraries and modules. If we are running this notebook on Google Colab, we will install the libraries that we need in the cell below. We are installing directly from GitHub, to ensure that we have the latest version of each library."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "799bee18ea91799"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if we are running on Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q -U transformers datasets evaluate accelerate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9b269ffe3096c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eca575341db8c32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset\n",
    "\n",
    "Using the load_dataset() function from the datasets library, we will load the customer service dataset. We will be using a dataset of customer service queries that is designed for use in training chatbots.\n",
    "\n",
    "The parts of the dataset that we are interested in are the customer's query and the 'intent' of the customer's query. The intent is the purpose of the customer's query. For example, if the customer's query is \"What is your return policy?\", the intent is \"return policy\". This makes it easier for the interaction to be routed to the correct department, or for collecting metrics on the types of queries that customers have."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a026f2ecffba3aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = load_dataset('bitext/Bitext-customer-support-llm-chatbot-training-dataset')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fc4fa817a6af62e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a quick look at a few random samples from the dataset. We will print the customer's query, the intent, and the response from the chatbot."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eec5a1084d16030d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from random import choice\n",
    "for i in range(5):\n",
    "    sample = choice(data['train'])\n",
    "    print(f'Question: {sample[\"instruction\"]}')\n",
    "    print(f'Customer Intent: {sample[\"intent\"]}')\n",
    "    print('---'*10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30315fe1fcf64372"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the data\n",
    "\n",
    "Now that we have our data downloaded, the next step is to preprocess it. For any model, each word in the model's vocabulary is assigned a unique number. The model doesn't see the actual words in the text, it only sees the numbers. So, we need to convert the text into numbers. This is called tokenization. Because we are fine-tuning an existing model, we can simply load the tokenizer that was created for that model. The tokenizer will convert the text into numbers, and also add any special tokens that the model needs. For example, the DistilBERT model needs a [CLS] token at the beginning of the text, and a [SEP] token at the end of the text. The tokenizer will add these tokens for us.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab9a58471fe2e439"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8edbee2aa0175ec2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what happens when we tokenize a sample text:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6332280f50f10d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"What is your return policy?\"\n",
    "tokens = tokenizer.encode(question)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd3bccf8d9052a7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each of these values corresponds to one of the words in the text. We can use the tokenizer to convert these numbers back into words, and see what the tokenizer did to the text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5f1291aed79b8fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the tokens back into words\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb272d564345f72a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the tokenizer converted the text into numbers, and added the [CLS] and [SEP] tokens.\n",
    "\n",
    "**Your Turn**\n",
    "\n",
    "Try tokenizing a few other sample texts. \n",
    "- What happens if you put in a nonsense word? \n",
    "- What happens if you put in a text that's longer than the maximum length of the model (512 tokens)? \n",
    "- What happens if you put in an emoji, or another character that's not in the model's vocabulary?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1247b804d6b50246"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_and_decode(question):\n",
    "    tokens = tokenizer.encode(question)\n",
    "    recons = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    for token, id in zip(recons, tokens):\n",
    "        print(f'{token:<12} {id}')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2fafa15026700d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encode_and_decode(\"What is your return policy?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51f38d902686249b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are ready to tokenize our dataset. We will use the map() function to apply the tokenizer to each sample in the dataset. We will also set the maximum length of the input to 512 tokens.\n",
    "\n",
    "There are a couple of parameters that we need to set. The first is padding. Although our input sentences can be any length, the model requires input to be a consistent size. The tokenizer can 'pad' the input by adding special tokens to the end of the input. The model will ignore these tokens, but they will allow the input to be the same size. \n",
    "\n",
    "The second parameter is truncation. If the input is longer than the maximum length, the tokenizer will truncate the input. We will set both of these parameters to True.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34637a28c3dba908"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['instruction'], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = data.map(tokenize, batched=True, batch_size=len(data), remove_columns=data['train'].column_names) # We don't want to keep the original text around, so we remove it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd99d9c29962f09a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The intents that we are trying to predict are text labels, like `payment_issue`, but the model is going to predict numbers. We can assign a number to each label, and then create a mapping from the label to the number. We will use this mapping to convert the labels into numbers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bd96395eb5585f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intents = set(data['train']['intent']) # Get all of the unique intents\n",
    "intents = list(intents) # Convert to a list, so that the order is consistent\n",
    "intents.sort() # Sort alphabetically\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(intents)} # Create a mapping from the index to the label\n",
    "label2id = {label: i for i, label in enumerate(intents)} # Create a mapping from the label to the index\n",
    "\n",
    "print(id2label)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82707fc897b5849e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ids = [label2id[label] for label in data['train']['intent']] # Get a list of all the labels as numbers\n",
    "tokenized_dataset = tokenized_dataset['train'].add_column('label', ids) # Add the labels to the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b85d899118642d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are almost finished preparing our data! The last thing we need to do is split the dataset into a training set and a test set. We will use 80% of the data for training, and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe21feb90cdd787a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e25ecd3ff67946"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can confirm that the data looks how we expect by printing out the object:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc95dbeb3941d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5733285f6b13ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also load a \"data collator\". This is a function that combines the data into batches, to make it easier for the model to process. We will use the default data collator, which simply combines the data into batches, and adds padding to the end of the input."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8bf8bdbfcb6a0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5808c135a2a7fde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Now that we have our data ready, we can fine-tune the model. As with the earlier stages, HuggingFace makes it very easy to load an existing model. Because the task we are trying to solve is text classification, we will use the AutoModelForSequenceClassification class. This class will load a model that has already been trained on a text classification task, and fine-tune it on our dataset. We will use the DistilBERT model, which is a smaller version of the BERT model. This will allow us to train the model faster, and use less memory.\n",
    "\n",
    "There are a few arguments that we have to pass:\n",
    "- The model name. We will use the DistilBERT model, which is a smaller version of the BERT model.\n",
    "- The number of labels (or possible intents) in our dataset. This is the number of outputs that the model will have. We can get this number from the length of the intents list.\n",
    "- The mapping from the label to the index. This is the mapping that we created earlier, which converts the label to a number.\n",
    "- The mapping from the index to the label. This is the mapping that we created earlier, which converts the number back into a label.\n",
    "\n",
    "You might get a warning that some of the weights are not initialized. This is fine, because we are fine-tuning the model, so these weights will be updated during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6af02a4919ed78a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(intents), id2label=id2label, label2id=label2id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3af316b2f9cebff7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define the arguments for training the model. Each argument is explained in the code below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05fc91680f0d6ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,                # How often to log results\n",
    "    evaluation_strategy='epoch',     # How often to evaluate the model - once per epoch\n",
    "    save_strategy='epoch',           # How often to save the model - once per epoch\n",
    "    load_best_model_at_end=True,     # At the end of training, load the model from the epoch with the best validation loss\n",
    ")\n",
    "\n",
    "# Move everything to the GPU/MPS, if available\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                                 # the model to be trained\n",
    "    args=training_args,                          # training arguments, defined above\n",
    "    train_dataset=tokenized_dataset['train'],    # training dataset\n",
    "    eval_dataset=tokenized_dataset['test'],      # evaluation dataset\n",
    "    data_collator=data_collator,                 # data collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cafa23576c7c41d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can train the model! This will take around 5 minutes on Google Colab."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bf2a97319d8fd5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b19d0afbd712ab50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# You can save your model to disk\n",
    "trainer.save_model(\"./models\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b2d91d6128f475c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ...and reload it at any time in the future!\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./models\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d4316d873b5ca35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266b5e81ea7f302"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's try it out!\n",
    "classifier(\"What is your return policy?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8442a20ac64286bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your Turn**\n",
    "\n",
    "Try asking a few questions to the chatbot. \n",
    "\n",
    "- What happens if you ask a question that is not in the dataset? \n",
    "- What happens if you ask a question that is in the dataset, but is phrased differently?\n",
    "- What happens if you ask a question that has a typo? Or a word that is not in the model's vocabulary?\n",
    "- What happens if you ask a question that is not in English?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc88a9c65b9e53ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier(\"What is your return policy?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fc4f0ffd00d0602"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now that we have trained our model, we can evaluate it. We will get the predictions for the test set, and calculate the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab5c7b9635f80526"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = trainer.predict(tokenized_dataset['test'])\n",
    "predictions = predictions.predictions.argmax(-1)\n",
    "labels = tokenized_dataset['test']['label']\n",
    "\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39e47f9e3aff3f94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow! As we can see, the model is very accurate. This is because we are using a model that has already been trained on a similar task. If we were training a model from scratch, we would expect the accuracy to be much lower."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9b68221972a4af0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your Turn**\n",
    "\n",
    "When evaluation performance of a new model, it's often helpful to look at the examples that the model gets wrong. This can help you understand what types of errors the model is making, and how you might improve the model.\n",
    "\n",
    "In the cell below, print out the examples that the model gets wrong. You can use the `predictions` and `labels` variables from the previous cell.\n",
    "\n",
    "- What types of errors is the model making?\n",
    "- Are there any patterns in the errors that the model is making?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18ee060d54796ecb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "15b64cb32b6ad555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bonus Task**\n",
    "\n",
    "Try fine-tuning a different model, such as [BERT](https://huggingface.co/transformers/model_doc/bert.html), [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html), or [XLNet](https://huggingface.co/transformers/model_doc/xlnet.html). How does the accuracy change? How does the training time change? __Hint: You can change the model name in the cell where we load the model.__"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcd13d37c88c12fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cefc7f3855a66851"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
