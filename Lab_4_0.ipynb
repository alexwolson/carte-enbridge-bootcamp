{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "## AI in Market Strategy\n",
    "\n",
    "We are starting off today a little differently! Because the value of AI in Market Strategy is centred around specific applications, we are going to work through three different case studies. Each case study will focus on both a different domain and a different technology. By the end, we will have a strong understanding of the growing role of AI in Market Strategy!\n",
    "\n",
    "## Case Study 1: Predictive Analytics\n",
    "\n",
    "To begin with, we will be looking at a dataset of avocado prices and demand over a three-year period. Grocery stores need to understand trends in demand and pricing for avocados, to ensure they have enough stock and to ensure they are pricing their avocados competitively. We will be using a toolkit from Meta (aka Facebook) called [Prophet](https://facebook.github.io/prophet/). Prophet is a forecasting tool that is designed to be easy to use, and to produce forecasts that are both accurate and explainable.\n",
    "\n",
    "Load the dataset in the cell below. Because we are using time-series data, we instruct Pandas to `parse` the dates in the dataset. This allows us to do things like compute the time between two dates, or to group data by year, month, or day. We specify the format to be `YYYY-MM-DD`, which is represented by `%Y-%m-%d`."
   ],
   "metadata": {
    "collapsed": false,
    "id": "74fa8938f2825d1c"
   },
   "id": "74fa8938f2825d1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/alexwolson/carte_workshop_datasets/raw/main/avocado.csv.zip\", compression=\"zip\", index_col=0)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "df.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head() # 4046, 4225, 4770 are the PLU codes for different types of avocados"
   ],
   "metadata": {
    "id": "eddc8457290d5f10",
    "outputId": "b41c75e3-2620-4e83-e789-c717676a8670",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    }
   },
   "id": "eddc8457290d5f10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As ever, we will start by exploring the data. Let's plot the average price of avocados over time. We can use the `resample` method to group the data by month, and then take the average of each group. We can then plot the result using the `plot` method."
   ],
   "metadata": {
    "collapsed": false,
    "id": "1e31084cb5129f34"
   },
   "id": "1e31084cb5129f34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.resample(\"M\")[\"AveragePrice\"].mean().plot(figsize=(15,7))\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.title(\"Average Price of Avocados\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "7bb9307aeaf8e33a",
    "outputId": "0e652c3d-42d4-4615-9d25-e52db5524f85",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    }
   },
   "id": "7bb9307aeaf8e33a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also look at the volume of each PLU code sold over time:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d5a15bf8bcb13653"
   },
   "id": "d5a15bf8bcb13653"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.resample(\"M\")[[\"4046\", \"4225\", \"4770\"]].sum().plot(figsize=(15,7))\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.title(\"Volume of Avocados Sold\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "9dc2d373c51da1d1",
    "outputId": "d7191208-e0c1-4aae-c1b9-d7612d6520ab",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    }
   },
   "id": "9dc2d373c51da1d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's move to building a predictive model. We will use Prophet to predict the average volume of avocados sold. Prophet is designed to be easy to use, and to produce forecasts that are both accurate and explainable. We will start by creating a new DataFrame with the columns that Prophet expects: `ds` for the date, and `y` for the value we want to predict. Since we want to be able to evaluate the quality of our predictions, we will separate out the last 6 months of data as a test set."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ce8cc8c3a26c7e09"
   },
   "id": "ce8cc8c3a26c7e09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prophet_df = df[[\"Total Volume\"]].resample(\"W\").sum().reset_index() # Aggregate to the week level\n",
    "prophet_df.columns = [\"ds\", \"y\"]\n",
    "prophet_df_train = prophet_df[:-26] # All but the last six months\n",
    "prophet_df_test = prophet_df[-26:]"
   ],
   "metadata": {
    "id": "ef869ba7216eb63b"
   },
   "id": "ef869ba7216eb63b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create a Prophet model and fit it to our training data. Prophet supports automatically considering holidays, but we don't expect holidays to have a large impact on avocado sales, so we won't take advantage of this. In other contexts, considering things like weather, 'shocks' (e.g. a pandemic), or other events can be very important."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e84a157a0f88a3a0"
   },
   "id": "e84a157a0f88a3a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U -q prophet plotly fastapi kaleido python-multipart uvicorn \"typing-extensions<4.6.0\""
   ],
   "metadata": {
    "id": "764a60b09c3caf92",
    "outputId": "25b46446-bac1-4429-c1e4-f7d3ff2acf19",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "764a60b09c3caf92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from time import time\n",
    "\n",
    "model = Prophet(interval_width=1)\n",
    "start_time = time()\n",
    "model.fit(prophet_df_train)\n",
    "print(f'Training time: {time() - start_time} seconds')"
   ],
   "metadata": {
    "id": "4d7cb0017fe0f6f3",
    "outputId": "690b7f80-9acb-40ab-f519-2bf93841949c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "4d7cb0017fe0f6f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.predict(prophet_df_test)\n",
    "# Calculate percentage of true values that fall between yhat_lower and yhat_upper\n",
    "correct = []\n",
    "for i in range(len(predictions)):\n",
    "    if (prophet_df_test[\"y\"].iloc[i] >= predictions[\"yhat_lower\"].iloc[i]) and (prophet_df_test[\"y\"].iloc[i] <= predictions[\"yhat_upper\"].iloc[i]):\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "print(f\"Percentage of true values that fall between yhat_lower and yhat_upper: {sum(correct)/len(correct) * 100:.2f}%\")"
   ],
   "metadata": {
    "id": "ca7ff8c1fb777043",
    "outputId": "2a86f26b-a5c0-4cc8-b964-aa6f2281344a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "ca7ff8c1fb777043"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that our model is able to predict the volume of avocados sold with a reasonable degree of accuracy. We can visualize the predictions using the `plot` method. The black dots represent the actual values, and the blue line represents the predictions. The shaded blue area represents the uncertainty in the predictions."
   ],
   "metadata": {
    "collapsed": false,
    "id": "501a882cfc8745ee"
   },
   "id": "501a882cfc8745ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "fig = plot_plotly(model, model.predict(prophet_df_test), figsize=(1300,600))\n",
    "fig.show()"
   ],
   "metadata": {
    "id": "c8841782d8e9cb0e",
    "outputId": "5bad6977-4668-4d91-f141-a2da87f16179",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    }
   },
   "id": "c8841782d8e9cb0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also break down the predictions into their components. The first plot shows the overall trend in avocado sales, and the second plot shows the weekly seasonality."
   ],
   "metadata": {
    "collapsed": false,
    "id": "cac01797e6fc014b"
   },
   "id": "cac01797e6fc014b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plot_components_plotly(model, model.predict(prophet_df), figsize=(1300,400))\n",
    "fig.show()"
   ],
   "metadata": {
    "id": "1b8802d4069eb196",
    "outputId": "c21ae46a-5645-449c-e3d6-1db18ae4356b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    }
   },
   "id": "1b8802d4069eb196"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your turn**\n",
    "\n",
    "The avocado dataset breaks down the data by organic and conventional avocados. Using the separated datasets below, fit two distinct models to predict the volume of organic and conventional avocados sold. How do the predictions compare? What are the main differences between the two models?"
   ],
   "metadata": {
    "collapsed": false,
    "id": "2ccc243299a7eeba"
   },
   "id": "2ccc243299a7eeba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prophet_df_conventional = df[df[\"type\"] == \"conventional\"][[\"Total Volume\"]].resample(\"W\").sum().reset_index()\n",
    "prophet_df_conventional.columns = [\"ds\", \"y\"]\n",
    "prophet_df_conventional_train = prophet_df_conventional[:-26]\n",
    "prophet_df_conventional_test = prophet_df_conventional[-26:]\n",
    "\n",
    "prophet_df_organic = df[df[\"type\"] == \"organic\"][[\"Total Volume\"]].resample(\"W\").sum().reset_index()\n",
    "prophet_df_organic.columns = [\"ds\", \"y\"]\n",
    "prophet_df_organic_train = prophet_df_organic[:-26]\n",
    "prophet_df_organic_test = prophet_df_organic[-26:]"
   ],
   "metadata": {
    "id": "8da635f4743ace29"
   },
   "id": "8da635f4743ace29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your Code Here"
   ],
   "metadata": {
    "id": "c047af87342f1246"
   },
   "id": "c047af87342f1246"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case Study 2: Natural Language Processing\n",
    "\n",
    "In this case study, we will be looking at a dataset of natural-text reviews of wine. We will be using a toolkit called [spaCy](https://spacy.io/). spaCy is a Python library for Natural Language Processing (NLP) that is designed to be fast and production-ready. spaCy is a very powerful toolkit, and we will only be scratching the surface of what it can do today.\n",
    "\n",
    "Load the dataset in the cell below. We will be using the `description` column, which contains the text of the review, and the `points` column, which contains the score given to the wine by the reviewer."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e22aea38879af568"
   },
   "id": "e22aea38879af568"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/alexwolson/carte_workshop_datasets/raw/main/winemag-data-130k-v2.csv.zip\", compression=\"zip\", index_col=0).sample(frac=0.5)"
   ],
   "metadata": {
    "id": "7a15096b5efd6648"
   },
   "id": "7a15096b5efd6648"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "id": "ba19f732fb354c40",
    "outputId": "81f87838-40f8-498c-f772-6f4b9a15c650",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    }
   },
   "id": "ba19f732fb354c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U -q \"spacy<3.7.0,>=3.6.0\""
   ],
   "metadata": {
    "id": "42c23854af587806"
   },
   "id": "42c23854af587806"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With spaCy, we can use a number of different language models made available for 73 different languages. To make sure that our code runs quickly, we will download the smallest English model, `en_core_web_sm`."
   ],
   "metadata": {
    "id": "7nzaf6QpZqFq"
   },
   "id": "7nzaf6QpZqFq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm -q"
   ],
   "metadata": {
    "id": "d82e1fe84da65e95",
    "outputId": "2f9afb3f-2c32-450b-df8a-019804b66962",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d82e1fe84da65e95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "id": "b4d4660b02294ac"
   },
   "id": "b4d4660b02294ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step in any NLP task is to tokenize the text. Tokenization is the process of breaking up a string into a list of words. When we looked at encoding on Tuesday, the HuggingFace library handled this for us, but spaCy leave us to decide how we want to accomplish this. spaCy provides a `tokenizer` object that we can use to tokenize a string. We can then iterate over the tokens to get the individual words. spaCy also provides a `lemmatizer` object that we can use to get the root form of each word. This is useful because it allows us to group together words that have the same meaning, but different forms (e.g. \"run\", \"runs\", \"running\")."
   ],
   "metadata": {
    "collapsed": false,
    "id": "fc28ee62ff4427d6"
   },
   "id": "fc28ee62ff4427d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemmas = []\n",
    "first_doc = nlp(df[\"description\"].iloc[0].lower())\n",
    "print(f'word       root       part       stop')\n",
    "print(f'--------------------------------------')\n",
    "for token in first_doc:\n",
    "    tokens.append(token.text)\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        lemmas.append(token.lemma_)\n",
    "    if token.text != token.lemma_:\n",
    "      print(f'{token.text:10} {token.lemma_:10} {token.pos_:10} {token.is_stop if token.is_stop else \"\"}')\n",
    "    else:\n",
    "      print(f'{token.text:10}            {token.pos_:10} {token.is_stop if token.is_stop else \"\"}')"
   ],
   "metadata": {
    "id": "3b7dcca70f0dcab8",
    "outputId": "6186173a-0e04-4549-e800-d3e415fc0d8d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "3b7dcca70f0dcab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will apply the process to the entire dataset, using the `nlp.pipe` method. This method allows us to efficiently process a large number of documents. We will also remove stop words, which are words that are very common and don't add much meaning to the text (e.g. \"the\", \"and\", \"a\").\n",
    "\n",
    "To speed up the process, we will disable the `parser` and `ner` components of the spaCy pipeline. The `parser` component is used to determine the syntactic structure of the text, and the `ner` component is used to identify named entities (e.g. people, places, organizations). Since we are only interested in the tokens, we can disable these components to speed up the process. We are also using the smallest spaCy model, which is faster but less accurate than the larger models. In a production setting, we would likely use a larger model, and a GPU to speed up the process."
   ],
   "metadata": {
    "collapsed": false,
    "id": "d236b3ae5d2993ed"
   },
   "id": "d236b3ae5d2993ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for doc in tqdm(nlp.pipe(df[\"description\"].str.lower(), disable=[\"parser\", \"ner\"]), total=len(df)):\n",
    "    tokens.append(\" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct]))"
   ],
   "metadata": {
    "id": "a88411b112bc21c7",
    "outputId": "200eadc1-b35c-45a2-e5ee-7f64046f9075",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "a88411b112bc21c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have tokenized the text, we can use it to build a predictive model. We will use the `tokens` column as our input, and the `points` column as our output. We will use a `CountVectorizer` to convert the tokens into a vector of counts. We will then use a `LinearRegression` model to predict the score given to the wine by the reviewer."
   ],
   "metadata": {
    "collapsed": false,
    "id": "693181070fcbb1ed"
   },
   "id": "693181070fcbb1ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(min_df=0.01)), # Only include words that appear in at least 1% of reviews\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tokens, df[\"points\"], test_size=0.2, random_state=42)\n",
    "\n",
    "start_time = time()\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Training time: {time() - start_time} seconds')"
   ],
   "metadata": {
    "id": "fda5d444e1692ff8",
    "outputId": "1c3e384e-dc60-470a-814f-be4a7c61f575",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "fda5d444e1692ff8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'MAE: {mean_absolute_error(y_test, model.predict(x_test)):.2f}')"
   ],
   "metadata": {
    "id": "ee047206a50f7e17",
    "outputId": "d81314eb-71a8-4bed-f041-9970e4725203",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "ee047206a50f7e17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a very strong result! We are able to predict the score given to a wine by the reviewer with a mean absolute error of 1.63 points out of 100. Let's look at the words that are most associated with high and low scores. We can do this by looking at the coefficients of the `LinearRegression` model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2f60ea2c903fc7e3"
   },
   "id": "2f60ea2c903fc7e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get words most associated with high scores\n",
    "words = model.named_steps[\"vectorizer\"].get_feature_names_out()\n",
    "coefficients = model.named_steps[\"regressor\"].coef_\n",
    "word_scores = pd.DataFrame({\"word\": words, \"score\": coefficients})\n",
    "word_scores.sort_values(\"score\", ascending=False).head(10)"
   ],
   "metadata": {
    "id": "9441944c54ac61e",
    "outputId": "c2ccd699-5b88-45a1-ea9a-64250478d5c3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    }
   },
   "id": "9441944c54ac61e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get words most associated with low scores\n",
    "word_scores.sort_values(\"score\", ascending=True).head(10)"
   ],
   "metadata": {
    "id": "4036c26b9d58583d",
    "outputId": "f89ab324-82c0-4171-d94e-45f4a15232ad",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    }
   },
   "id": "4036c26b9d58583d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get top 10 reviews with worst predictions in the test set\n",
    "test_df = pd.DataFrame({\"text\": x_test, \"actual\": y_test, \"predicted\": model.predict(x_test)})\n",
    "test_df[\"error\"] = abs(test_df[\"actual\"] - test_df[\"predicted\"])\n",
    "test_df.sort_values(\"error\", ascending=False)"
   ],
   "metadata": {
    "id": "e3cf8bbbb7dc3b8c",
    "outputId": "7002428c-ec1e-4d95-b1ce-5925bb3232e6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    }
   },
   "id": "e3cf8bbbb7dc3b8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your Turn**\n",
    "\n",
    "Our model predicts the score given to a wine based on the text of the review. But there are a few different columns that we could alternatively predict! Choose one of the following columns, and build a model to predict it based on the text of the review. Explore the results. Do you find anything interesting?\n",
    "\n",
    "* `country`\n",
    "* `price`\n",
    "* `variety`\n",
    "* `winery`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a1baad02308a850c"
   },
   "id": "a1baad02308a850c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "id": "db85302e8a42bd55"
   },
   "id": "db85302e8a42bd55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case Study 3: Recommendation\n",
    "\n",
    "For our last case study, we are going to look at a dataset of movie ratings. We're going to start by building a simple recommendation system that recommends movies by finding the most similar users. Then, we will move on to using a powerful library that implements some of the state-of-the-art approaches.\n",
    "\n",
    "Let's begin by loading part of the MovieLens dataset. This is a popular dataset of user ratings of movies. We will be using the `ratings` dataset, which contains the ratings given by users to movies. We will also load the `movies` dataset, which contains information about each movie."
   ],
   "metadata": {
    "collapsed": false,
    "id": "67646fe45e991a11"
   },
   "id": "67646fe45e991a11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"https://github.com/alexwolson/carte_workshop_datasets/raw/main/movies.csv.zip\", compression=\"zip\")\n",
    "ratings = pd.read_csv(\"https://github.com/alexwolson/carte_workshop_datasets/raw/main/ratings.csv.zip\", compression=\"zip\")"
   ],
   "metadata": {
    "id": "4c7a6978dd7dae50"
   },
   "id": "4c7a6978dd7dae50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movies.head()"
   ],
   "metadata": {
    "id": "39e6e4dff5f4a1c2",
    "outputId": "dada58e1-66b9-44ca-b8f4-43e022c72b64",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    }
   },
   "id": "39e6e4dff5f4a1c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratings.head()"
   ],
   "metadata": {
    "id": "d1886f32ca515564",
    "outputId": "feb50b62-a01c-4e45-f9a2-cd89030bab58",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    }
   },
   "id": "d1886f32ca515564"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the `ratings` dataset contains a `userId`, a `movieId`, a `rating`, and a `timestamp`. The `movies` dataset contains a `movieId`, a `title`, and a list of `genres`. We are not going to make predictions based on genre today, but it's a common approach to recommendation in this area. Instead, we will just focus on the users and their ratings.\n",
    "\n",
    "Let's look at a random user to get a sense of what a users' ratings could look like:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ab4401ba85afd69e"
   },
   "id": "ab4401ba85afd69e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratings[ratings.userId == 42].merge(movies, on=\"movieId\") # Merging so that we can see what the movies are"
   ],
   "metadata": {
    "id": "a1e21043de9902f7",
    "outputId": "87a75779-3a59-4ea1-806d-12dd5a526437",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    }
   },
   "id": "a1e21043de9902f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we are working with users who have already rated a number of movies on the system, one approach is to look for the most similar users, and then recommend movies that those users have rated highly. We can do this by computing the similarity between users. We will use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between the ratings of two users as our measure of similarity. The cosine similarity is a measure of the angle between two vectors. If the angle is small, the vectors are similar. If the angle is large, the vectors are dissimilar. We will use the `cosine_similarity` function from the `sklearn.metrics.pairwise` module to compute the cosine similarity between users.\n",
    "\n",
    "We will also need to convert the format of our data from a list of users and reviews, to a matrix of users and reviews. We can do this using the `pivot_table` method. This method takes a DataFrame, and converts it from a long format to a wide format. We will use the `userId` as the index, the `movieId` as the columns, and the `rating` as the values. We will also fill in any missing values with 0, since we are only interested in whether a user has rated a movie or not."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9d93ead3fbd0504b"
   },
   "id": "9d93ead3fbd0504b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "ratings_matrix = ratings.pivot_table(index=\"userId\", columns=\"movieId\", values=\"rating\", fill_value=0)"
   ],
   "metadata": {
    "id": "ba1741e1dd4236f"
   },
   "id": "ba1741e1dd4236f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_one = ratings_matrix.iloc[42]\n",
    "user_two = ratings_matrix.iloc[43]\n",
    "print(f'Cosine similarity between user 42 and user 43: {cosine_similarity([user_one], [user_two])[0][0]:.2f}')"
   ],
   "metadata": {
    "id": "85db50c4ffd9767c",
    "outputId": "9c2df2f8-27b6-4056-b997-85306ad12bc7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "85db50c4ffd9767c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have our matrix and our method, let's go ahead and compute the similarity between each pair of users. We will store the results in a DataFrame, with the `userId` as the index and the `similarity` as the value."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5172081af3055f08"
   },
   "id": "5172081af3055f08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Convert the ratings matrix to a sparse matrix format if not already\n",
    "ratings_sparse = csr_matrix(ratings_matrix.values)\n",
    "\n",
    "# Compute the cosine similarity matrix in a vectorized way\n",
    "# This computes the full n x n similarity matrix\n",
    "similarities = cosine_similarity(ratings_sparse)\n",
    "\n",
    "# Since the similarity with itself is always 1, we can fill the diagonal with 1s\n",
    "np.fill_diagonal(similarities, 1)"
   ],
   "metadata": {
    "id": "e18226e6b273c6f2"
   },
   "id": "e18226e6b273c6f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the similarity between each pair of users, we can use it to make recommendations. For user 42, we can take the top 10 users who are most similar, and then recommend the movies that they have rated most highly."
   ],
   "metadata": {
    "collapsed": false,
    "id": "8ccf501eea9e2199"
   },
   "id": "8ccf501eea9e2199"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarities_df = pd.DataFrame(similarities, index=ratings_matrix.index, columns=ratings_matrix.index)"
   ],
   "metadata": {
    "id": "3ac2d5b5e7686d45"
   },
   "id": "3ac2d5b5e7686d45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_users = similarities_df[42].sort_values(ascending=False).head(10)"
   ],
   "metadata": {
    "id": "fc9957b3a25141b"
   },
   "id": "fc9957b3a25141b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommended_movies = ratings_matrix.loc[similar_users.index].mean().sort_values(ascending=False)\n",
    "# Remove movies that the user has already rated\n",
    "recommended_movies = recommended_movies[~recommended_movies.index.isin(ratings_matrix.iloc[42].replace(0, np.nan).dropna().index)]"
   ],
   "metadata": {
    "id": "8a3a36e4ced130aa"
   },
   "id": "8a3a36e4ced130aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for movie_id, rating in recommended_movies.head(10).items():\n",
    "    print(f'{movies[movies[\"movieId\"] == movie_id][\"title\"].iloc[0]} ({rating:.2f})')"
   ],
   "metadata": {
    "id": "ddc21c8359cc441d",
    "outputId": "a6be9440-4473-4677-90c1-5892c1d64a49",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "ddc21c8359cc441d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And there we have it - a simple recommendation system! Unfortunately, this approach has some major problems.\n",
    "\n",
    "1. Scalability - while it doesn't take too long to calculate similarities between 600 or so users, company like Netflix has millions or even billions of users!\n",
    "2. Cold start - what if we have a new user who hasn't rated any movies yet? We can't make any recommendations for them.\n",
    "3. Popularity bias - this approach will recommend popular movies, since lots of people have rated them, even if they are not a good fit for the user.\n",
    "\n",
    "Let's use the same data, but employ a more sophisticated approach. We will use a library called Surprise, which implements a number of state-of-the-art methods for recommendation."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e252470b98865351"
   },
   "id": "e252470b98865351"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U -q surprise"
   ],
   "metadata": {
    "id": "e6cf8f40c2ce8436",
    "outputId": "dd4603a1-0180-445a-a8d4-20652dafa024",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "e6cf8f40c2ce8436"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we have to convert the data into a format that Surprise can understand. We will use the `Reader` class to specify the range of ratings, and then use the `Dataset` class to convert the data."
   ],
   "metadata": {
    "collapsed": false,
    "id": "62b8ae3e879ec6cf"
   },
   "id": "62b8ae3e879ec6cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(ratings[[\"userId\", \"movieId\", \"rating\"]], reader)"
   ],
   "metadata": {
    "id": "887d90625b0e73fb"
   },
   "id": "887d90625b0e73fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to use Singular Value Decomposition, or SVD. SVD works by breaking down our single, huge user-movie matrix into three smaller matrices. This process allows us to capture the most important patterns in the data using fewer details, which is essential when working with millions or even _billions_ of users. Using these three smaller matrices, SVD can approximate the expected values for missing entries in the user-movie matrix. This allows us to make predictions for new users, and to make recommendations for movies that have not been rated by many users."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2f6bd79db25d5142"
   },
   "id": "2f6bd79db25d5142"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SVD(random_state=42)\n",
    "start_time = time()\n",
    "model.fit(data.build_full_trainset())\n",
    "print(f'Training time: {time() - start_time} seconds')"
   ],
   "metadata": {
    "id": "6a14f8e783b39c5d",
    "outputId": "f07eb74f-a644-40b9-faaa-ff569f9979c9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "6a14f8e783b39c5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get top 10 movies for user 42\n",
    "user_42_movies = ratings[ratings[\"userId\"] == 42][\"movieId\"].unique()\n",
    "predicted_ratings = []\n",
    "for movie_id in movies[\"movieId\"].unique():\n",
    "    if movie_id in user_42_movies:\n",
    "        continue\n",
    "    predicted_ratings.append((movie_id, model.predict(42, movie_id).est))\n",
    "predicted_ratings.sort(key=lambda x: x[1], reverse=True)"
   ],
   "metadata": {
    "id": "f718991a0c06f996"
   },
   "id": "f718991a0c06f996"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for movie_id, rating in predicted_ratings[:10]:\n",
    "    print(f'{movies[movies[\"movieId\"] == movie_id][\"title\"].iloc[0]} ({rating:.2f})')"
   ],
   "metadata": {
    "id": "23e42e1e3de1f8a0",
    "outputId": "fbfb0759-80a8-42d6-d863-fb810d48fe67",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "23e42e1e3de1f8a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, while many of these films are certainly popular, the SVD approach allows us to recommend movies that are more tailored to the user. We can also use the model to predict the rating that a user will give to a movie. This is a good way of evaluating the quality of the model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "6532b03bdc8cd5aa"
   },
   "id": "6532b03bdc8cd5aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratings[ratings[\"userId\"] == 42].merge(movies, on=\"movieId\") # Merging so that we can see what the movies are"
   ],
   "metadata": {
    "id": "1c4bfe595b022071",
    "outputId": "09d7d517-48fa-4a54-bde0-e770f0110330",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    }
   },
   "id": "1c4bfe595b022071"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for movie_id in user_42_movies:\n",
    "    predictions.append({\n",
    "        \"movieId\": movies[movies[\"movieId\"] == movie_id][\"title\"].iloc[0],\n",
    "        \"predicted\": model.predict(42, movie_id).est,\n",
    "        \"actual\": ratings[(ratings[\"userId\"] == 42) & (ratings[\"movieId\"] == movie_id)][\"rating\"].iloc[0]\n",
    "    })\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df[\"error\"] = abs(predictions_df[\"predicted\"] - predictions_df[\"actual\"])\n",
    "print(f'MAE: {predictions_df[\"error\"].mean():.2f}')"
   ],
   "metadata": {
    "id": "f55c833919da1161",
    "outputId": "2c1d7990-cac2-4336-ab1b-741e96ab82c6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "f55c833919da1161"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compare this against our original method:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d4a162dce33571f5"
   },
   "id": "d4a162dce33571f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_users = similarities_df[42].sort_values(ascending=False).head(10)\n",
    "recommended_movies = ratings_matrix.loc[similar_users.index].mean().sort_values(ascending=False)\n",
    "\n",
    "predictions = []\n",
    "for movie_id in user_42_movies:\n",
    "    predictions.append({\n",
    "        \"movieId\": movies[movies[\"movieId\"] == movie_id][\"title\"].iloc[0],\n",
    "        \"predicted\": recommended_movies[movie_id],\n",
    "        \"actual\": ratings[(ratings[\"userId\"] == 42) & (ratings[\"movieId\"] == movie_id)][\"rating\"].iloc[0]\n",
    "    })\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df[\"error\"] = abs(predictions_df[\"predicted\"] - predictions_df[\"actual\"])\n",
    "print(f'MAE: {predictions_df[\"error\"].mean():.2f}')"
   ],
   "metadata": {
    "id": "8e566d59c34b7ec",
    "outputId": "3b88a68c-9025-4f0a-d6b4-ced2779e3923",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "8e566d59c34b7ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the SVD approach is not only much faster, but more accurate in reproducing the user's original ratings. SVD is simple, effective, and highly scalable - which is why it was the industry standard for companies like Amazon, Netflix, and Spotify for many years.\n",
    "\n",
    "**Your Turn**\n",
    "\n",
    "One challenge with a five-star rating system (and a big reason why companies like YouTube and Netflix have long since moved to a 'thumbs up, thumbs down' approach) is that each user has a different idea of what each rating means. For example, one user might give a 5-star rating to their favourite movie, while another user might only give a 5-star rating to a movie that they consider to be perfect. Try setting all ratings to 1 if a user rated 4 or 5, or 0 otherwise. How does this affect prediction quality?"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c1ecc8a731b5a4e0"
   },
   "id": "c1ecc8a731b5a4e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "id": "b79f30af8dcd6484"
   },
   "id": "b79f30af8dcd6484"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion, and bonus\n",
    "\n",
    "We have covered a lot of ground today! We have looked at three different case studies, each of which uses a different approach to AI in Market Strategy. We have seen how AI can be used to predict the future, to understand text, and to make recommendations. We've looked at not just real-world examples, but also state-of-the-art toolkits that are used by many companies today.\n",
    "\n",
    "As a bonus exercise, pick the case study that you found most interesting, and see if you can expand on the results from today. Some ideas:\n",
    "\n",
    "* Predictive Analytics: Can you visualize the data in a more informative way? Can you predict the volume of avocados sold for a specific region, or for a specific type of avocado?\n",
    "* Natural Language Processing: Can you identify reviewers' favourite regions or varieties of wine? Can you identify the most common words used to describe different types of wine?\n",
    "* Recommendation: Surprise includes a number of different models. Can you try a different model, and compare the results? Can you use the model to recommend movies to a new user?"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9a259fd31e363f25"
   },
   "id": "9a259fd31e363f25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "ff21f5ab784c44cd"
   },
   "id": "ff21f5ab784c44cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
