{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 1-2\n",
    "\n",
    "### Scikit-learn Basics\n",
    "\n",
    "Scikit-learn is a great library to use for doing machine learning in Python. Data preparation, exploratory data analysis (EDA), classification, regression, clustering; it has it all. \n",
    "\n",
    "Scikit-learn usually expects data to be in the form of a 2D matrix with dimensions *n_samples x n_features* with an additional column for the target. To get acquainted with scikit-learn, we are going to use the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris), one of the most famous datasets in pattern recognition. \n",
    "\n",
    "Each entry in the dataset represents an iris plant, and is categorized as: \n",
    "\n",
    "* Setosa (class 0)\n",
    "* Versicolor (class 1)\n",
    "* Virginica (class 2)\n",
    "\n",
    "These represent the target classes to predict. Each entry also includes a set of features, namely:\n",
    "\n",
    "* Sepal width (cm)\n",
    "* Sepal length (cm)\n",
    "* Petal length (cm)\n",
    "* Petal width (cm)\n",
    "\n",
    "In the context of machine learning classification, the remainder of the lab is going to investigate the following question:  \n",
    "*Can we design a model that, based on the iris sample features, can accurately predict the iris sample class? *\n",
    "\n",
    "Scikit-learn has a copy of the iris dataset readily importable for us. Let's grab it now and conduct some EDA."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ff1075f14e4dfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "feature_data = iris_data.data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43bdf3b5a48cbae7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**YOUR TURN:** \"feature_data\" now contains the feature data for all of the iris samples. \n",
    "* What is the shape of this feature data? ________________\n",
    "* The data type? ________________\n",
    "* How many samples are there? ________________\n",
    "* How many features are there? ________________"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a4c5df7875d750e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Enter your code here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0a6e00b7e475cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will save the target classification data in a similar fashion."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98604e20b0686e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_data = iris_data.target\n",
    "target_names = iris_data.target_names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "743aac4e3389c934"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**YOUR TURN:**\n",
    "* What values are in \"target_data\"? ________________\n",
    "* What is the data type? ________________\n",
    "* What values are in \"target_names\"? ________________\n",
    "* What is the data type? ____________\n",
    "* How many samples are of type \"setosa\"? ________________"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b0590d6c07aa79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Enter your code here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ff2472bddf83c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also do some more visual EDA by plotting the samples according to a subset of the features and coloring the data points to coincide with the sample classification. We will use [matplotlib](https://matplotlib.org/) as before, to accomplish this, combined with [seaborn](https://seaborn.pydata.org/), a Python library for statistical data visualization.\n",
    "\n",
    "For example, let's plot sepal width vs. sepal length.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ebcd6e9576a7402"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preparing data for plotting\n",
    "setosa = feature_data[target_data == 0]\n",
    "versicolor = feature_data[target_data == 1]\n",
    "virginica = feature_data[target_data == 2]\n",
    "\n",
    "# Plotting setosa\n",
    "plt.scatter(setosa[:, 0], setosa[:, 1], label=\"setosa\")\n",
    "\n",
    "# Plotting versicolor\n",
    "plt.scatter(versicolor[:, 0], versicolor[:, 1], label=\"versicolor\")\n",
    "\n",
    "# Plotting virginica\n",
    "plt.scatter(virginica[:, 0], virginica[:, 1], label=\"virginica\")\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"sepal length (cm)\")\n",
    "plt.ylabel(\"sepal width (cm)\")\n",
    "plt.title(\"Visual EDA\")\n",
    "plt.legend(title=\"Class\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f43d9303c5ec00a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above step, we used boolean indexing to filter the feature data based on the target data class. This allowed us to create a scatter plot for each of the iris classes and distinguish them by color.\n",
    "\n",
    "*Observations*: We can see that the \"setosa\" class typically consists of medium-to-high sepal width with low-to-medium sepal length, while the other two classes have lower width and higher length. The \"virginica\" class appears to have the largest combination of the two. \n",
    "\n",
    "**YOUR TURN:** \n",
    "* Which of the iris classes is seperable based on sepal characteristics? ________________\n",
    "* Which of the iris classes is not? ________________\n",
    "* Can we (easily) visualize each of the samples w.r.t. all features on the same plot? Why/why not? ________________"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68945967661fda43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a k-Nearest Neighbors Classifier\n",
    "\n",
    "Now that we've explored the data a little bit, we're going to use scikit-learn to create a k-nearest neighbors classifier for the data. Effectively we'll be developing a model whose job it is to build a relationship over input feature data (sepal and petal characteristics) that predicts the iris sample class (e.g. \"setosa\"). This is an example of a *supervised learning* task; we have all the features and all the target classes.\n",
    "\n",
    "Nearest neightbors classifiers are quite simple. They predict the class of a new data sample based off the *nearest* data points to that sample. The 'nearest' metric is calculated via a distance function (often [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)). \n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/knn.png?raw=1\" alt=\"knn\" width=\"200\"/>\n",
    "\n",
    "For example, in the above diagram, suppose we are looking to classify the green circle as either a red triangle, or a blue square. If k = 1 (i.e., we look at one neighbor), our model would predict *red triangle*. If k=2, it would still predict *red triangle*. If k=3, the model would predict *red triangle* as it is the *majority* class of the 3 nearest neighbors. It isn't until k=5 that the algorithm actually predicts *blue square*.\n",
    "\n",
    "Model creation in scikit-learn follows a **data prep -> fit -> predict** process. The \"fit\" function is where the actual model is trained and parameter values are selected, while the \"predict\" function actually takes the trained model and applies it to the new samples.\n",
    "\n",
    "First, we're going to save our feature data into an array called 'X' and our target data into an array called 'y'. We don't *need* to do this, but it is traditional to think of the problem using this notation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c725382b956caebf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = feature_data\n",
    "y = target_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924951aa94533bd5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create our nearest neighbor classifier object:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a280e9d3eda6c32f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9021876a68633336"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And then we *fit* it to the data (i.e., train the classifier)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c46acea3e368f41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4cddaf46238d042"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a model! If you're new to this, you've officially built your first machine learning model. If you use \"knn.predict(*[[feature array here]]*)\", you can use your trained model to predict the class of a new iris sample. \n",
    "\n",
    "**YOUR TURN:**\n",
    "* What is the predicted class of a new iris sample with feature vector [3,4,5,2]? What is its name? ________________\n",
    "* Do you think this model is overfit or underfit to the iris dataset? Why? ________________\n",
    "* How many neighbors does our model consider when classifying a new sample? ________________"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "203fda45e8c10b43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Insert code here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4851381cbdc2e008"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you may have noted in the previous cell, we've trained this classifier on our *entire dataset*. This typically isn't done in practice and results in overfitting to the data. Here's a bit of a tricky question:\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If we use our classifier to predict the classes of the iris samples that were used to train the model itself, what will our overall accuracy be? ________________\n",
    "\n",
    "We can validate our hypothesis fairly easily using either: i) the NumPy technique for calculating accuracy we used earlier in the lab, or ii) scikit-learn's in-house \"accuracy_score()\" function.\n",
    "\n",
    "Let's use our technique first:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92df3fb4bcf3bf97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy = np.mean(target_data == knn.predict(feature_data))\n",
    "print(f\"Accuracy: {accuracy * 100}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60944b1d8a954ba2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "and then using scikit-learn's customized function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a445cb1d3063fcfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(target_data, knn.predict(feature_data))\n",
    "print(f\"Accuracy: {accuracy * 100}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4402c4236f0f792e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that our classifier has achieved 100% accuracy (and both calculation methods agree)!\n",
    "\n",
    "**DISCUSSION:** \n",
    "* Why do you think the model was able to achieve such a \"great\" result? ______________________\n",
    "* What does this really tell us?  __________________________________\n",
    "* Do you expect the model to perform this well on new data? __________________________________"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db42e47da2599ba7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross Validation\n",
    "\n",
    "A popular way to mitigate this overfitting issue is to train your model on *some* of the data (the training set) and validate your model on the remaining data (the validation set). You will then select the model/configuration that performs best on the validation data. The train/validate division of the data is usually done with a 70%/30% split. Often, practitioners will use a third data set, the test set (or hold-out set), to get a sense for how their best model performs on unseen, real-world data. In this scenario, you will tune your models to perform best on the validation set and then test their \"real-world\" performance on the unseen test set.\n",
    "\n",
    "Sometimes applications don't have enough data to do these splits meaningfully (e.g., the test data is only a few samples). In these cases, *cross-validation* is a useful technique (and, indeed, has become standard in machine learning practice). \n",
    "\n",
    "The general premise of \"k-folds\" cross validation is to first divide the entire dataset (grey) into a training set (green) and a test set (unseen data, blue). Then, we divide the training set into different folds and use these folds to form new sub-training and sub-test sets. We select the model configuration that performs the best on all of these. The below figure provides a nice visualization for what's going on here:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/cross-val.png?raw=1\" alt=\"cross-val\" width=\"500\"/>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea6821f7f73d8cd4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accomplishing k-folds cross validation in scikit-learn is a manageable task. First, we divide our data into a train and test set, then we conduct the cross validation and look at the mean scores across the splits, then we conduct our final evaluation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48f1bc9620542089"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_data, target_data, test_size=0.3, random_state=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea55fa8a95d88cde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have divided our data into two sections: training data (70% of the data) and testing data (30% of the data). Now we will fit our nearest neighbors classifier to the training data with 5 folds and see how it performs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd5c4d1627b74c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "print(f\"Accuracy: {scores.mean():.2f} (+/- {scores.std() * 1.96:.2f})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecd905b6c8a7749b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our cross-validated model has an accuracy of 94% across all the splits on the training data. If we think that is a reasonable value, we can train our final model on the training data and then see how it performs on the held-out test data. \n",
    "\n",
    "##### Comparing classifiers\n",
    "However, to get a true sense for the utility of cross-validation, let's create a second nearest neighbors classifier that uses two neighbors instead of one. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e51ba582e3e8b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_2_neighbors = KNeighborsClassifier(n_neighbors=2)\n",
    "scores = cross_val_score(knn_2_neighbors, X_train, y_train, cv=5)\n",
    "print(f\"Accuracy: {scores.mean():.2f} (+/- {scores.std() * 1.96:.2f})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88cb7297ba711791"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we see above, our second classifier (the one with two neighbors) actually performs worse when cross-validated (92% vs. 94% mean accuracy on the 5 folds)! So, we'll stick with the first one.\n",
    "\n",
    "Evaluating the model on held-out test data is a critical step in assessing its performance. By using data that the model has not seen during training, we can gauge how well the model generalizes to new, unseen data. This provides a more realistic estimation of how the model would perform in a real-world scenario where it encounters data points it has not seen before.\n",
    "\n",
    "Let's train the chosen model on the training data and use it to predict the final held-out test data. Evaluating on the test data will help us understand the model's ability to generalize, which is a key aspect of a reliable and robust machine learning model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1b192e83f8e8a8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
    "print(f\"Test set accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fae52f947be350b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we see our model has a 97.8% accuracy on the held out test data (30% of the original dataset).\n",
    "\n",
    "## Bonus Task: Confusion Matrix Analysis\n",
    "\n",
    "A confusion matrix is a table used to describe the performance of a classification model on a set of data for which the true values are known. It is especially powerful for multi-class classification, like our iris dataset.\n",
    "\n",
    "In this bonus task, you will create a confusion matrix for the k-nearest neighbors classifier you trained earlier on the test data. Then, calculate specific metrics like precision, recall, and F1-score for each class. Here's a step-by-step guide to help you get started:\n",
    "\n",
    "1. Predict the Test Data: You've already done this, so you can simply use the predictions you made for the test data. \n",
    "2. Create the Confusion Matrix: You can use scikit-learn's confusion_matrix function for this.\n",
    "3. Analyze the Matrix: Calculate precision, recall, and F1-score for each class. You might find scikit-learn's classification_report helpful or calculate them manually.\n",
    "\n",
    "Below is a snippet to help you create the confusion matrix. The rest is up to you!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d098b388660125f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Create the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Now it's your turn to analyze this matrix and calculate precision, recall, and F1-score for each class!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6461ffc136350dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
