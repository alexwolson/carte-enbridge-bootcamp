{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcK5U2gBGPUf"
   },
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 5-2\n",
    "\n",
    "Dealing with Unlabelled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPe6o2o3GPUj"
   },
   "source": [
    "Throughout the week, we have focused on methods for machine learning that rely on predicting a known quantity. That is, all the data we've worked with requires some examples of the factor we want to predict - whether that's the rating of a bottle of wine, or the severity of a car crash.\n",
    "\n",
    "However, there are often situations where we want to understand data better, but we don't have reliably labelled data. Maybe this is because the label is challenging to acquire (maybe we need people to label our data by hand), or maybe it's just that we don't know what we want to predict yet.\n",
    "\n",
    "In this final lab, we will run through a number of methods for analyzing data without having clear labels to predict on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ku8GGD-GPUk"
   },
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8okplLlmGPUl",
    "outputId": "79e4d474-418d-4d9c-b77e-a6ea1ca59ccb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN7HkqxbGPUm"
   },
   "source": [
    "K-Means clustering is an excellent method for uncovering groups within a dataset, without needing to know what those groups are beforehand. The intuition is simple - we are trying to find groups of items that are more similar to each other than they are to items in any other group.\n",
    "\n",
    "The K-Means algorithm identifies a pre-determined number of clusters (K) within an unlabeled dataset. It does this using a simple procedure describing what each cluster should look like: \n",
    "\n",
    "* Each cluster can be represented by the average (mean) of all the points in that cluster.\n",
    "* Each point is closer to the mean point in its own cluster than to that of any other cluster.\n",
    "\n",
    "Let's get a visual representation of what this looks like. First we will generate a simple 2D dataset containing four distinct blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CBDWjbnGPUn",
    "outputId": "1c218be7-4981-4473-969b-ea4f0b401cd2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjW1bEALGPUo"
   },
   "source": [
    "It's pretty easy to tell where the four clusters are by eye, but let's see how K-Means performs at picking them out. We will also indicate the representative point for each cluster (the _cluster centre_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRc3SwelGPUp",
    "outputId": "3b6c9464-5a08-421f-9680-731825ec5997",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu8MLZ7MGPUq"
   },
   "source": [
    "The good news is that the K-Means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. But how does K-Means identify these groups? The first thought for many is that we could directly compare every pair of points, and use that information to group them. But doing this requires exponentially more computing for every point we add, just like with our recommendation case yesterday. Instead, we use a clever algorithm called Expectation Maximization (EM) that allows us to make a much smaller number of comparisons, dramatically speeding things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nOig1lxGPUr"
   },
   "source": [
    "In short, K-Means consists of the following procedure:\n",
    "\n",
    "* Guess some cluster centres (at random, or using a heuristic)\n",
    "* Repeat until convergence (i.e. the clusters stop changing between steps):\n",
    "    * Assign each point to the closest cluster centre\n",
    "    * Re-calculate the average point for each cluster (i.e. its cluster centre)\n",
    "    \n",
    "The K-Means algorithm is simple enough that we can write it in a few lines of code. The following is a very basic implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1-5nzKuGPUs",
    "outputId": "16a1489e-aaf2-4c46-974d-de1f26700ec1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "                \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check if the clusters changed\n",
    "        if np.all(centers == new_centers):\n",
    "            break # If the clusters didn't change, we are finished!\n",
    "            \n",
    "        centers = new_centers\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "                    s=50, cmap='viridis')\n",
    "        plt.pause(0.05)\n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0fWky2dGPUt"
   },
   "source": [
    "## Caveats\n",
    "\n",
    "If K-Means were perfect, we could stop here. But unfortunately, while elegant, this algorithm does come with its own set of problems. For example, the random seed used to initialize the clusters can affect the results - there is no guarantee that the algorithm will find the best _possible_ solution. (For an example of this, try changing the random seed in the previous example to `3`).\n",
    "\n",
    "Another common problem with K-Means is that you must tell it how many clusters to look for - it cannot learn this from the data. This is fine in simple, low-dimensional examples such as the one above, but when the data is high-dimensional it becomes impossible to eyeball the correct number of clusters for your data. In fact, we can suggest an obviously incorrect number of clusters, and the model won't tell us that we're doing anything wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny8EfiW4GPUt",
    "outputId": "d866fbc6-8c9b-4001-f1a8-d374a04334ad",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=50)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=25, cmap='nipy_spectral')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV4Cm6PCGPUu"
   },
   "source": [
    "Again, this is obviously wrong in this example, but the algorithm has no way of knowing this. However, there *is* a way to identify the best value for K from the data - at least, heuristically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH1k3zECGPUu"
   },
   "source": [
    "## Finding the Elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAI6Qh1eGPUu"
   },
   "source": [
    "If you were tasked with calculating a score that indicates the best possible fit for the data, you might try the average distance for each point in a cluster to its cluster centre. The logic here would be that a better cluster should have all its points close to its centre - hence, the descriptive power of that cluster centre would be higher.\n",
    "\n",
    "Unfortunately, you may realize, the more clusters there are, the better this score would be. In fact, for N data points, the optimal value would be N=K! At this value, every point would be exactly its cluster centre. But then we aren't really clustering at all!\n",
    "\n",
    "The point is, as K increases, the average proximity to the cluster centre is _guaranteed_ to increase. But there is still some information we can gain from this metric. While the value itself always increases, the _rate of change_ can differ - and here is the secret to identifying the best value for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pm_uMcLBGPUv",
    "outputId": "49da2a4b-926a-4db2-dc15-743f5c7d5ccc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "av_dist = []\n",
    "\n",
    "for k in range(1,10):\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    av_dist.append(\n",
    "        sum(\n",
    "            np.min(\n",
    "                cdist(X, km.cluster_centers_, 'euclidean'),\n",
    "                axis=1\n",
    "            )\n",
    "        ) / X.shape[0]\n",
    "    )\n",
    "    \n",
    "plt.plot(range(1,10), av_dist)\n",
    "plt.xlabel('Value for K')\n",
    "plt.ylabel('Average distance to cluster centre');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsIwdxXTGPUv"
   },
   "source": [
    "Looking at this graph, it's transparently obvious where the improvement with a higher K stops becoming substantial - K = 4, just as we would expect. If we plot the _improvement_, this transition becomes even more obvious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jNqDL6SGPUw",
    "outputId": "8da00056-ac86-4dea-94f4-e3e5d24d8da2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,10),-np.gradient(av_dist))\n",
    "plt.xlabel('Value for K')\n",
    "plt.ylabel('Improvement from previous K');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4KjCinpGPUw"
   },
   "source": [
    "The _elbow_ is the point in the line where improvement transitions from substantial to minimal. Of course, such a clear elbow is not guaranteed - if your dataset doesn't cluster very well, it might be the case that there's no clear transition from good improvement to poor improvement. But in situations where there *is* a good value for K waiting to be found, the Elbow method can help us consistently find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGV7ndAUGPUy"
   },
   "source": [
    "### Dimensionality reduction and principle component analysis\n",
    "\n",
    "When working with data in the real world, we're almost always dealing with more features than we can conveniently plot in a single figure. But sometimes, we want to be able to quickly see relationships or patterns across all of our data in a single go. This is where dimensionality reduction can come in. \"Dimensionality Reduction\" is exactly what it sounds like - reducing the number of dimensions (aka. features) so that we can more easily look at our data.\n",
    "\n",
    "Obviously, as suggested above, one reason to do this is to aid visualization. But that's far from the only reason dimensionality reduction is useful! These techniques also allow us to filter noise, extract useful features, and much more.\n",
    "\n",
    "Let's dive into PCA. We'll first take a two dimensional dataset and reduce it to one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LCs_PESGPUy",
    "outputId": "74db3222-b6e6-46ef-ae05-0e13e0c03ab4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "X = np.dot(rng.rand(2,2),\n",
    "          rng.randn(2,200)).T\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNnI-cPmGPUy"
   },
   "source": [
    "Here we have a two dimensional dataset that, by eye, has a nearly linear relationship between the X and Y values. This is reminiscent of the type of data we would look to explore with linear regression, but the problem setting here is slightly different: rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values.\n",
    "\n",
    "In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn's PCA estimator, we can compute this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M5vDm-mGPUy"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kExbKbkGPUz"
   },
   "source": [
    "The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1YDvyu2GPUz",
    "outputId": "480761e3-1bc8-4857-e57a-00867c6c3074",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSBVHdnnGPUz",
    "outputId": "2cdb2813-face-4dab-c9a3-0ce69d99b791",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esw1b3ltGPU0"
   },
   "source": [
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_T6MNK-GPU0",
    "outputId": "16ebb0df-dfe5-4d21-aeaf-083692db6409",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='black')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf9Gvj0UGPU0"
   },
   "source": [
    "These vectors represent the principal axes of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
    "\n",
    "You have likely noticed that in this example, almost all of the variance is explained by the first axis, while the second axis explains very little variance. This is where PCA becomes a tool for dimensionality reduction. By transforming the data to lie along the PCA dimensions and then only keeping the first dimension, we can reduce our dataset into one-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdg0VebdGPU0",
    "outputId": "fa87ffe8-759b-47ad-8fe0-043cf0def632",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "plt.hist(X_pca,bins=20)\n",
    "plt.xlabel('Value in reduced space')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzw1cDaUGPU1"
   },
   "source": [
    "If we project our new, low-dimensional data back into the original space, we can get a good visual intuition of what PCA has done for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHWQrAXEGPU1",
    "outputId": "9f92d50b-9f42-4d74-dfc5-6e939cd72e27",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVfR41wnGPU1"
   },
   "source": [
    "The light points are the original data, while the orange points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved.\n",
    "\n",
    "The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data. To see this, let's take a quick look at the application of PCA to a much higher dimensional dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKAM_YoFGPU2",
    "outputId": "a5ecf2fd-e82c-47ee-ecd7-d094767caf15",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()\n",
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lNM9-MUGPU2"
   },
   "source": [
    "This dataset contains a series of hand-written digits, translated to an 8x8 (i.e. 64 dimensional) grid. This is similar to the MNIST data we worked with previously, except it starts out with even fewer pixels. To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1yhLKN0GPU2",
    "outputId": "8ea8ba7c-f088-4221-acdf-5a520ec194a5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXwTOVaEGPU2",
    "outputId": "3734076c-6e46-441e-ce6b-aa2366f3764c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9zF9ZoPGPU3"
   },
   "source": [
    "Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner—that is, without reference to the labels.\n",
    "\n",
    "By looking at this figure, we can begin to understand the ways that some digits are more or less similar to each other. For example, it makes some sense that we see significant overlap between 7 and 1 - which can often be drawn in a simliar way to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3K5lsV8GPU3"
   },
   "source": [
    "## TSNE\n",
    "\n",
    "We are now going to look at a different method of dimensionality reduction: t-SNE, a method partially developed here at U of T by Geoffrey Hinton in 2008. It differs from PCA in that it uses the _local relationships_ between points to create a low-dimensional mapping. Among other things, this allows t-SNE to capture non-linear structures in the data.\n",
    "\n",
    "There are a number of other benefits to t-SNE over PCA, but since we are focusing on using dimensionality reduction for visualization we will stick to the benefits which are apparent in that space. Because t-SNE considers the local relationships between points, it ensures that the distances between points in the low dimensional mapping are representative of the distances in the original space. This makes it a lot more useful for visualization compared to PCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyHXxVcuGPU3"
   },
   "source": [
    "### How t-SNE works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIwBl1DJGPU4"
   },
   "source": [
    "t-SNE – at a high level – basically works like this:\n",
    "\n",
    "Step 1: In the high-dimensional space, create a probability distribution that dictates the relationships between various neighboring points\n",
    "\n",
    "Step 2: It then tries to recreate a low dimensional space that follows that probability distribution as best as possible.\n",
    "\n",
    "The “t” in t-SNE comes from the t-distribution, which is the distribution used in Step 2. The “S” and “N” (“stochastic” and “neighbor”) come from the fact that it uses a probability distribution across neighboring points.\n",
    "\n",
    "Let's swap out our smaller version of the dataset for MNIST, so that we can reduce even more dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMuzJqwIGPU4",
    "outputId": "b81e7739-3341-4adc-e87e-d61dc1c58aaf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784',cache=False) #This could take a little while - it's a lot of data.\n",
    "X = mnist['data']\n",
    "y = mnist['target']\n",
    "y = [int(t) for t in y]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i2HAFp-GPU4"
   },
   "source": [
    "First, we use our knowledge of PCA to reduce MNIST to two dimensions and plot it, color coded by target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjjE2pdmGPU4",
    "outputId": "11c274d7-1e80-47d8-d5a6-955202a4f3f4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10));\n",
    "\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrbFZCnUGPU4"
   },
   "source": [
    "Okay, so that should be similar to what we saw above with the smaller digit dataset, but it's honestly not very useful. Everything is just in a huge, formless blob and we aren't really gathering much information - it's just overwhelming. So let's see what t-SNE can do. \n",
    "\n",
    "**NOTE** t-SNE is going to take a lot longer to run than PCA. Although you *can* run it on the entire 70,000 image dataset, this could take a significant amount of time even on a high end computer. If you don't want to wait around that long, uncomment the line below to get a random subsample of the data which you can use instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTrNCTonGPU5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_,X,_,y = train_test_split(X,y,test_size=1/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbBPUsqBGPU5",
    "outputId": "e25890b7-39fc-4ae1-cb26-4f26fe3b13f1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(X)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xF4LBI-wGPU5",
    "outputId": "f62fbc3b-c524-4168-e2f3-2506858f9ef9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1],\n",
    "            c=y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHBmgSbIGPU5"
   },
   "source": [
    "Woah! Using t-SNE, the clusters are much more clearly defined, with different numbers being clearly separated in the low dimensional space. Running a clustering algorithm like K-Means now would likely give us pretty reasonable performance on classifying the digits, even just from two dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "colab": {
   "name": "lab-2-2-pca.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
