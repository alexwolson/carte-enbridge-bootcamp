{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 2-1\n",
    "\n",
    "# Introduction to TensorFlow\n",
    "\n",
    "In this lab, we will go over the basics of using TensorFlow to build and train a neural network. TensorFlow is one of the two most popular deep learning frameworks (the other being PyTorch). It is developed by Google and is used in many of their products. \n",
    "\n",
    "Using the sub-module Keras, we will build a simple neural network to classify images of handwritten digits from the MNIST dataset.\n",
    "\n",
    "First, we will import TensorFlow and check the version:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93242a83b1519681"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "print(\"Using TensorFlow version\", tf.__version__)\n",
    "\n",
    "# Use GPU, if available\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != \"/device:GPU:0\":\n",
    "    print(\"GPU device not found\")\n",
    "else:\n",
    "    print(f\"Found GPU at: {device_name}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bc536b237a5fc4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will download the MNIST dataset. Keras provides a convenient function for this. The dataset is already split into training and test sets. We will use the training set to train the model and the test set to evaluate the model's performance on unseen data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "662ed750c8b05456"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download and prepare the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a33f3358b1ac1191"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we move forward, it's always helpful to visualize our data to get a sense of what we're working with. Let's display a few of the images from the training set along with their corresponding labels:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28bcaee35fafcca2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the first five images from the training dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a43bf44db5b98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's establish some basic information about our dataset, while we're at it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f206cadcaa6a16be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "num_classes = len(set(y_train))\n",
    "print(\"There are\", num_classes, \"classes in our dataset\")\n",
    "print(\"The shape of each image is\", input_shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1a8d046cd047751"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When working with neural networks, it is important to normalize the data so that the values all fall between 0 and 1. This is done by dividing each value by the maximum value in the dataset, which is 255 in the case of the MNIST dataset. \n",
    "\n",
    "We will also one-hot encode the labels. One-hot encoding is a process where we replace each label with a vector of length equal to the number of possible classes. It's called 'one-hot' because only one of the values in the vector is 1 (aka 'hot'), and the rest are 0.\n",
    "\n",
    "Part of the reason we do this here is that even though the labels are numbers, they are not ordinal. That is, the fact that the label for a 3 is greater than the label for a 2 does not mean that a 3 is more similar to a 2 than it is to a 4. In fact, the labels are categorical, not numerical. One-hot encoding allows us to treat the labels as categorical, which is important for the loss function we will use later on.\n",
    " \n",
    "This means that we will convert the labels from a single number to a vector whose length is equal to the number of possible classes. The vector will be all 0s except for the index corresponding to the label, which will be 1. For example, if the label is 3, the one-hot encoded label will be [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82bbeb103b543215"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize the data so that the values all fall between 0 and 1\n",
    "x_train = x_train / x_train.max()\n",
    "x_test = x_test / x_test.max()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7548256d16579945"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "print(f\"Before one-hot encoding: {y_train[0]}\")\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print(f\"After one-hot encoding:\\n {y_train[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e86f587dcc61847a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last thing we will do before getting to building our actual model is reshape our input. The images in the MNIST dataset are 28x28 pixels, but we need to flatten them into a single vector of length 784 in order to feed them into our neural network. Later on in the lab, we'll look at a neural network that can accept a 2D image, but for now we will keep it simple. We will do this using the reshape() function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa12f939983120e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Flatten the input images\n",
    "x_train_flattened = x_train.reshape(60000, 28 * 28)\n",
    "x_test_flattened = x_test.reshape(10000, 28 * 28)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(x_train[0], cmap=plt.cm.binary)\n",
    "ax[0].set_title(\"Before flattening\")\n",
    "ax[1].bar(range(28 * 28), x_train_flattened[0])\n",
    "ax[1].set_title(\"After flattening\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e78e7512cf429a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, now we're ready to build our model! The Keras Sequential API makes this very easy. We just define one layer of our neural network at a time, starting with the input layer. The first layer we add must specify the input shape, which is (28*28,) in our case. \n",
    "\n",
    "We will also be using two activation functions we haven't seen before - inside the network, we will use ReLU, and at the end we will use Softmax. ReLU stands for Rectified Linear Unit, and it is defined as f(x) = max(0, x). It is a very simple function, but it is very effective in neural networks. \n",
    "\n",
    "Softmax is a function that takes a vector of numbers and outputs a vector of the same length, where each value is between 0 and 1 and the sum of the values is 1. It is often used as the activation function for the output layer of a neural network, because it allows us to interpret the output as a probability distribution over the possible classes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fbd416d2ce60833"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Plot the ReLU function\n",
    "x = np.linspace(-5, 5, 100)\n",
    "ax[0].plot(x, np.maximum(0, x))\n",
    "ax[0].set_title(\"ReLU\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"f(x)\")\n",
    "# Plot the softmax function\n",
    "ax[1].plot(x, np.exp(x) / np.sum(np.exp(x)))\n",
    "ax[1].set_title(\"Softmax\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"f(x)\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1f542856e1ec2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First, define the input to the neural network\n",
    "input = keras.layers.Input(shape=(28 * 28,)) # Aka (784,)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaf1dace5963191b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Next, define the first hidden layer\n",
    "hidden1 = keras.layers.Dense(128, activation=\"relu\")(input) # 128 neurons, ReLU activation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41e19f1f8d8b49b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Next, define the second hidden layer\n",
    "hidden2 = keras.layers.Dense(64, activation=\"relu\")(hidden1) # 64 neurons, ReLU activation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81d7058973ab11f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, define the output layer\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(hidden2) # 10 neurons, softmax activation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcf5b9cea071e1f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we can define the model, specifying the input and output layers\n",
    "model = keras.models.Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa079ec0fe7af24b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the model\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7c3dfc69784ee18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It really is that easy! Now we just need to compile the model, specifying the optimizer, loss function, and metrics we want to use. We will use the standard stochastic gradient descent optimizer, the categorical cross-entropy loss function, and the accuracy metric. We use categorical cross-entropy because we have more than two classes. If we had only two classes, we would use binary cross-entropy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9abbc523b969dbef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = \"sgd\" # Stochastic gradient descent - the foundation of most neural network optimizers\n",
    "loss = \"categorical_crossentropy\" # The loss function we will use to train the model - categorical cross-entropy for more than two classes\n",
    "metrics = [\"accuracy\"] # The metric we will use to evaluate the model - accuracy is the proportion of correct predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97d481da44647a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss, \n",
    "    metrics=metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9748e97c508d66f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can train the model. We just need to specify the training data, the number of epochs, and the batch size. The batch size is the number of training examples that are fed into the model at once. The number of epochs is the number of times the model will see the entire training set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ef98bb7ea7ec12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train_flattened, y_train, epochs=5, batch_size=32, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb3fc69b9122d32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the end of each epoch, the model will evaluate its performance on the validation set - this is a held out part of our training data that allows us to monitor how well the model is training. It gives us an idea of how well the model will generalize to unseen data. But we still need to evaluate the model on the test set to get a final measure of its performance:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e0b313cb16df33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test_flattened, y_test)\n",
    "print(f\"Test loss: {loss:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d9535b19e79b143"
  },
  {
   "cell_type": "markdown",
   "source": [
    "That is very good! Let's take a look at some of the predictions the model made:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f00abf561d1089f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the first 5 correct predictions\n",
    "import numpy as np\n",
    "\n",
    "predictions = model.predict(x_test_flattened)\n",
    "correct_indices = np.nonzero(\n",
    "    np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)\n",
    ")[0]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[correct_indices[i]].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.xlabel(np.argmax(predictions[correct_indices[i]]))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a11ba7adcd9d4782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the first 5 incorrect predictions\n",
    "incorrect_indices = np.nonzero(\n",
    "    np.argmax(predictions, axis=1) != np.argmax(y_test, axis=1)\n",
    ")[0]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[incorrect_indices[i]].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.xlabel(f'{np.argmax(predictions[incorrect_indices[i]])} (True label: {np.argmax(y_test[incorrect_indices[i]])})')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "847648e8367f37a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As above, it can be helpful to look at some of the incorrect predictions to get a sense of what the model is getting wrong. Sometimes, there might be something surprising that we can learn from, but here it seems like the model is just getting stuck on some of the more unusually written digits."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c01c7659cae5d06c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Now we will build a convolutional neural network to classify the same images. Convolutional neural networks are a special type of neural network that are designed to work with images. With convolutional layers, a \"filter\" is learned, which is a small matrix of weights. The filter is applied to each part of the image, and the output is a new image that is a \"filtered\" version of the original image. The filters are learned during training, and the goal is for the filters to learn to detect certain features of the image, such as edges or corners.\n",
    "\n",
    "In this way, CNNs can accept 2D images as input, rather than a flattened vector. This allows us to preserve the spatial information of the image, which is important for image classification."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72ed4ab60b35ee89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Build the model\n",
    "\n",
    "input = keras.layers.Input(\n",
    "    shape=(28, 28, 1)\n",
    ")  # Instead of a flattened vector, we specify the shape of the input to be a 28x28 image with 1 channel (grayscale)\n",
    "\n",
    "conv1 = Conv2D(32, (3, 3), activation=\"relu\")(\n",
    "    input\n",
    ")  # Our first convolutional layer - uses a 3x3 filter of weights to produce 32 filtered images\n",
    "\n",
    "conv2 = Conv2D(64, (3, 3), activation=\"relu\")(\n",
    "    conv1\n",
    ")  # Our second convolutional layer - uses a 3x3 filter of weights to produce 64 filtered images\n",
    "\n",
    "flatten = Flatten()(\n",
    "    conv2\n",
    ")  # Flatten the output of the convolutional layers into a vector, so we can make a prediction\n",
    "\n",
    "hidden1 = Dense(128, activation=\"relu\")(\n",
    "    flatten\n",
    ")  # Our first hidden layer - takes in the flattened output of the convolutional layers, and outputs 128 values\n",
    "\n",
    "output = Dense(10, activation=\"softmax\")(\n",
    "    hidden1\n",
    ")  # Our output layer - takes in the output of the first hidden layer, and outputs 10 values (one for each possible class)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87ecf04400ccd6cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Like before, we just need to compile the model, specifying the optimizer, loss function, and metrics we want to use. We will use the Adam optimizer, the categorical cross-entropy loss function, and the accuracy metric."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b760c5277736d129"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd97639adbda969c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model - this time we will use adam, a more advanced optimizer based on stochastic gradient descent\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f29690e10c61469e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "431131ac125c58ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {loss:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "361451e1ba958075"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Between swapping out stochastic gradient descent for Adam, and using convolutional layers instead of dense layers, we were able to improve our accuracy by 1.5%!\n",
    "\n",
    "Next, we are going to introduce a couple of 'Callbacks'. These are functions that are called during training. We will use two callbacks: EarlyStopping and ModelCheckpoint. \n",
    "\n",
    "EarlyStopping will stop the training process if the validation loss stops improving. This means that we don't have to specify the number of epochs ahead of time - the model will train until it stops improving, and then stop automatically.\n",
    "\n",
    "ModelCheckpoint will save the best model (the one with the lowest validation loss) to a file. This is useful because we can then load the best model and evaluate it on the test set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4672396c0912e34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5\n",
    ")  # Stop training if the validation loss hasn't improved in 5 epochs\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"best_model\", monitor=\"val_loss\", save_best_only=True\n",
    ")  # Save the model with the lowest validation loss to a folder called 'best_model'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdb546b943490024"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part is the same as before, but we want a new model so we'll run it again:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62f52a580fe63e85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input = keras.layers.Input(shape=(28, 28, 1))\n",
    "conv1 = Conv2D(32, (3, 3), activation=\"relu\")(input)\n",
    "pool1 = MaxPooling2D((2, 2))(\n",
    "    conv1\n",
    ")  # Add a pooling layer to reduce the size of the output of the convolutional layer - this will speed up training\n",
    "conv2 = Conv2D(64, (3, 3), activation=\"relu\")(pool1)\n",
    "pool2 = MaxPooling2D((2, 2))(conv2)  # Add another pooling layer\n",
    "flatten = Flatten()(pool2)\n",
    "hidden1 = Dense(128, activation=\"relu\")(flatten)\n",
    "output = Dense(10, activation=\"softmax\")(hidden1)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f032d8ccac4b96e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can train:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7178b54e8a32faaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "323640a59e8f2be5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = keras.models.load_model(\"best_model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "386f8ce1ff470da9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {loss:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d36dc907d78b2d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we train a model, we can also access the history of the training process. This includes the loss and accuracy on the training and validation sets at each epoch. We can use this to plot the training and validation loss over time, which can help us diagnose problems with our model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f307b24ce97d0925"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f60799572820a09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at some of the incorrect predictions for this model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c6fcafa4b95cbf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the first 5 incorrect predictions\n",
    "predictions = model.predict(x_test)\n",
    "incorrect_indices = np.nonzero(\n",
    "    np.argmax(predictions, axis=1) != np.argmax(y_test, axis=1)\n",
    ")[0]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[incorrect_indices[i]], cmap=plt.cm.binary)\n",
    "    plt.xlabel(\n",
    "        f\"{np.argmax(predictions[incorrect_indices[i]])} (True label: {np.argmax(y_test[incorrect_indices[i]])})\"\n",
    "    )\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5a77c08e9607fac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn\n",
    "\n",
    "Now it's your turn to build a neural network! We will use the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 classes. We will build a neural network to classify these images.\n",
    "\n",
    "First, we will download the dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "274bc85b38f49b06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "# Preview the first 5 images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.xlabel(class_names[y_train[i][0]])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e312f3db8300b074"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train / x_train.max()\n",
    "x_test = x_test / x_test.max()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cafe841f1accaa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6a75ab20cd19419"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the example code above, build a model that can classify these images. You can copy directly one of the model definitions from above, but if you feel like it, you can also extend the network further. You can get more information about available layers and other options in the [Keras documentation](https://keras.io/)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46ffdcb7d2a51821"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Your code here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e8f57dd07bd4407"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
