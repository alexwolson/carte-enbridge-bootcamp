{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 4-2\n",
    "\n",
    "# Understanding the Carbon Cost of Machine Learning\n",
    "\n",
    "With the rise of Large Language Models, there has been a growing discussion about the climate impact of using deep learning. In this lab, we are going to explore the carbon cost of training a model. We will use the [codecarbon](https://github.com/mlco2/codecarbon) library to measure the carbon footprint of a few different machine learning methods."
   ],
   "metadata": {
    "collapsed": false,
    "id": "a5683a009321e576"
   },
   "id": "a5683a009321e576"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q codecarbon pint transformers datasets torch \"accelerate>=0.20.1\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if we are running with a GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "else:\n",
    "    raise Exception('GPU not available - select Runtime -> Change runtime type -> GPU')"
   ],
   "metadata": {
    "id": "YVT9Oa5J-0El",
    "outputId": "ada57b22-0f40-4ac1-c1d6-055fd19fe6a4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "YVT9Oa5J-0El",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!codecarbon init"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bea33321fb4e842",
    "outputId": "c5c36521-8976-4446-9646-c75391547f71"
   },
   "id": "9bea33321fb4e842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "CodeCarbon is a Python library that allows you to measure the carbon footprint of your code. It works by measuring the power consumption of your machine and estimating the carbon emissions associated with that power consumption. It generates a detailed report that includes the carbon footprint of your code, helping us to understand and compare the impact of different models.\n",
    "\n",
    "Let's start by using CodeCarbon to investigate the impact of training a simple linear regression model. We will use the [California Housing dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#california-housing-dataset) from scikit-learn. This dataset contains information about housing prices in California in the 1990s. We will use the median income of the residents to predict the median house value."
   ],
   "metadata": {
    "collapsed": false,
    "id": "7582f100ddafe54"
   },
   "id": "7582f100ddafe54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pint import UnitRegistry\n",
    "import pandas as pd\n",
    "from codecarbon import EmissionsTracker\n",
    "ureg = UnitRegistry()"
   ],
   "metadata": {
    "id": "da21b69715fc0f98"
   },
   "id": "da21b69715fc0f98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "c5e93b5f72f69eb9"
   },
   "id": "c5e93b5f72f69eb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we go any further, let's take a look at the data we're working with. It's always important to understand what we're predicting.\n",
    "\n",
    "This housing dataset tasks us with predicting the median house value in a given area. The dataset contains 8 features:\n",
    "- MedInc: median income in block\n",
    "- HouseAge: median house age in block\n",
    "- AveRooms: average number of rooms\n",
    "- AveBedrms: average number of bedrooms\n",
    "- Population: block population\n",
    "- AveOccup: average house occupancy\n",
    "- Latitude: house block latitude\n",
    "- Longitude: house block longitude\n",
    "\n",
    "The house values are measured in hundreds of thousands of dollars.\n",
    "\n",
    "We will use `Mean Absolute Error` as our evaluation metric. This metric is easy to interpret, as it is in the same units as the target variable. It is also robust to outliers, which is important in this dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ce594fcc9e6d66f4"
   },
   "id": "ce594fcc9e6d66f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1dc750c91486515d",
    "outputId": "7901c045-6c9e-44d5-c178-f80e6a47e3fe"
   },
   "id": "1dc750c91486515d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train[:5]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f035233438acdfae",
    "outputId": "9792e223-1b15-4c9d-88a3-3ecbb6245586"
   },
   "id": "f035233438acdfae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using CodeCarbon's EmissionsTracker is easy. When we want to record the cost of a specific training run, we simply wrap the training code in a with statement. Let's train a linear regression model and see how much carbon it emits."
   ],
   "metadata": {
    "collapsed": false,
    "id": "57498b7137a99e50"
   },
   "id": "57498b7137a99e50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def report_emissions(emissions_tracker: EmissionsTracker):\n",
    "    energy = emissions_tracker.final_emissions_data.energy_consumed\n",
    "    energy = energy * ureg.kilowatt_hour\n",
    "    carbon = emissions_tracker.final_emissions_data.emissions\n",
    "    carbon = carbon * ureg.kilogram\n",
    "    print(f'Carbon emitted:      {carbon.to_compact():~.2f}')\n",
    "    print(f'Energy consumed:     {energy.to_compact():~.2f}')\n"
   ],
   "metadata": {
    "id": "b7af566d112d609"
   },
   "id": "b7af566d112d609"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Wrap the training code in a with statement\n",
    "with EmissionsTracker(project_name=\"Linear Regression\") as tracker:\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "with EmissionsTracker(project_name=\"Linear Regression Prediction\") as predict:\n",
    "    y_hat = model.predict(x_test)"
   ],
   "metadata": {
    "id": "8e9513e7ad6f5663"
   },
   "id": "8e9513e7ad6f5663"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to also record the carbon cost of making a prediction with each of these models. This is for later, when we look at LLMs."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e5cb9827a54c3f06"
   },
   "id": "e5cb9827a54c3f06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_emissions(tracker)\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_hat):.2f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5be959a3d51429d3",
    "outputId": "8f09906e-af24-477a-e186-18c12d2f2f0e"
   },
   "id": "5be959a3d51429d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unsurprisingly, training a simple linear regression model has a very small carbon footprint. Let's see what happens when we train a more complex model. Let's train a large Random Forest model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "d48e76fe356b5392"
   },
   "id": "d48e76fe356b5392"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=None, n_jobs=-1) # Use all cores, any depth\n",
    "with EmissionsTracker(project_name=\"Random Forest\") as tracker:\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "with EmissionsTracker(project_name=\"Random Forest Prediction\") as predict:\n",
    "    y_hat = model.predict(x_test)"
   ],
   "metadata": {
    "id": "255ccbdf76dd6d76"
   },
   "id": "255ccbdf76dd6d76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_emissions(tracker)\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_hat):.2f}')"
   ],
   "metadata": {
    "id": "cc9fe42ea9b7cac2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "418a275e-7d87-4b70-ec13-f16f3d790958"
   },
   "id": "cc9fe42ea9b7cac2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model has improved significantly, but at an increased carbon cost.\n",
    "\n",
    "Something important to note is that we are currently running these experiments in Google Colab. The location of the computing resources powering our code has a significant impact on the carbon footprint of our code. We can check what region the code is running in, using CodeCarbon:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "82a630713645806e"
   },
   "id": "82a630713645806e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Region:         {tracker.final_emissions_data.region}')\n",
    "print(f'Country:        {tracker.final_emissions_data.country_name}')\n",
    "emissions_rate = tracker.final_emissions_data.emissions_rate * ureg.kilogram / ureg.kilowatt_hour\n",
    "print(f'Emissions rate: {emissions_rate.to_compact():~.2f}')"
   ],
   "metadata": {
    "id": "468b05e3b1f1c78f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3fb66d25-bb65-45a5-e273-5cfde07e4afb"
   },
   "id": "468b05e3b1f1c78f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The region you see here is variable, but it's likely to be in the US. In one test, we received the following results:\n",
    "\n",
    "```\n",
    "Region:         oregon\n",
    "Country:        United States\n",
    "Emissions rate: 1.82 Âµg / Wh\n",
    "```\n",
    "\n",
    "Unsurprisingly, if you run this code in Ontario, where we have a much higher proportion of renewable energy, the emissions rate is much lower:\n",
    "\n",
    "```\n",
    "Region:         ontario\n",
    "Country:        Canada\n",
    "Emissions rate: 312.58 ng / Wh\n",
    "```\n",
    "\n",
    "Another important factor to consider is the efficiency of the hardware that we're using. CodeCarbon reports the power consumption of the computing resources we are working on:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "47b1aa4ce1962b01"
   },
   "id": "47b1aa4ce1962b01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'CPU Power: {tracker.final_emissions_data.cpu_power * ureg.watt:~.2f}')\n",
    "print(f'RAM Power:  {tracker.final_emissions_data.ram_power * ureg.watt:~.2f}')"
   ],
   "metadata": {
    "id": "1072cdf453a49c97",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "abd33e12-4332-4338-a210-b7ab0f12a0ae"
   },
   "id": "1072cdf453a49c97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your turn**\n",
    "\n",
    "Before we move on, let's try one more experiment. Choose a machine learning model in [Scikit-Learn](https://scikit-learn.org/stable/supervised_learning.html) and train it on the California Housing dataset. Use CodeCarbon to measure the carbon footprint of your model. How does it compare to the models we've already trained? __Hint: If you aren't sure what model to use, try the [Extra Random Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) model.__"
   ],
   "metadata": {
    "collapsed": false,
    "id": "597e42a47ede943e"
   },
   "id": "597e42a47ede943e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "id": "fcfc87da08c68fb3"
   },
   "id": "fcfc87da08c68fb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Understanding the Carbon Cost of Large Language Models\n",
    "\n",
    "Now that we have a better understanding of how CodeCarbon works, let's use it to investigate the carbon cost of using a large language model.\n",
    "\n",
    "We are going to use HuggingFace to train a version of GPT-2 for a single epoch (i.e. one pass through the training data). We will then use the model to generate some text, and measure the carbon cost of the training and prediction steps.\n",
    "\n",
    "While the training part of this code is quite simple, getting the data ready requires a little bit of effort. We are going to rush through it here, as it isn't the focus of this lab, but the code is commented in case you're interested."
   ],
   "metadata": {
    "collapsed": false,
    "id": "43da7db949aaad9"
   },
   "id": "43da7db949aaad9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\") # Load the raw dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # Load the tokenizer - converts text into numbers\n",
    "\n",
    "# To speed up, let's use half the data\n",
    "dataset['train'] = dataset['train'].select(range(0, len(dataset['train']), 2))\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"]), # Tells this function how to use the tokenizer\n",
    "    batched=True, # Apply to groups of examples\n",
    "    num_proc=4, # Use 4 cores\n",
    "    remove_columns=[\"text\"] # Remove the text column, as we don't need it anymore\n",
    ")\n",
    "\n",
    "block_size = 256 # The maximum number of tokens in a single input\n",
    "\n",
    "# The main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Finally, apply the function above to our data\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")"
   ],
   "metadata": {
    "id": "c87c4a0917dfae99"
   },
   "id": "c87c4a0917dfae99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whew! With that out of the way, we can set up the actual training and measure its carbon cost."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ca8273956a661ddd"
   },
   "id": "ca8273956a661ddd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-wikitext2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1, # Train for one epoch\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=None,\n",
    ")"
   ],
   "metadata": {
    "id": "29497895d19cf94",
    "outputId": "67458459-ad2b-478a-aec7-2131a484cdd8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "29497895d19cf94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with EmissionsTracker(project_name=\"GPT-2 Training\") as tracker:\n",
    "  trainer.train()"
   ],
   "metadata": {
    "id": "4d056d6ba8cc62b2",
    "outputId": "0f1ea54d-4ca3-46ea-b8de-0b3db6572003",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "id": "4d056d6ba8cc62b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "report_emissions(tracker)"
   ],
   "metadata": {
    "id": "d34e826bb1736877",
    "outputId": "e2867baa-c56d-425d-b3ca-4209223c6bf6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d34e826bb1736877"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your turn**\n",
    "\n",
    "Now that we've trained our model, let's use it to generate some text. Use the `generate` method on the `trainer` object to generate some text. You can use the [documentation](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) to help you. Use CodeCarbon to measure the carbon footprint of generating the text. How does it compare to the carbon footprint of training the model? We can also look at the results of all our experiments thus far by running the following command:\n",
    "\n",
    "`pd.read_csv('codecarbon.csv')`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "44c98aa99ad0be3c"
   },
   "id": "44c98aa99ad0be3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "fad4de0f01cd5525"
   },
   "id": "fad4de0f01cd5525"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For reference, one estimate of the carbon cost to train GPT-4 is around 12,500 metric tons of CO2. This is based on the assumption that the model is trained in California, using about 25,000 NVIDIA A100 GPUs. This is the equivalent of the anual emissions of 2,700 cars. This is a lot of carbon, but it's important to remember that this is a one-time cost. Once the model is trained, it can be used by many people, with a much lower carbon cost per user."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e73f4f228f98c9b7"
   },
   "id": "e73f4f228f98c9b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, we have explored the carbon cost of training a machine learning model. We have seen that the cost of training a large language model is substantially higher than a traditional machine learning algorithm. There are a number of ways that we can try to reduce carbon emissions in machine learning:\n",
    "\n",
    "- **Use more efficient hardware**: The hardware we use to train our models has a significant impact on the carbon footprint of our code. Using more efficient hardware, such as GPUs, can reduce the carbon footprint of our code.\n",
    "- **Use more efficient algorithms**: Some algorithms are more efficient than others. For example, linear regression is much more efficient than a large language model.\n",
    "- **Run code in regions with renewable energy**: The location of the computing resources powering our code has a significant impact on the carbon footprint of our code. Running our code in regions with a high proportion of renewable energy can reduce the carbon footprint of our code.\n",
    "- **Train less often**: Training a model has a much higher carbon cost than using it. If we are careful about how often we train our models, we can spread the carbon cost over a longer period of time.\n",
    "- **Use smaller models**: Large language models are very powerful, but they also have a high carbon cost. If we can use a smaller model, we can reduce the carbon footprint of our code."
   ],
   "metadata": {
    "collapsed": false,
    "id": "6ae6664da985350d"
   },
   "id": "6ae6664da985350d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
