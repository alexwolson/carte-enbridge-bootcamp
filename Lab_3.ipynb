{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46dc0f1c81b0784e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictive Maintenance\n",
    "\n",
    "In this lab, we are going to take a look at using machine learning to predict when a machine will fail, so you can perform maintenance before it does, or better understand the factors that lead to failure. This is called *predictive maintenance*, and it is a common application of machine learning in industry.\n",
    "\n",
    "## The Data\n",
    "\n",
    "The data we are going to use is a synthetic (simulated) dataset modelled after a milling machine used in industrial applications. It was created to assist in measuring model performance for a scientific conference. The original dataset is available [here](https://www.kaggle.com/datasets/stephanmatzka/predictive-maintenance-dataset-ai4i-2020). The data is available as a CSV file, which we will read into a Pandas dataframe:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b7f671a91c25c31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/alexwolson/carte_workshop_datasets/raw/main/ai4i2020.csv.zip\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "567a37c2f563f31b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the data includes a number of features that might be useful in predicting when a machine is likely to fail. The **UDI** column is a unique identifier for each machine, and the **Product ID** column identifies the specific model of machine. The remaining columns (**Air temperature**, **Process temperature**, **Rotational speed**, **Torque**, **Tool wear**, and **Machine failure**) are numeric values that indicate the machine's operating characteristics at a point in time. The **Machine failure** column indicates whether the machine failed in the following hour (1) or not (0). The remaining fields represent different modes of failure:\n",
    "\n",
    "- TWF: Tool Wear Failure\n",
    "- HDF: Heat Dissipation Failure\n",
    "- PWF: Power Failure\n",
    "- OSF: Overspeed Failure\n",
    "- RNF: Random Failure\n",
    "\n",
    "We are going to begin by just predicting whether the machine is going to fail or not, but we can come back to the specific failure modes later.\n",
    "\n",
    "Let's start by pre-processing our data. We are going to rely on many helpful functions from the [scikit-learn](https://scikit-learn.org/stable/) library, so let's import that now:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "524ac297f8c22958"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6279abdb53ba3d27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to split our data into features and labels. The **UDI** and **Product ID** columns are not useful for predicting machine failure, so we can drop them. The **Machine failure** column is our target, so we can drop that from the features and assign it to a separate variable:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "103a8bd680013090"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = data[\"Machine failure\"]\n",
    "features = data.drop(\n",
    "    [\"UDI\", \"Product ID\", \"Machine failure\", \"TWF\", \"HDF\", \"PWF\", \"OSF\", \"RNF\"], axis=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14261cb23b9ad2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `type` column is a categorical value, so we need to encode it numerically. We can use the `LabelEncoder` class from scikit-learn to do this. Like encoding that we've done before, `LabelEncoder` will assign a unique value to each category, and replace the original value."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c136075f00e56871"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(features[\"Type\"])\n",
    "features[\"Type\"] = le.transform(features[\"Type\"])\n",
    "\n",
    "features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "383610e29d822fc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's often helpful to visualize our features to get a sense of what pre-processing we might need to do. We can use the `hist` method of the Pandas dataframe to plot histograms of each feature. It conveniently ignores the categorical features, so we don't have to worry about those, and generates plots for each of the numerical features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac4ab20ecd4eb44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features.hist(figsize=(12, 12));"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd108d913d6ed82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The features are all on different scales, so normalizing them will help to ensure that they get considered with equal weight in the model. Scikit-Learn provides a `MinMaxScaler` class that will scale our numeric features to a value between 0 and 1:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "627412e1824cbb54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "features = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "features.hist(figsize=(12, 12));"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69f9f929f12a67d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking good! Now we have our features and labels prepared, we can split them into training and test sets. We'll use our old friend the `train_test_split` function from scikit-learn to do this. We'll use 20% of the data for testing, and the remaining 80% for training. We'll also set the `random_state` parameter to 0 so that we get the same split every time we run the code. This is useful for reproducibility."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa46d0583866f30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "print(\"Training Set: %d, Test Set: %d \\n\" % (x_train.shape[0], x_test.shape[0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43bef34dfa0d8eb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a Classification Model\n",
    "\n",
    "Scikit-Learn includes a variety of machine learning algorithms that we can use to train a classification model. Let's try a simple logistic regression model first. We are going to measure the time it takes each of our models to train today, so we can compare them later."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e7eaac286ec788"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "\n",
    "lr = LogisticRegression()\n",
    "start_time = time()\n",
    "lr.fit(x_train, y_train)\n",
    "print(f'Training time: {time() - start_time:.2f}s')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c034bcf553b386"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The way that we measure the performance of a model is dependent on the task that we are performing. When it comes to predictive maintenance, it's probably more important that we don't miss a machine failure than it is to avoid false alarms. In other words, we want to minimize the number of *false negatives* (predicting that the machine will not fail in the next hour when it actually does) rather than the number of *false positives* (predicting that the machine will fail when it actually doesn't). \n",
    "\n",
    "We can calculate these metrics directly:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "906109f19e17eca3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_negatives_metrics(x, y, model):\n",
    "    y_hat = model.predict(x)\n",
    "    false_negatives = np.logical_and((y_hat == 0), (y == 1)).sum()\n",
    "    false_positives = np.logical_and((y_hat == 1), (y == 0)).sum()\n",
    "    return false_negatives, false_positives"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dee9e2442c34ef3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "false_negatives, false_positives = get_negatives_metrics(x_test, y_test, lr)\n",
    "print(f\"False Negatives: {false_negatives}, False Positives: {false_positives}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1511a7bd51e3729f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the Logistic Regression model has way too many false negatives to be useful for predictive maintenance. Another way to look at this is using a balanced accuracy score. While the standard accuracy metric is the number of correct predictions divided by the total number of predictions, the balanced accuracy score is the average of the recall obtained on each class. In other words, it's the average of the true positive rate and the true negative rate. We can calculate this using the `balanced_accuracy_score` function from scikit-learn:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb811035f4806e83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "print(\n",
    "    f\"Balanced accuracy: {balanced_accuracy_score(y_test, lr.predict(x_test)) * 100:.2f}%\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5913a499ca715da9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "50% accuracy for a binary task means that our model is no better than a coin flip. Clearly, this Logistic Regression model is not going to be useful for predictive maintenance. Let's try a more sophisticated algorithm - a decision tree. In order to help with visualization, we're going to limit the depth of the tree to 3 - this means that the tree will only ask 3 questions about the data before making a prediction. We'll also measure the training time again."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c19e92b4601a278"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "start_time = time()\n",
    "dt.fit(x_train, y_train)\n",
    "print(f'Training time: {time() - start_time:.2f}s')\n",
    "\n",
    "false_negatives, false_positives = get_negatives_metrics(x_test, y_test, dt)\n",
    "print(f\"False Negatives: {false_negatives}, False Positives: {false_positives}\")\n",
    "print(\n",
    "    f\"Balanced accuracy: {balanced_accuracy_score(y_test, dt.predict(x_test)) * 100:.2f}%\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca91dfc90b4d55d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're getting better! While the number of false positives did increase, the number of false negatives _decreased_ dramatically. Since that's what we care about, this is a big improvement.\n",
    " \n",
    "One good thing about decision trees is that it's easy to follow the logic that the model is using to make predictions. We can visualize the decision tree using the `plot_tree` function from scikit-learn:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ab1547fc6dc6701"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(\n",
    "    dt,\n",
    "    feature_names=features.columns,\n",
    "    class_names=[\"No Failure\", \"Failure\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    ");"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cba9da2016028127"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The tree plot shows us how the model is making predictions. Each node represents a \"question\" being asked about the data. For instance, the first node is `Torque[Nm] <= 0.785`. For a given sample, if the measured torque is less than or equal to 0.785Nm, we follow the left branch, otherwise we follow the right branch. \n",
    "\n",
    "The `gini` value is a measure that indicates how many samples in each node are in a single class. A `gini` value of 0 means that all of the samples in the node are in the same class, while a value of 0.5 means that the samples are evenly split between the two classes. This is actually the same gini coefficient that is used to measure income inequality in economics.\n",
    "\n",
    "**Your Turn**\n",
    "\n",
    "Try changing the `max_depth` parameter of the decision tree model to see if you can improve the balanced accuracy score.\n",
    "\n",
    "---\n",
    "\n",
    "Let's try one more algorithm: a random forest. A random forest is an ensemble of decision trees, which means that it combines the predictions of multiple decision trees to improve the overall performance. It's typically more accurate than a single decision tree, but it's harder to interpret, since we can't visualize the entire forest.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f6f4ab39bb1a08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "start_time = time()\n",
    "rf.fit(x_train, y_train)\n",
    "print(f'Training time: {time() - start_time:.2f}s')\n",
    "\n",
    "false_negatives, false_positives = get_negatives_metrics(x_test, y_test, rf)\n",
    "print(f\"False Negatives: {false_negatives}, False Positives: {false_positives}\")\n",
    "print(\n",
    "    f\"Balanced accuracy: {balanced_accuracy_score(y_test, rf.predict(x_test)) * 100:.2f}%\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91d65a151ffd5088"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your Turn**\n",
    "\n",
    "Is this better or worse than the decision tree? How do we decide?\n",
    "\n",
    "## Interpreting the Model\n",
    "\n",
    "One of the challenges with machine learning is that it can be difficult to understand how the model is making predictions. This is especially true for more complex algorithms, such as random forests. \n",
    "\n",
    "One way to get a sense of how the model is making predictions is to look at the feature importances. The feature importances indicate how much each feature contributes to the overall prediction. \n",
    "\n",
    "We can get the feature importances from the random forest model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be15221d4531a1e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(\n",
    "    rf.feature_importances_, index=x_train.columns, columns=[\"importance\"]\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "feature_importances.plot.bar();"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cdc5fa7025f123"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the `Torque [Nm]` feature is the most important in the model. Least important is the `Type` feature. Investigating feature importance is useful to understand which measurements are worth taking, and which might not be.\n",
    "\n",
    "Another way to understand how the model is making predictions is to look at the partial dependence of the model on each feature. The partial dependence is the marginal effect of a feature on the predicted outcome. In other words, it shows us how the probability of failure changes as we change the value of a feature."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28031536a9ac1588"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "disp = PartialDependenceDisplay.from_estimator(rf, x_train, [\"Torque [Nm]\"], ax=ax)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7de9f7983285aef4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This plot shows us that as the torque increases, the probability of failure increases. Go ahead and try plotting the partial dependence for some of the other features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c8954adb79f87f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting Failure Modes\n",
    "\n",
    "So far, we have only been predicting whether the machine will fail or not. Let's see if we can predict the specific failure mode. We are going to use the random forest model that we trained above, but we are going to add the failure mode columns back into the features:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f9c209e4e5cdd91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = data[[\"TWF\", \"HDF\", \"PWF\", \"OSF\", \"RNF\"]]\n",
    "features = data.drop(\n",
    "    [\"UDI\", \"Product ID\", \"Machine failure\", \"TWF\", \"HDF\", \"PWF\", \"OSF\", \"RNF\"], axis=1\n",
    ")\n",
    "\n",
    "# Encode the Type column\n",
    "features[\"Type\"] = le.transform(features[\"Type\"])\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "features = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train_modes, x_test_modes, y_train_modes, y_test_modes = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90c9326ac561feea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "start_time = time()\n",
    "rf.fit(x_train_modes, y_train_modes)\n",
    "print(f'Training time: {time() - start_time:.2f}s')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "898a70c861740106"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we now have 5 different failure modes, we need to use a different metric to evaluate the model. We can use the balanced accuracy score for each failure mode, and then average them together to get an overall score:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44fc9a51c07db13a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "y_hat = rf.predict(x_test_modes)\n",
    "accuracies = []\n",
    "for i, confusion in enumerate(multilabel_confusion_matrix(y_test_modes, y_hat)):\n",
    "    print(labels.columns[i])\n",
    "    print(f\"False Negatives: {confusion[1, 0]}\")\n",
    "    print(f\"False Positives: {confusion[0, 1]}\")\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test_modes.iloc[:, i], y_hat[:, i])\n",
    "    accuracies.append(balanced_accuracy)\n",
    "    print(\n",
    "        f\"Balanced accuracy: {balanced_accuracy * 100:.2f}%\"\n",
    "    )\n",
    "    print()\n",
    "    \n",
    "print(f\"Average balanced accuracy: {np.mean(accuracies) * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5c57fa53a8b557c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like we are doing well at predicting the `HDF`, `PWF` and `OSF` failure modes, but not so well at predicting the `TWF` and `RNF` failure modes.\n",
    "\n",
    "**Your Turn**\n",
    "\n",
    "Try changing the parameters of the random forest model to see if you can improve the balanced accuracy score for the `TWF` and `RNF` failure modes. If you'd like to go further, you could also try using a different ensemble method. For documentation on the ensemble methods available in scikit-learn, see [here](https://scikit-learn.org/stable/modules/ensemble.html)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463fc76c32b5f595"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2fcf53ab08bc290"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus: Testing Explainable Boosting Machines\n",
    "\n",
    "As we discussed in the lecture, Explainable Boosting Machines (EBMs) are a relatively new machine learning algorithm that is designed to be more interpretable than other algorithms. Let's see if we can use an EBM to predict machine failure. We are going to use the [interpret](https://interpret.ml/) library from Microsoft to train the EBM, then we can compare its performance to the random forest model that we trained above."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a1fc620652be8f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q -U interpret"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6cbedb719ea027a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "ebm = ExplainableBoostingClassifier()\n",
    "start_time = time()\n",
    "ebm.fit(x_train, y_train)\n",
    "print(f'Training time: {time() - start_time:.2f}s')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4659043edef3f0ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "false_negatives, false_positives = get_negatives_metrics(x_test, y_test, ebm)\n",
    "print(f\"False Negatives: {false_negatives}, False Positives: {false_positives}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e4bad66ff2440a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "One great feature of Microsoft's interpret library is that it includes a dashboard that we can use to visualize the model's performance. We can use the `show` method of the `ExplanationDashboard` class to launch the dashboard:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b12c114e50f049bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "\n",
    "ebm_global = ebm.explain_global()\n",
    "\n",
    "show(ebm_global)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c84621d032f26a09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "26228ff18eb66ffe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
