{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 5-0\n",
    "\n",
    "# Building a GPT model from scratch"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a4cba22cdd1889e"
   },
   "id": "a4cba22cdd1889e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we are going to build a very simple version of GPT. Our GPT will have a small vocabulary and a small number of layers. Let's begin by importing the necessary libraries."
   ],
   "metadata": {
    "collapsed": false,
    "id": "b1108de7761cb146"
   },
   "id": "b1108de7761cb146"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check that we are using a GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    raise SystemError('GPU device not found! Enable GPU by going to Runtime > Change runtime type > GPU')"
   ],
   "metadata": {
    "id": "109498fd48e49b4e",
    "outputId": "cb42e057-471f-4a0c-80f1-978f5ea11e9e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "109498fd48e49b4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "At its core, GPT is a model to predict the next word in a sequence. In order to be able to learn, we need to first convert words into values that can be fed into the model.\n",
    "\n",
    "We are going to load a dataset of samples from [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page). This is a version of Wikipedia that aims to cover the same content, but using a reduced vocabulary and simpler grammar. This makes it easier for language learners to understand. We will use this dataset to train our model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "de23c7596a50c73"
   },
   "id": "de23c7596a50c73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/alexwolson/carte_workshop_datasets/main/corpus.txt.zip\"\n",
    "\n",
    "# Stream the download so we can track its progress\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# Total size in bytes.\n",
    "total_size = int(response.headers.get('content-length', 0))\n",
    "block_size = 1024  # 1KB\n",
    "progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "with open('corpus.txt.zip', 'wb') as file:\n",
    "    for data in response.iter_content(block_size):\n",
    "        progress_bar.update(len(data))\n",
    "        file.write(data)\n",
    "progress_bar.close()\n",
    "\n",
    "if total_size != 0 and progress_bar.n != total_size:\n",
    "    print(\"ERROR, something went wrong\")\n",
    "\n",
    "# Now we will extract the zip file\n",
    "z = zipfile.ZipFile('corpus.txt.zip')\n",
    "z.extractall()\n",
    "\n",
    "# Read the first 1000 characters from the corpus\n",
    "with open('corpus.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(corpus[:1000])"
   ],
   "metadata": {
    "id": "6bea0f8983a1dc85",
    "outputId": "ad740ffa-a810-4826-8f45-eb5b7cd2aebd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "6bea0f8983a1dc85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we want our model to be very simple, we are going to determine the 500 most common words, and use those as our vocabulary."
   ],
   "metadata": {
    "collapsed": false,
    "id": "79f68b6893648c0"
   },
   "id": "79f68b6893648c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from re import sub\n",
    "\n",
    "# Strip out all punctuation and numbers\n",
    "corpus = sub(r'[^\\w\\s]', '', corpus)\n",
    "corpus = sub('\\n', ' ', corpus)\n",
    "corpus = sub(r'\\d+', '', corpus)\n",
    "corpus = sub('  ', ' ', corpus)\n",
    "\n",
    "words = corpus.lower().split(' ')\n",
    "word_counts = Counter(words)\n",
    "\n",
    "vocab_size = 500\n",
    "most_common_words = word_counts.most_common(vocab_size+1)\n",
    "most_common_words = [word for word, count in most_common_words if word != '']\n",
    "print(most_common_words)"
   ],
   "metadata": {
    "id": "4a2455193456af50",
    "outputId": "44d59159-f246-494b-9bee-ca176a564312",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "4a2455193456af50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our reduced vocabulary, we will now take our dataset and strip out all words that are not in our vocabulary. Because we are dropping a LOT of words, we are going to keep only segments that are at least 6 words long."
   ],
   "metadata": {
    "collapsed": false,
    "id": "514d75e42b331ef4"
   },
   "id": "514d75e42b331ef4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context_length = 5\n",
    "new_corpus = []\n",
    "phrase = []\n",
    "for word in tqdm(words):\n",
    "    if word in most_common_words:\n",
    "        phrase.append(word)\n",
    "    elif len(phrase) >= context_length+1:\n",
    "        new_corpus.append(' '.join(phrase))\n",
    "        phrase = []\n",
    "    else:\n",
    "        phrase = []\n",
    "\n",
    "    if len(phrase) >= context_length+1:\n",
    "        new_corpus.append(' '.join(phrase))\n",
    "        phrase = []"
   ],
   "metadata": {
    "id": "bb5a1f3e60dd3b2d",
    "outputId": "fb9c132b-6aa1-4394-e17d-383bcc1666e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "bb5a1f3e60dd3b2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "new_corpus = list(set(new_corpus))"
   ],
   "metadata": {
    "id": "d4add13895c71355"
   },
   "id": "d4add13895c71355"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(new_corpus[:10])"
   ],
   "metadata": {
    "id": "54ef06ded893198e",
    "outputId": "7aa811d9-2aff-47c7-9eb5-94e2a43ccf8d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "54ef06ded893198e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fantastic! We now have a dataset of grammatical six-word phrases. Next, we need to encode our words into values, so that we can feed them into our model:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c8755a12484db33a"
   },
   "id": "c8755a12484db33a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words_to_int = {word: i for i, word in enumerate(most_common_words)}\n",
    "int_to_words = {i: word for i, word in enumerate(most_common_words)}"
   ],
   "metadata": {
    "id": "4068aa9e80c52455"
   },
   "id": "4068aa9e80c52455"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can encode any sentence (as long as it's made up of words in our vocabulary) into a sequence of integers:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c871e261116f2054"
   },
   "id": "c871e261116f2054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode(sentence):\n",
    "    return [words_to_int[word] for word in sentence.split(' ')]\n",
    "\n",
    "def encode_one_hot(word):\n",
    "    return [1 if i == words_to_int[word] else 0 for i in range(vocab_size)]\n",
    "\n",
    "def decode(sequence):\n",
    "    return ' '.join([int_to_words[i] for i in sequence])\n",
    "\n",
    "def decode_one_hot(word):\n",
    "    return int_to_words[np.argmax(word)]\n",
    "\n",
    "encoded = encode('all of the people')\n",
    "print(encoded)\n",
    "print(decode(encoded))"
   ],
   "metadata": {
    "id": "b6e34b9b06fd0b8d",
    "outputId": "d5443e05-95be-4990-bffc-c9198fa6bcd8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "b6e34b9b06fd0b8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a way to convert words into integers, we can create our training data. We will use the first 5 words in a sequence to predict the 6th word. For example, given the sequence \"all of the people in the\", we will use \"all of the people in\" to predict \"the\". We will do this for every sequence in our dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "3849592cbcfba197"
   },
   "id": "3849592cbcfba197"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence in new_corpus:\n",
    "    words = sentence.split(' ')\n",
    "    for i in range(len(words)-context_length):\n",
    "        X.append(encode(' '.join(words[i:i+context_length])))\n",
    "        y.append(encode_one_hot(words[i+context_length]))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "for i in range(10):\n",
    "    print(decode(X_train[i]), '->', decode_one_hot(y_train[i]))"
   ],
   "metadata": {
    "id": "4b33800e48cab7b5",
    "outputId": "5c3adcb7-5069-4150-cc68-5c9f084d00e2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "4b33800e48cab7b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's use Keras to build our model. At its simplest, GPT has the following structure:\n",
    "\n",
    "1. An embedding layer that converts each word into a vector\n",
    "2. A transformer block\n",
    "3. A linear layer that converts the output of the transformer blocks into a vector of probabilities for each word in the vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "_Optional math:_\n",
    "\n",
    "The transformer is a key concept in GPT. At its core, you can think of a single transformer block as an equivalent to a layer of neurons, but with a more complex architecture. The transformer block is made up of two parts:\n",
    "\n",
    "1. Multi-head attention\n",
    "2. A standard fully-connected layer\n",
    "\n",
    "Multi-head attention is a way of combining information from different parts of the input. \"Multi-head\" really just means that we do this multiple times and combine the results. Attention can be thought of as a replacement for a standard neuron - instead of taking in all the inputs and combining them based on a single set of weights, we instead learn three different sets of weights and combine them in a more complex way. So instead of our neuron working like this:\n",
    "\n",
    "$$ y = activation(WX) $$\n",
    "\n",
    "It works like this:\n",
    "\n",
    "$$ y = activation(\\frac{W_1X*W_2X}{\\sqrt{size(X)}}) * W_3X $$\n",
    "\n",
    "If that seems confusing, don't worry - it's a very new concept in deep learning and we aren't explaining it in much detail. The real takeaway is that we are replacing our standard neuron with its one set of parameters, with a more complex neuron that has three sets of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "Each word in our vocabulary will be transformed into a vector of size `8` that will be learned by the model. We will use two transformer blocks, each with two heads. The feedforward layer will have 32 neurons. The output of the feedforward layer will be flattened, and then fed into a linear layer that will output a vector of size `vocab_size`. This vector will be a probability distribution over the words in our vocabulary. We will use the `softmax` activation function to ensure that the output is a valid probability distribution."
   ],
   "metadata": {
    "collapsed": false,
    "id": "4d9337a15522ef33"
   },
   "id": "4d9337a15522ef33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_size = len(words_to_int)\n",
    "embedding_size = 8\n",
    "num_heads = 2\n",
    "num_transformer_blocks = 2\n",
    "feedforward_dim = 32\n",
    "\n",
    "inputs = keras.layers.Input(shape=(context_length,)) # Take in three words\n",
    "embedding_layer = keras.layers.Embedding(vocab_size, embedding_size)(inputs) # Convert each word to a vector\n",
    "transformer_block = keras.layers.MultiHeadAttention(num_heads, embedding_size)(embedding_layer, embedding_layer) # Apply multi-head attention, aka transformer block\n",
    "transformer_block = keras.layers.MultiHeadAttention(num_heads, embedding_size)(transformer_block, transformer_block) # Apply multi-head attention again\n",
    "transformer_block = keras.layers.Dense(feedforward_dim, activation='relu')(transformer_block) # Feedforward layer\n",
    "transformer_flattened = keras.layers.Flatten()(transformer_block) # Flatten the output - we currently get 3 vectors because we have 3 words\n",
    "outputs = keras.layers.Dense(vocab_size, activation='softmax')(transformer_flattened) # Output probabilities for each word in the vocabulary\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "9ea317afb44f1e21",
    "outputId": "b67f8119-a3db-42f2-f7be-e83a6d87201c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "9ea317afb44f1e21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is our own microscopic GPT! Our model has 85,924 trainable parameters - GPT 3.5 has 154 billion. Let's train our model on our dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9b6b32ae8b6a7571"
   },
   "id": "9b6b32ae8b6a7571"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "fed21afb192e7120",
    "outputId": "7167c912-94d4-4298-8bc7-4850d09724f0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "fed21afb192e7120"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, our accuracy is not very good. It picks the right word something like 1 in 5 times. That's still a lot better than random, which would be 1 in 500, but it's not enough to be useful in practice. This is because our model is very small, and so is our dataset. However, the principles we are using here are exactly the same as in GPT - just on a smaller scale. Let's see how our model performs on some sample sentence:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "21ba53e8835e1b32"
   },
   "id": "21ba53e8835e1b32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_sentence = 'the united states is one'\n",
    "sample_sentence_encoded = encode(sample_sentence)\n",
    "print(sample_sentence_encoded)\n",
    "predictions = model.predict(np.array([sample_sentence_encoded]))\n",
    "print(decode_one_hot(predictions[0]))"
   ],
   "metadata": {
    "id": "5cf1c9ce7ceb0bdd",
    "outputId": "a8720769-10dd-44e1-9a0f-63d15b45dc01",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "5cf1c9ce7ceb0bdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we talk about models like GPT-3.5, we talk about the 'context window'. This is the number of tokens we feed into the model to get one word out. In our case, our context window is 5. In GPT-3.5, the context window is 4096, or 16384, depending on the model. The latest version of GPT-4 supports a context window of up to 128,000 tokens - as much as 300 pages of text. No matter what the context is, we are only getting one word out - if we want to produce a larger sequence, we have to successively feed the output back into the model. This is called 'autoregressive generation'. We can use our model to generate a sequence of words like this:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "e220f26dfb10b515"
   },
   "id": "e220f26dfb10b515"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_sequence(model, context, length):\n",
    "    result = context\n",
    "    for i in range(length):\n",
    "        predictions = model.predict(np.array([context]))\n",
    "        context = np.append(context, np.argmax(predictions[0]))\n",
    "        result = np.append(result, np.argmax(predictions[0]))\n",
    "        context = context[1:]\n",
    "    return result\n",
    "\n",
    "print(decode(generate_sequence(model, encode('in the way of the'), 10)))"
   ],
   "metadata": {
    "id": "4c386e6f93a7434b",
    "outputId": "472d6232-2513-4eac-d6e6-e6f6221d45b8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "4c386e6f93a7434b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is precisely how models like ChatGPT generate text. They take in the context (which, as we discussed, is typically much longer than 5 words) and generate the next word. However, unlike our case, where we choose a fixed number of words to generate, ChatGPT keeps generating words until it reaches a special token that marks the end of a sequence (often `<end>`). This is how it can generate text of arbitrary length.\n",
    "\n",
    "Now we have created our very own GPT model. But this is not the same as ChatGPT. Models like ChatGPT go one step further, to make the model more useful for conversation. This is done using a technique called Reinforcement Learning from Human Feedback (RLHF).\n",
    "\n",
    "RLHF expands on the training process we've seen above by adding a second model, called the discriminator or the adversary. The role of the adversary is to rate the quality of a response based on some conditions that we care about. In the case of ChatGPT, the adversary is looking for things like whether the response fits the conversational style, and whether it avoids sensitive topics. The adversary is trained using _human feedback_ - humans rate the quality of responses, and the adversary learns to predict the human rating. The adversary is then used to train the generator (the GPT model) - the generator is rewarded for producing responses that the adversary rates highly. This is called adversarial training, and it is a very powerful technique for training models.\n",
    "\n",
    "We are going to make our own extremely simple adversary. Our adversary will assign a score to the response based on how many times the letter 'e' appears in the response. We will then use this score to train our generator. This is a very simple example, but it demonstrates the principle of adversarial training.\n",
    "\n",
    "Because we can directly calculate how many 'e's appear in each of our vocabulary words, we don't need to 'train' our adversary - we can just use it directly. We will use the following function to calculate the adversary score of a word:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "93868f6cfe2e9955"
   },
   "id": "93868f6cfe2e9955"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adversary_scores = [word.count('e') for word in most_common_words]\n",
    "def adversary_score(y_true, y_pred):\n",
    "    return tf.reduce_sum(y_pred * adversary_scores, axis=-1)"
   ],
   "metadata": {
    "id": "edeae9d23913c798"
   },
   "id": "edeae9d23913c798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_sentence = \"in the same way i\"\n",
    "sample_sentence_encoded = encode(sample_sentence)\n",
    "prediction = model.predict(np.array([sample_sentence_encoded]))[0]\n",
    "print(decode_one_hot(prediction))\n",
    "print(adversary_score(None, prediction))"
   ],
   "metadata": {
    "id": "ad51ae2e0a7fb1e2",
    "outputId": "448acb96-48cf-4747-96ff-0227aeb31b14",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "ad51ae2e0a7fb1e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you should be able to see, the adversary assigns a score greater than zero even if the predicted word doesn't have any 'e's in it. This is because the predicted word is not a one-hot vector - it is a probability distribution. The adversary is assigning a score to the entire distribution, not just the most likely word. This is valuable because we typically avoid methods which can produce zero as an error - in a nutshell, if the error is zero, the model doesn't know how to change things in order to improve. Our approach instead will incentivize the model to consider all words containing 'e's more strongly."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c7c08535fa523049"
   },
   "id": "c7c08535fa523049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_sentence = \"in the same way i\"\n",
    "sample_sentence_encoded = encode(sample_sentence)\n",
    "prediction = model.predict(np.array([sample_sentence_encoded]))[0]\n",
    "print(\"Top 10 most likely words\")\n",
    "print(\"Word       | Chance \\t| e count | Adversary score\")\n",
    "for i in sorted(zip(prediction, most_common_words), reverse=True)[:10]:\n",
    "    print(f'{i[1]:10} | {i[0]*100:.0f}% \\t| {i[1].count(\"e\")}       | {i[0]*i[1].count(\"e\"):.4f}')"
   ],
   "metadata": {
    "id": "e6c725e058889c45",
    "outputId": "f55454a2-ca26-4379-dc69-0d06167c1265",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "e6c725e058889c45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "    adversary_weight = 0.75 # Modify this to increase or decrease the influence of the adversary\n",
    "    return tf.losses.categorical_crossentropy(y_true, y_pred) - adversary_weight * adversary_score(y_true, y_pred)\n",
    "\n",
    "# Duplicate the model\n",
    "model_adversary = keras.models.clone_model(model)\n",
    "\n",
    "model_adversary.compile(optimizer='adam',\n",
    "              loss=combined_loss,\n",
    "              metrics=['accuracy', adversary_score])"
   ],
   "metadata": {
    "id": "38f41c757aab2c61"
   },
   "id": "38f41c757aab2c61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_adversary.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "f11a8845dadb63d1",
    "outputId": "278e8131-f00c-4bc5-9050-9a72ff543c57",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "f11a8845dadb63d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compare the predictions of our original model and our adversary model:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "74d84484ed9cb21c"
   },
   "id": "74d84484ed9cb21c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_predictions = model.predict(X_test)\n",
    "adversary_predictions = model_adversary.predict(X_test)\n",
    "\n",
    "print(f'Adversary score for original model: {np.mean(adversary_score(None, original_predictions))}')\n",
    "print(f'Adversary score for adversary model: {np.mean(adversary_score(None, adversary_predictions))}')\n",
    "\n",
    "for i in range(10):\n",
    "    true_word = decode_one_hot(y_test[i])\n",
    "    original_word = decode_one_hot(original_predictions[i])\n",
    "    adversary_word = decode_one_hot(adversary_predictions[i])\n",
    "    print(f'{true_word:10} | {original_word:10} | {adversary_word:10}')"
   ],
   "metadata": {
    "id": "378821759b649a5d",
    "outputId": "33cba925-1c85-4890-d8ba-21d1b57b1936",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "378821759b649a5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, introducing the adversary has dramatically increased the model's likelihood to choose words with lots of 'e's in them. Of course, in this setting, that's at a cost to the model's accuracy. However, in a real-world setting, we would use a more sophisticated adversary, and we would use a more sophisticated metric than just accuracy.\n",
    "\n",
    "So there you have it! We have built our own GPT model, and we have seen how we can use an adversary to obtain specific behaviour. Of course, there are many more details that go into building a model like ChatGPT, but this is the core of it. I hope you enjoyed this tutorial!"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6584ddbdaa8b325f"
   },
   "id": "6584ddbdaa8b325f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
