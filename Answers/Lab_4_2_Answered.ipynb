{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 4-2\n",
    "\n",
    "# Understanding the Carbon Cost of Machine Learning\n",
    "\n",
    "With the rise of Large Language Models, there has been a growing discussion about the climate impact of using deep learning. In this lab, we are going to explore the carbon cost of training a model. We will use the [codecarbon](https://github.com/mlco2/codecarbon) library to measure the carbon footprint of a few different machine learning methods."
   ],
   "metadata": {
    "collapsed": false,
    "id": "a5683a009321e576"
   },
   "id": "a5683a009321e576"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:33.213735848Z",
     "start_time": "2023-11-09T21:38:30.692410932Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U -q codecarbon pint transformers datasets torch \"accelerate>=0.20.1\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if we are running with a GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "else:\n",
    "    raise Exception('GPU not available - select Runtime -> Change runtime type -> GPU')"
   ],
   "metadata": {
    "id": "YVT9Oa5J-0El",
    "outputId": "ada57b22-0f40-4ac1-c1d6-055fd19fe6a4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:34.102467535Z",
     "start_time": "2023-11-09T21:38:33.213500541Z"
    }
   },
   "id": "YVT9Oa5J-0El",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Welcome to CodeCarbon, here is your experiment id:\r\n",
      "\u001B[92m7da01208-8255-4211-92aa-4cb84f2ff963\u001B[0m (from \u001B[94m./.codecarbon.config\u001B[0m)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!codecarbon init"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bea33321fb4e842",
    "outputId": "c5c36521-8976-4446-9646-c75391547f71",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:34.747290281Z",
     "start_time": "2023-11-09T21:38:34.111395598Z"
    }
   },
   "id": "9bea33321fb4e842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "CodeCarbon is a Python library that allows you to measure the carbon footprint of your code. It works by measuring the power consumption of your machine and estimating the carbon emissions associated with that power consumption. It generates a detailed report that includes the carbon footprint of your code, helping us to understand and compare the impact of different models.\n",
    "\n",
    "Let's start by using CodeCarbon to investigate the impact of training a simple linear regression model. We will use the [California Housing dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#california-housing-dataset) from scikit-learn. This dataset contains information about housing prices in California in the 1990s. We will use the median income of the residents to predict the median house value."
   ],
   "metadata": {
    "collapsed": false,
    "id": "7582f100ddafe54"
   },
   "id": "7582f100ddafe54"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pint import UnitRegistry\n",
    "import pandas as pd\n",
    "from codecarbon import EmissionsTracker\n",
    "ureg = UnitRegistry()"
   ],
   "metadata": {
    "id": "da21b69715fc0f98",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:35.696442067Z",
     "start_time": "2023-11-09T21:38:34.749429806Z"
    }
   },
   "id": "da21b69715fc0f98"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "c5e93b5f72f69eb9",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:35.709332889Z",
     "start_time": "2023-11-09T21:38:35.698456106Z"
    }
   },
   "id": "c5e93b5f72f69eb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we go any further, let's take a look at the data we're working with. It's always important to understand what we're predicting.\n",
    "\n",
    "This housing dataset tasks us with predicting the median house value in a given area. The dataset contains 8 features:\n",
    "- MedInc: median income in block\n",
    "- HouseAge: median house age in block\n",
    "- AveRooms: average number of rooms\n",
    "- AveBedrms: average number of bedrooms\n",
    "- Population: block population\n",
    "- AveOccup: average house occupancy\n",
    "- Latitude: house block latitude\n",
    "- Longitude: house block longitude\n",
    "\n",
    "The house values are measured in hundreds of thousands of dollars.\n",
    "\n",
    "We will use `Mean Absolute Error` as our evaluation metric. This metric is easy to interpret, as it is in the same units as the target variable. It is also robust to outliers, which is important in this dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ce594fcc9e6d66f4"
   },
   "id": "ce594fcc9e6d66f4"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n14196  3.2596      33.0  5.017657   1.006421      2300.0  3.691814     32.71   \n8267   3.8125      49.0  4.473545   1.041005      1314.0  1.738095     33.77   \n17445  4.1563       4.0  5.645833   0.985119       915.0  2.723214     34.66   \n14265  1.9425      36.0  4.002817   1.033803      1418.0  3.994366     32.69   \n2271   3.5542      43.0  6.268421   1.134211       874.0  2.300000     36.78   \n\n       Longitude  \n14196    -117.03  \n8267     -118.16  \n17445    -120.48  \n14265    -117.11  \n2271     -119.80  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14196</th>\n      <td>3.2596</td>\n      <td>33.0</td>\n      <td>5.017657</td>\n      <td>1.006421</td>\n      <td>2300.0</td>\n      <td>3.691814</td>\n      <td>32.71</td>\n      <td>-117.03</td>\n    </tr>\n    <tr>\n      <th>8267</th>\n      <td>3.8125</td>\n      <td>49.0</td>\n      <td>4.473545</td>\n      <td>1.041005</td>\n      <td>1314.0</td>\n      <td>1.738095</td>\n      <td>33.77</td>\n      <td>-118.16</td>\n    </tr>\n    <tr>\n      <th>17445</th>\n      <td>4.1563</td>\n      <td>4.0</td>\n      <td>5.645833</td>\n      <td>0.985119</td>\n      <td>915.0</td>\n      <td>2.723214</td>\n      <td>34.66</td>\n      <td>-120.48</td>\n    </tr>\n    <tr>\n      <th>14265</th>\n      <td>1.9425</td>\n      <td>36.0</td>\n      <td>4.002817</td>\n      <td>1.033803</td>\n      <td>1418.0</td>\n      <td>3.994366</td>\n      <td>32.69</td>\n      <td>-117.11</td>\n    </tr>\n    <tr>\n      <th>2271</th>\n      <td>3.5542</td>\n      <td>43.0</td>\n      <td>6.268421</td>\n      <td>1.134211</td>\n      <td>874.0</td>\n      <td>2.300000</td>\n      <td>36.78</td>\n      <td>-119.80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1dc750c91486515d",
    "outputId": "7901c045-6c9e-44d5-c178-f80e6a47e3fe",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:35.731965112Z",
     "start_time": "2023-11-09T21:38:35.712189514Z"
    }
   },
   "id": "1dc750c91486515d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.03 , 3.821, 1.726, 0.934, 0.965])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f035233438acdfae",
    "outputId": "9792e223-1b15-4c9d-88a3-3ecbb6245586",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:35.732490455Z",
     "start_time": "2023-11-09T21:38:35.728077503Z"
    }
   },
   "id": "f035233438acdfae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using CodeCarbon's EmissionsTracker is easy. When we want to record the cost of a specific training run, we simply wrap the training code in a with statement. Let's train a linear regression model and see how much carbon it emits."
   ],
   "metadata": {
    "collapsed": false,
    "id": "57498b7137a99e50"
   },
   "id": "57498b7137a99e50"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def report_emissions(emissions_tracker: EmissionsTracker):\n",
    "    energy = emissions_tracker.final_emissions_data.energy_consumed\n",
    "    energy = energy * ureg.kilowatt_hour\n",
    "    carbon = emissions_tracker.final_emissions_data.emissions\n",
    "    carbon = carbon * ureg.kilogram\n",
    "    print(f'Carbon emitted:      {carbon.to_compact():~.2f}')\n",
    "    print(f'Energy consumed:     {energy.to_compact():~.2f}')\n"
   ],
   "metadata": {
    "id": "b7af566d112d609",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:35.799070611Z",
     "start_time": "2023-11-09T21:38:35.732171956Z"
    }
   },
   "id": "b7af566d112d609"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Wrap the training code in a with statement\n",
    "with EmissionsTracker(project_name=\"Linear Regression\") as tracker:\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "with EmissionsTracker(project_name=\"Linear Regression Prediction\") as predict:\n",
    "    y_hat = model.predict(x_test)"
   ],
   "metadata": {
    "id": "8e9513e7ad6f5663",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:44.612758727Z",
     "start_time": "2023-11-09T21:38:35.773506947Z"
    }
   },
   "id": "8e9513e7ad6f5663"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to also record the carbon cost of making a prediction with each of these models. This is for later, when we look at LLMs."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e5cb9827a54c3f06"
   },
   "id": "e5cb9827a54c3f06"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon emitted:      4.07 µg\n",
      "Energy consumed:     103.09 µWh\n",
      "Mean absolute error: 0.53\n"
     ]
    }
   ],
   "source": [
    "report_emissions(tracker)\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_hat):.2f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5be959a3d51429d3",
    "outputId": "8f09906e-af24-477a-e186-18c12d2f2f0e",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:44.620922720Z",
     "start_time": "2023-11-09T21:38:44.617385524Z"
    }
   },
   "id": "5be959a3d51429d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unsurprisingly, training a simple linear regression model has a very small carbon footprint. Let's see what happens when we train a more complex model. Let's train a large Random Forest model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "d48e76fe356b5392"
   },
   "id": "d48e76fe356b5392"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=None, n_jobs=-1) # Use all cores, any depth\n",
    "with EmissionsTracker(project_name=\"Random Forest\") as tracker:\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "with EmissionsTracker(project_name=\"Random Forest Prediction\") as predict:\n",
    "    y_hat = model.predict(x_test)"
   ],
   "metadata": {
    "id": "255ccbdf76dd6d76",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:54.662019258Z",
     "start_time": "2023-11-09T21:38:44.622925077Z"
    }
   },
   "id": "255ccbdf76dd6d76"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon emitted:      605.80 µg\n",
      "Energy consumed:     15.34 mWh\n",
      "Mean absolute error: 0.33\n"
     ]
    }
   ],
   "source": [
    "report_emissions(tracker)\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_hat):.2f}')"
   ],
   "metadata": {
    "id": "cc9fe42ea9b7cac2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "418a275e-7d87-4b70-ec13-f16f3d790958",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:54.672702633Z",
     "start_time": "2023-11-09T21:38:54.664244259Z"
    }
   },
   "id": "cc9fe42ea9b7cac2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model has improved significantly, but at an increased carbon cost.\n",
    "\n",
    "Something important to note is that we are currently running these experiments in Google Colab. The location of the computing resources powering our code has a significant impact on the carbon footprint of our code. We can check what region the code is running in, using CodeCarbon:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "82a630713645806e"
   },
   "id": "82a630713645806e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region:         ontario\n",
      "Country:        Canada\n",
      "Emissions rate: 470.27 ng / Wh\n"
     ]
    }
   ],
   "source": [
    "print(f'Region:         {tracker.final_emissions_data.region}')\n",
    "print(f'Country:        {tracker.final_emissions_data.country_name}')\n",
    "emissions_rate = tracker.final_emissions_data.emissions_rate * ureg.kilogram / ureg.kilowatt_hour\n",
    "print(f'Emissions rate: {emissions_rate.to_compact():~.2f}')"
   ],
   "metadata": {
    "id": "468b05e3b1f1c78f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3fb66d25-bb65-45a5-e273-5cfde07e4afb",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:54.713858817Z",
     "start_time": "2023-11-09T21:38:54.667480657Z"
    }
   },
   "id": "468b05e3b1f1c78f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The region you see here is variable, but it's likely to be in the US. In one test, we received the following results:\n",
    "\n",
    "```\n",
    "Region:         oregon\n",
    "Country:        United States\n",
    "Emissions rate: 1.82 µg / Wh\n",
    "```\n",
    "\n",
    "Unsurprisingly, if you run this code in Ontario, where we have a much higher proportion of renewable energy, the emissions rate is much lower:\n",
    "\n",
    "```\n",
    "Region:         ontario\n",
    "Country:        Canada\n",
    "Emissions rate: 312.58 ng / Wh\n",
    "```\n",
    "\n",
    "Another important factor to consider is the efficiency of the hardware that we're using. CodeCarbon reports the power consumption of the computing resources we are working on:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "47b1aa4ce1962b01"
   },
   "id": "47b1aa4ce1962b01"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Power: 32.50 W\n",
      "RAM Power:  5.73 W\n"
     ]
    }
   ],
   "source": [
    "print(f'CPU Power: {tracker.final_emissions_data.cpu_power * ureg.watt:~.2f}')\n",
    "print(f'RAM Power:  {tracker.final_emissions_data.ram_power * ureg.watt:~.2f}')"
   ],
   "metadata": {
    "id": "1072cdf453a49c97",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "abd33e12-4332-4338-a210-b7ab0f12a0ae",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:38:54.714080739Z",
     "start_time": "2023-11-09T21:38:54.713515107Z"
    }
   },
   "id": "1072cdf453a49c97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your turn**\n",
    "\n",
    "Before we move on, let's try one more experiment. Choose a machine learning model in [Scikit-Learn](https://scikit-learn.org/stable/supervised_learning.html) and train it on the California Housing dataset. Use CodeCarbon to measure the carbon footprint of your model. How does it compare to the models we've already trained? __Hint: If you aren't sure what model to use, try the [Extra Random Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) model.__"
   ],
   "metadata": {
    "collapsed": false,
    "id": "597e42a47ede943e"
   },
   "id": "597e42a47ede943e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:38:54] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:38:54] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:38:54] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:38:54] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:38:54] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 16:38:55] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:38:55] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:38:55]   Platform system: Linux-6.2.0-36-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 16:38:55]   Python version: 3.11.6\n",
      "[codecarbon INFO @ 16:38:55]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 16:38:55]   Available RAM : 15.283 GB\n",
      "[codecarbon INFO @ 16:38:55]   CPU count: 12\n",
      "[codecarbon INFO @ 16:38:55]   CPU model: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:38:55]   GPU count: 1\n",
      "[codecarbon INFO @ 16:38:55]   GPU model: 1 x NVIDIA GeForce RTX 3060\n",
      "[codecarbon INFO @ 16:38:59] Energy consumed for RAM : 0.000001 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:38:59] Energy consumed for all GPUs : 0.000001 kWh. Total GPU Power : 7.19155372907921 W\n",
      "[codecarbon INFO @ 16:38:59] Energy consumed for all CPUs : 0.000004 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:38:59] 0.000005 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:59] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:38:59] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:38:59] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:38:59] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:38:59] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 16:39:00] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:39:00] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:39:00]   Platform system: Linux-6.2.0-36-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 16:39:00]   Python version: 3.11.6\n",
      "[codecarbon INFO @ 16:39:00]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 16:39:00]   Available RAM : 15.283 GB\n",
      "[codecarbon INFO @ 16:39:00]   CPU count: 12\n",
      "[codecarbon INFO @ 16:39:00]   CPU model: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:39:00]   GPU count: 1\n",
      "[codecarbon INFO @ 16:39:00]   GPU model: 1 x NVIDIA GeForce RTX 3060\n",
      "[codecarbon INFO @ 16:39:03] Energy consumed for RAM : 0.000000 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:39:03] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 16:39:03] Energy consumed for all CPUs : 0.000000 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:39:03] 0.000000 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon emitted:      202.47 µg\n",
      "Energy consumed:     5.13 mWh\n",
      "Mean absolute error: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Train Extra Random Trees model:\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model = ExtraTreesRegressor(n_estimators=100, max_depth=None, n_jobs=-1) # Use all cores, any depth\n",
    "\n",
    "with EmissionsTracker(project_name=\"Extra Random Trees\") as tracker:\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "with EmissionsTracker(project_name=\"Extra Random Trees Prediction\") as predict:\n",
    "    y_hat = model.predict(x_test)\n",
    "    \n",
    "report_emissions(tracker)\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_hat):.2f}')"
   ],
   "metadata": {
    "id": "fcfc87da08c68fb3",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:39:03.852704995Z",
     "start_time": "2023-11-09T21:38:54.713698105Z"
    }
   },
   "id": "fcfc87da08c68fb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Understanding the Carbon Cost of Large Language Models\n",
    "\n",
    "Now that we have a better understanding of how CodeCarbon works, let's use it to investigate the carbon cost of using a large language model.\n",
    "\n",
    "We are going to use HuggingFace to train a version of GPT-2 for a single epoch (i.e. one pass through the training data). We will then use the model to generate some text, and measure the carbon cost of the training and prediction steps.\n",
    "\n",
    "While the training part of this code is quite simple, getting the data ready requires a little bit of effort. We are going to rush through it here, as it isn't the focus of this lab, but the code is commented in case you're interested."
   ],
   "metadata": {
    "collapsed": false,
    "id": "43da7db949aaad9"
   },
   "id": "43da7db949aaad9"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/mambaforge/envs/enbridge_pytorch/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\") # Load the raw dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # Load the tokenizer - converts text into numbers\n",
    "\n",
    "# To speed up, let's use half the data\n",
    "dataset['train'] = dataset['train'].select(range(0, len(dataset['train']), 2))\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"]), # Tells this function how to use the tokenizer\n",
    "    batched=True, # Apply to groups of examples\n",
    "    num_proc=4, # Use 4 cores\n",
    "    remove_columns=[\"text\"] # Remove the text column, as we don't need it anymore\n",
    ")\n",
    "\n",
    "block_size = 256 # The maximum number of tokens in a single input\n",
    "\n",
    "# The main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Finally, apply the function above to our data\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")"
   ],
   "metadata": {
    "id": "c87c4a0917dfae99",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:39:05.368043003Z",
     "start_time": "2023-11-09T21:39:03.852564645Z"
    }
   },
   "id": "c87c4a0917dfae99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whew! With that out of the way, we can set up the actual training and measure its carbon cost."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ca8273956a661ddd"
   },
   "id": "ca8273956a661ddd"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:39:06] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:39:06] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:39:06] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:39:06] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:39:06] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 16:39:07] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:39:07] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:39:07]   Platform system: Linux-6.2.0-36-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 16:39:07]   Python version: 3.11.6\n",
      "[codecarbon INFO @ 16:39:07]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 16:39:07]   Available RAM : 15.283 GB\n",
      "[codecarbon INFO @ 16:39:07]   CPU count: 12\n",
      "[codecarbon INFO @ 16:39:07]   CPU model: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:39:07]   GPU count: 1\n",
      "[codecarbon INFO @ 16:39:07]   GPU model: 1 x NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-wikitext2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1, # Train for one epoch\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=None,\n",
    ")"
   ],
   "metadata": {
    "id": "29497895d19cf94",
    "outputId": "67458459-ad2b-478a-aec7-2131a484cdd8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T21:39:10.997997171Z",
     "start_time": "2023-11-09T21:39:05.369794044Z"
    }
   },
   "id": "29497895d19cf94"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:39:10] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:39:10] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:39:11] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:39:11] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:39:11] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 16:39:12] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:39:12] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:39:12]   Platform system: Linux-6.2.0-36-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 16:39:12]   Python version: 3.11.6\n",
      "[codecarbon INFO @ 16:39:12]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 16:39:12]   Available RAM : 15.283 GB\n",
      "[codecarbon INFO @ 16:39:12]   CPU count: 12\n",
      "[codecarbon INFO @ 16:39:12]   CPU model: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:39:12]   GPU count: 1\n",
      "[codecarbon INFO @ 16:39:12]   GPU model: 1 x NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1169 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:39:30] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:39:30] Energy consumed for all GPUs : 0.000569 kWh. Total GPU Power : 136.46114814490255 W\n",
      "[codecarbon INFO @ 16:39:30] Energy consumed for all CPUs : 0.000135 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:39:30] 0.000728 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:30] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:39:30] Energy consumed for all GPUs : 0.000572 kWh. Total GPU Power : 137.19117396879653 W\n",
      "[codecarbon INFO @ 16:39:30] Energy consumed for all CPUs : 0.000135 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:39:30] 0.000731 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:45] Energy consumed for RAM : 0.000048 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:39:45] Energy consumed for all GPUs : 0.001157 kWh. Total GPU Power : 141.23462647096108 W\n",
      "[codecarbon INFO @ 16:39:45] Energy consumed for all CPUs : 0.000271 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:39:45] 0.001476 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:45] Energy consumed for RAM : 0.000048 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:39:45] Energy consumed for all GPUs : 0.001160 kWh. Total GPU Power : 141.1532496750833 W\n",
      "[codecarbon INFO @ 16:39:45] Energy consumed for all CPUs : 0.000271 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:39:45] 0.001479 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:00] Energy consumed for RAM : 0.000072 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:00] Energy consumed for all GPUs : 0.001749 kWh. Total GPU Power : 142.02245049690114 W\n",
      "[codecarbon INFO @ 16:40:00] Energy consumed for all CPUs : 0.000406 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:00] 0.002227 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:00] Energy consumed for RAM : 0.000072 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:00] Energy consumed for all GPUs : 0.001753 kWh. Total GPU Power : 142.2748275642665 W\n",
      "[codecarbon INFO @ 16:40:00] Energy consumed for all CPUs : 0.000406 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:00] 0.002231 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:15] Energy consumed for RAM : 0.000096 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:15] Energy consumed for all GPUs : 0.002339 kWh. Total GPU Power : 141.66305160684715 W\n",
      "[codecarbon INFO @ 16:40:15] Energy consumed for all CPUs : 0.000542 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:15] 0.002976 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:15] Energy consumed for RAM : 0.000096 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:15] Energy consumed for all GPUs : 0.002343 kWh. Total GPU Power : 141.5766209958817 W\n",
      "[codecarbon INFO @ 16:40:15] Energy consumed for all CPUs : 0.000542 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:15] 0.002980 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:30] Energy consumed for RAM : 0.000119 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:30] Energy consumed for all GPUs : 0.002928 kWh. Total GPU Power : 141.41479561016007 W\n",
      "[codecarbon INFO @ 16:40:30] Energy consumed for all CPUs : 0.000677 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:30] 0.003725 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:30] Energy consumed for RAM : 0.000119 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:30] Energy consumed for all GPUs : 0.002931 kWh. Total GPU Power : 141.34201182963176 W\n",
      "[codecarbon INFO @ 16:40:30] Energy consumed for all CPUs : 0.000677 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:30] 0.003728 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:45] Energy consumed for RAM : 0.000143 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:45] Energy consumed for all GPUs : 0.003523 kWh. Total GPU Power : 142.90533593269254 W\n",
      "[codecarbon INFO @ 16:40:45] Energy consumed for all CPUs : 0.000813 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:45] 0.004479 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:45] Energy consumed for RAM : 0.000143 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:40:45] Energy consumed for all GPUs : 0.003527 kWh. Total GPU Power : 143.07682835445496 W\n",
      "[codecarbon INFO @ 16:40:45] Energy consumed for all CPUs : 0.000813 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:40:45] 0.004483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:00] Energy consumed for RAM : 0.000167 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:00] Energy consumed for all GPUs : 0.004114 kWh. Total GPU Power : 141.66704879856314 W\n",
      "[codecarbon INFO @ 16:41:00] Energy consumed for all CPUs : 0.000948 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:00] 0.005229 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:00] Energy consumed for RAM : 0.000167 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:00] Energy consumed for all GPUs : 0.004117 kWh. Total GPU Power : 141.5662071933855 W\n",
      "[codecarbon INFO @ 16:41:00] Energy consumed for all CPUs : 0.000948 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:00] 0.005232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:15] Energy consumed for RAM : 0.000191 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:15] Energy consumed for all GPUs : 0.004705 kWh. Total GPU Power : 141.84677534081302 W\n",
      "[codecarbon INFO @ 16:41:15] Energy consumed for all CPUs : 0.001083 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:15] 0.005979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:15] Energy consumed for RAM : 0.000191 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:15] Energy consumed for all GPUs : 0.004709 kWh. Total GPU Power : 141.95679684072198 W\n",
      "[codecarbon INFO @ 16:41:15] Energy consumed for all CPUs : 0.001083 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:15] 0.005983 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:30] Energy consumed for RAM : 0.000215 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:30] Energy consumed for all GPUs : 0.005302 kWh. Total GPU Power : 143.46713917244085 W\n",
      "[codecarbon INFO @ 16:41:30] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:30] 0.006736 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:30] Energy consumed for RAM : 0.000215 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:30] Energy consumed for all GPUs : 0.005306 kWh. Total GPU Power : 143.28362638255527 W\n",
      "[codecarbon INFO @ 16:41:30] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:30] 0.006739 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:45] Energy consumed for RAM : 0.000239 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:45] Energy consumed for all GPUs : 0.005895 kWh. Total GPU Power : 142.29216139542706 W\n",
      "[codecarbon INFO @ 16:41:45] Energy consumed for all CPUs : 0.001354 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:45] 0.007488 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:45] Energy consumed for RAM : 0.000239 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:41:45] Energy consumed for all GPUs : 0.005898 kWh. Total GPU Power : 142.2653223888415 W\n",
      "[codecarbon INFO @ 16:41:45] Energy consumed for all CPUs : 0.001354 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:41:45] 0.007491 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:00] Energy consumed for RAM : 0.000263 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:00] Energy consumed for all GPUs : 0.006488 kWh. Total GPU Power : 142.4007315296677 W\n",
      "[codecarbon INFO @ 16:42:00] Energy consumed for all CPUs : 0.001490 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:00] 0.008241 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:00] Energy consumed for RAM : 0.000263 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:00] Energy consumed for all GPUs : 0.006491 kWh. Total GPU Power : 142.36106395896675 W\n",
      "[codecarbon INFO @ 16:42:00] Energy consumed for all CPUs : 0.001490 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:00] 0.008244 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:15] Energy consumed for RAM : 0.000286 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:15] Energy consumed for all GPUs : 0.007082 kWh. Total GPU Power : 142.5533413328669 W\n",
      "[codecarbon INFO @ 16:42:15] Energy consumed for all CPUs : 0.001625 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:15] 0.008994 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:15] Energy consumed for RAM : 0.000287 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:15] Energy consumed for all GPUs : 0.007086 kWh. Total GPU Power : 142.74110960965731 W\n",
      "[codecarbon INFO @ 16:42:15] Energy consumed for all CPUs : 0.001625 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:15] 0.008998 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:30] Energy consumed for RAM : 0.000310 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:30] Energy consumed for all GPUs : 0.007669 kWh. Total GPU Power : 140.8192555729788 W\n",
      "[codecarbon INFO @ 16:42:30] Energy consumed for all CPUs : 0.001760 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:30] 0.009740 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:30] Energy consumed for RAM : 0.000310 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:30] Energy consumed for all GPUs : 0.007672 kWh. Total GPU Power : 140.7314450188939 W\n",
      "[codecarbon INFO @ 16:42:30] Energy consumed for all CPUs : 0.001760 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:30] 0.009743 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:45] Energy consumed for RAM : 0.000334 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:45] Energy consumed for all GPUs : 0.008271 kWh. Total GPU Power : 144.5884500855775 W\n",
      "[codecarbon INFO @ 16:42:45] Energy consumed for all CPUs : 0.001896 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:45] 0.010501 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:45] Energy consumed for RAM : 0.000334 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:42:45] Energy consumed for all GPUs : 0.008274 kWh. Total GPU Power : 144.52196829652803 W\n",
      "[codecarbon INFO @ 16:42:45] Energy consumed for all CPUs : 0.001896 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:42:45] 0.010505 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:00] Energy consumed for RAM : 0.000358 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:43:00] Energy consumed for all GPUs : 0.008859 kWh. Total GPU Power : 141.0162844395877 W\n",
      "[codecarbon INFO @ 16:43:00] Energy consumed for all CPUs : 0.002031 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:43:00] 0.011248 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:00] Energy consumed for RAM : 0.000358 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:43:00] Energy consumed for all GPUs : 0.008863 kWh. Total GPU Power : 141.19147033847003 W\n",
      "[codecarbon INFO @ 16:43:00] Energy consumed for all CPUs : 0.002031 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:43:00] 0.011252 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:07] Energy consumed for RAM : 0.000370 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:43:07] Energy consumed for all GPUs : 0.009152 kWh. Total GPU Power : 143.11373352570294 W\n",
      "[codecarbon INFO @ 16:43:07] Energy consumed for all CPUs : 0.002097 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:43:07] 0.011619 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:07] Energy consumed for RAM : 0.000370 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:43:07] Energy consumed for all GPUs : 0.009152 kWh. Total GPU Power : 143.08069456747666 W\n",
      "[codecarbon INFO @ 16:43:07] Energy consumed for all CPUs : 0.002098 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:43:07] 0.011620 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker(project_name=\"GPT-2 Training\") as tracker:\n",
    "  trainer.train()"
   ],
   "metadata": {
    "id": "4d056d6ba8cc62b2",
    "outputId": "0f1ea54d-4ca3-46ea-b8de-0b3db6572003",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T21:43:07.813982420Z",
     "start_time": "2023-11-09T21:39:10.999811883Z"
    }
   },
   "id": "4d056d6ba8cc62b2"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon emitted:      458.96 mg\n",
      "Energy consumed:     11.62 Wh\n"
     ]
    }
   ],
   "source": [
    "report_emissions(tracker)"
   ],
   "metadata": {
    "id": "d34e826bb1736877",
    "outputId": "e2867baa-c56d-425d-b3ca-4209223c6bf6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-09T21:43:07.819358664Z",
     "start_time": "2023-11-09T21:43:07.813470981Z"
    }
   },
   "id": "d34e826bb1736877"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Your turn**\n",
    "\n",
    "Now that we've trained our model, let's use it to generate some text. Use the `generate` method on the `trainer` object to generate some text. You can use the [documentation](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) to help you. Use CodeCarbon to measure the carbon footprint of generating the text. How does it compare to the carbon footprint of training the model? We can also look at the results of all our experiments thus far by running the following command:\n",
    "\n",
    "`pd.read_csv('codecarbon.csv')`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "44c98aa99ad0be3c"
   },
   "id": "44c98aa99ad0be3c"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:43:07] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:43:07] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:43:07] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:43:07] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:43:07] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 16:43:09] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:43:09] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:43:09]   Platform system: Linux-6.2.0-36-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 16:43:09]   Python version: 3.11.6\n",
      "[codecarbon INFO @ 16:43:09]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 16:43:09]   Available RAM : 15.283 GB\n",
      "[codecarbon INFO @ 16:43:09]   CPU count: 12\n",
      "[codecarbon INFO @ 16:43:09]   CPU model: 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz\n",
      "[codecarbon INFO @ 16:43:09]   GPU count: 1\n",
      "[codecarbon INFO @ 16:43:09]   GPU model: 1 x NVIDIA GeForce RTX 3060\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[codecarbon INFO @ 16:43:12] Energy consumed for RAM : 0.000001 kWh. RAM Power : 5.7310380935668945 W\n",
      "[codecarbon INFO @ 16:43:12] Energy consumed for all GPUs : 0.000005 kWh. Total GPU Power : 37.467016822370674 W\n",
      "[codecarbon INFO @ 16:43:12] Energy consumed for all CPUs : 0.000004 kWh. Total CPU Power : 32.5 W\n",
      "[codecarbon INFO @ 16:43:12] 0.000010 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon emitted:      385.13 µg\n",
      "Energy consumed:     9.75 mWh\n",
      "Hello, my name is Hilda. \n",
      " = = = = Koopas = = = \n",
      " In the movie, O'Donnell ( Bill Mullen \" Hilda \", and Barry Johnson ) narrates O'Donnell and the K\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker(project_name=\"GPT-2 Prediction\") as predict:\n",
    "  tokens = tokenizer.encode(\"Hello, my name is\", return_tensors='pt').to('cuda')\n",
    "  output = model.generate(tokens, do_sample=True, max_length=50, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "report_emissions(predict)\n",
    "print(tokenizer.decode(output[0]))"
   ],
   "metadata": {
    "id": "fad4de0f01cd5525",
    "ExecuteTime": {
     "end_time": "2023-11-09T21:43:12.656579836Z",
     "start_time": "2023-11-09T21:43:07.813713470Z"
    }
   },
   "id": "fad4de0f01cd5525"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "              timestamp                   project_name  \\\n0   2023-11-09T16:30:51              Linear Regression   \n1   2023-11-09T16:30:56   Linear Regression Prediction   \n2   2023-11-09T16:31:02                  Random Forest   \n3   2023-11-09T16:31:06       Random Forest Prediction   \n4   2023-11-09T16:31:35             Extra Random Trees   \n5   2023-11-09T16:31:40  Extra Random Trees Prediction   \n6   2023-11-09T16:36:04                 GPT-2 Training   \n7   2023-11-09T16:36:19               GPT-2 Prediction   \n8   2023-11-09T16:36:33               GPT-2 Prediction   \n9   2023-11-09T16:36:41               GPT-2 Prediction   \n10  2023-11-09T16:37:05               GPT-2 Prediction   \n11  2023-11-09T16:37:33               GPT-2 Prediction   \n12  2023-11-09T16:37:44               GPT-2 Prediction   \n13  2023-11-09T16:38:05               GPT-2 Prediction   \n14  2023-11-09T16:38:40              Linear Regression   \n15  2023-11-09T16:38:44   Linear Regression Prediction   \n16  2023-11-09T16:38:50                  Random Forest   \n17  2023-11-09T16:38:54       Random Forest Prediction   \n18  2023-11-09T16:38:59             Extra Random Trees   \n19  2023-11-09T16:39:03  Extra Random Trees Prediction   \n20  2023-11-09T16:43:07                 GPT-2 Training   \n21  2023-11-09T16:43:12               GPT-2 Prediction   \n\n                                  run_id    duration     emissions  \\\n0   1b5b8cc1-f631-4d82-96be-900600d1fb03    0.102026  4.966841e-08   \n1   5ea4c281-4ce7-4b7c-9efc-5574ace3e7f2    0.011489  4.711100e-09   \n2   4f297ca6-cb5a-400d-9c0e-514f3f7eb116    1.490961  6.943096e-07   \n3   90659181-1c8e-4cc5-aae8-49056b065497    0.027983  1.163435e-08   \n4   ad6bee1a-473a-4dac-b43d-5ed0513ab5a6    0.425336  1.867338e-07   \n5   b5e2ac6f-cf99-4b51-abce-4c72521b22b7    0.038878  1.888297e-08   \n6   ff6edc35-701f-486c-a1e3-1f51dc82359d  232.182654  4.583554e-04   \n7   c9ca7b40-3b5b-44dd-8af6-524d9a0d0d2d    0.003132  1.030058e-09   \n8   c94227e9-57d1-439f-9b6b-5bc044582fa7    0.003734  1.088585e-09   \n9   a8dee499-a4f8-4658-85a4-d950eb87878f    0.003347  1.070240e-09   \n10  ce604510-01d5-4ff0-b4c2-44694345026b    0.003324  1.029192e-09   \n11  16140f06-8f15-4347-b07c-eed775eaa5e1    0.005158  1.808987e-09   \n12  d96ed251-7fe1-456d-9021-26f1fefe06c3    0.016630  6.947820e-09   \n13  093cc5ba-988a-4172-9adb-96bb3c4428ed    1.066819  8.070748e-07   \n14  67dd9fb8-2646-4cc5-8ba3-fcc0621995c8    0.010108  4.071640e-09   \n15  c8af6b3b-34af-4684-85ee-a30633644b66    0.002886  1.119355e-09   \n16  a620b3ce-9f7f-4f3c-869c-9978c563e8e7    1.288191  6.058023e-07   \n17  6e49c03f-b4fd-4f46-ab20-5f2ceac656ee    0.026212  1.089720e-08   \n18  4a01889c-9cbf-4f96-8414-eca1e5e4bec4    0.407088  2.024677e-07   \n19  f3fe2372-d8a3-4061-b61b-151d063b2307    0.038591  1.592424e-08   \n20  89eba29f-4887-4b7b-9522-69ee474f615b  232.401682  4.589567e-04   \n21  eaea4258-44c7-45a4-8287-e9f593ca8dd6    0.464987  3.851254e-07   \n\n    emissions_rate  cpu_power   gpu_power  ram_power    cpu_energy  ...  \\\n0     4.868223e-07       32.5    6.359228   5.731038  9.195742e-07  ...   \n1     4.100391e-07       32.5    0.000000   5.731038  1.030198e-07  ...   \n2     4.656793e-07       32.5    4.223889   5.731038  1.345939e-05  ...   \n3     4.157655e-07       32.5    0.000000   5.731038  2.519846e-07  ...   \n4     4.390261e-07       32.5    1.856501   5.731038  3.836022e-06  ...   \n5     4.856956e-07       32.5    6.990868   5.731038  3.470277e-07  ...   \n6     1.974116e-06       32.5  138.559339   5.731038  2.095987e-03  ...   \n7     3.288959e-07       32.5    0.000000   5.731038  2.421654e-08  ...   \n8     2.915431e-07       32.5    0.000000   5.731038  2.566725e-08  ...   \n9     3.197687e-07       32.5    0.000000   5.731038  2.495911e-08  ...   \n10    3.096437e-07       32.5    0.000000   5.731038  2.432631e-08  ...   \n11    3.506859e-07       32.5    0.000000   5.731038  4.174991e-08  ...   \n12    4.177900e-07       32.5    2.295661   5.731038  1.440530e-07  ...   \n13    7.565246e-07       32.5   30.797728   5.731038  9.626179e-06  ...   \n14    4.028233e-07       32.5    0.000000   5.731038  8.965565e-08  ...   \n15    3.879134e-07       32.5    0.000000   5.731038  2.544771e-08  ...   \n16    4.702736e-07       32.5    4.646370   5.731038  1.162879e-05  ...   \n17    4.157300e-07       32.5    0.000000   5.731038  2.360117e-07  ...   \n18    4.973562e-07       32.5    7.191554   5.731038  3.671594e-06  ...   \n19    4.126372e-07       32.5    0.000000   5.731038  3.449872e-07  ...   \n20    1.974842e-06       32.5  143.080695   5.731038  2.097971e-03  ...   \n21    8.282493e-07       32.5   37.467017   5.731038  4.192488e-06  ...   \n\n    cpu_count                                      cpu_model  gpu_count  \\\n0          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n1          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n2          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n3          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n4          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n5          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n6          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n7          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n8          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n9          12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n10         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n11         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n12         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n13         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n14         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n15         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n16         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n17         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n18         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n19         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n20         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n21         12  11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz          1   \n\n                      gpu_model longitude latitude  ram_total_size  \\\n0   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n1   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n2   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n3   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n4   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n5   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n6   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n7   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n8   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n9   1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n10  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n11  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n12  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n13  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n14  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n15  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n16  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n17  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n18  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n19  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n20  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n21  1 x NVIDIA GeForce RTX 3060  -79.3999  43.6638       15.282768   \n\n    tracking_mode on_cloud  pue  \n0         machine        N  1.0  \n1         machine        N  1.0  \n2         machine        N  1.0  \n3         machine        N  1.0  \n4         machine        N  1.0  \n5         machine        N  1.0  \n6         machine        N  1.0  \n7         machine        N  1.0  \n8         machine        N  1.0  \n9         machine        N  1.0  \n10        machine        N  1.0  \n11        machine        N  1.0  \n12        machine        N  1.0  \n13        machine        N  1.0  \n14        machine        N  1.0  \n15        machine        N  1.0  \n16        machine        N  1.0  \n17        machine        N  1.0  \n18        machine        N  1.0  \n19        machine        N  1.0  \n20        machine        N  1.0  \n21        machine        N  1.0  \n\n[22 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>project_name</th>\n      <th>run_id</th>\n      <th>duration</th>\n      <th>emissions</th>\n      <th>emissions_rate</th>\n      <th>cpu_power</th>\n      <th>gpu_power</th>\n      <th>ram_power</th>\n      <th>cpu_energy</th>\n      <th>...</th>\n      <th>cpu_count</th>\n      <th>cpu_model</th>\n      <th>gpu_count</th>\n      <th>gpu_model</th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>ram_total_size</th>\n      <th>tracking_mode</th>\n      <th>on_cloud</th>\n      <th>pue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-11-09T16:30:51</td>\n      <td>Linear Regression</td>\n      <td>1b5b8cc1-f631-4d82-96be-900600d1fb03</td>\n      <td>0.102026</td>\n      <td>4.966841e-08</td>\n      <td>4.868223e-07</td>\n      <td>32.5</td>\n      <td>6.359228</td>\n      <td>5.731038</td>\n      <td>9.195742e-07</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-11-09T16:30:56</td>\n      <td>Linear Regression Prediction</td>\n      <td>5ea4c281-4ce7-4b7c-9efc-5574ace3e7f2</td>\n      <td>0.011489</td>\n      <td>4.711100e-09</td>\n      <td>4.100391e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>1.030198e-07</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-11-09T16:31:02</td>\n      <td>Random Forest</td>\n      <td>4f297ca6-cb5a-400d-9c0e-514f3f7eb116</td>\n      <td>1.490961</td>\n      <td>6.943096e-07</td>\n      <td>4.656793e-07</td>\n      <td>32.5</td>\n      <td>4.223889</td>\n      <td>5.731038</td>\n      <td>1.345939e-05</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-11-09T16:31:06</td>\n      <td>Random Forest Prediction</td>\n      <td>90659181-1c8e-4cc5-aae8-49056b065497</td>\n      <td>0.027983</td>\n      <td>1.163435e-08</td>\n      <td>4.157655e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>2.519846e-07</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-11-09T16:31:35</td>\n      <td>Extra Random Trees</td>\n      <td>ad6bee1a-473a-4dac-b43d-5ed0513ab5a6</td>\n      <td>0.425336</td>\n      <td>1.867338e-07</td>\n      <td>4.390261e-07</td>\n      <td>32.5</td>\n      <td>1.856501</td>\n      <td>5.731038</td>\n      <td>3.836022e-06</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2023-11-09T16:31:40</td>\n      <td>Extra Random Trees Prediction</td>\n      <td>b5e2ac6f-cf99-4b51-abce-4c72521b22b7</td>\n      <td>0.038878</td>\n      <td>1.888297e-08</td>\n      <td>4.856956e-07</td>\n      <td>32.5</td>\n      <td>6.990868</td>\n      <td>5.731038</td>\n      <td>3.470277e-07</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2023-11-09T16:36:04</td>\n      <td>GPT-2 Training</td>\n      <td>ff6edc35-701f-486c-a1e3-1f51dc82359d</td>\n      <td>232.182654</td>\n      <td>4.583554e-04</td>\n      <td>1.974116e-06</td>\n      <td>32.5</td>\n      <td>138.559339</td>\n      <td>5.731038</td>\n      <td>2.095987e-03</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2023-11-09T16:36:19</td>\n      <td>GPT-2 Prediction</td>\n      <td>c9ca7b40-3b5b-44dd-8af6-524d9a0d0d2d</td>\n      <td>0.003132</td>\n      <td>1.030058e-09</td>\n      <td>3.288959e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>2.421654e-08</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2023-11-09T16:36:33</td>\n      <td>GPT-2 Prediction</td>\n      <td>c94227e9-57d1-439f-9b6b-5bc044582fa7</td>\n      <td>0.003734</td>\n      <td>1.088585e-09</td>\n      <td>2.915431e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>2.566725e-08</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2023-11-09T16:36:41</td>\n      <td>GPT-2 Prediction</td>\n      <td>a8dee499-a4f8-4658-85a4-d950eb87878f</td>\n      <td>0.003347</td>\n      <td>1.070240e-09</td>\n      <td>3.197687e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>2.495911e-08</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2023-11-09T16:37:05</td>\n      <td>GPT-2 Prediction</td>\n      <td>ce604510-01d5-4ff0-b4c2-44694345026b</td>\n      <td>0.003324</td>\n      <td>1.029192e-09</td>\n      <td>3.096437e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>2.432631e-08</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2023-11-09T16:37:33</td>\n      <td>GPT-2 Prediction</td>\n      <td>16140f06-8f15-4347-b07c-eed775eaa5e1</td>\n      <td>0.005158</td>\n      <td>1.808987e-09</td>\n      <td>3.506859e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>4.174991e-08</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2023-11-09T16:37:44</td>\n      <td>GPT-2 Prediction</td>\n      <td>d96ed251-7fe1-456d-9021-26f1fefe06c3</td>\n      <td>0.016630</td>\n      <td>6.947820e-09</td>\n      <td>4.177900e-07</td>\n      <td>32.5</td>\n      <td>2.295661</td>\n      <td>5.731038</td>\n      <td>1.440530e-07</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2023-11-09T16:38:05</td>\n      <td>GPT-2 Prediction</td>\n      <td>093cc5ba-988a-4172-9adb-96bb3c4428ed</td>\n      <td>1.066819</td>\n      <td>8.070748e-07</td>\n      <td>7.565246e-07</td>\n      <td>32.5</td>\n      <td>30.797728</td>\n      <td>5.731038</td>\n      <td>9.626179e-06</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2023-11-09T16:38:40</td>\n      <td>Linear Regression</td>\n      <td>67dd9fb8-2646-4cc5-8ba3-fcc0621995c8</td>\n      <td>0.010108</td>\n      <td>4.071640e-09</td>\n      <td>4.028233e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>8.965565e-08</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2023-11-09T16:38:44</td>\n      <td>Linear Regression Prediction</td>\n      <td>c8af6b3b-34af-4684-85ee-a30633644b66</td>\n      <td>0.002886</td>\n      <td>1.119355e-09</td>\n      <td>3.879134e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>2.544771e-08</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2023-11-09T16:38:50</td>\n      <td>Random Forest</td>\n      <td>a620b3ce-9f7f-4f3c-869c-9978c563e8e7</td>\n      <td>1.288191</td>\n      <td>6.058023e-07</td>\n      <td>4.702736e-07</td>\n      <td>32.5</td>\n      <td>4.646370</td>\n      <td>5.731038</td>\n      <td>1.162879e-05</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2023-11-09T16:38:54</td>\n      <td>Random Forest Prediction</td>\n      <td>6e49c03f-b4fd-4f46-ab20-5f2ceac656ee</td>\n      <td>0.026212</td>\n      <td>1.089720e-08</td>\n      <td>4.157300e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>2.360117e-07</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2023-11-09T16:38:59</td>\n      <td>Extra Random Trees</td>\n      <td>4a01889c-9cbf-4f96-8414-eca1e5e4bec4</td>\n      <td>0.407088</td>\n      <td>2.024677e-07</td>\n      <td>4.973562e-07</td>\n      <td>32.5</td>\n      <td>7.191554</td>\n      <td>5.731038</td>\n      <td>3.671594e-06</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2023-11-09T16:39:03</td>\n      <td>Extra Random Trees Prediction</td>\n      <td>f3fe2372-d8a3-4061-b61b-151d063b2307</td>\n      <td>0.038591</td>\n      <td>1.592424e-08</td>\n      <td>4.126372e-07</td>\n      <td>32.5</td>\n      <td>0.000000</td>\n      <td>5.731038</td>\n      <td>3.449872e-07</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2023-11-09T16:43:07</td>\n      <td>GPT-2 Training</td>\n      <td>89eba29f-4887-4b7b-9522-69ee474f615b</td>\n      <td>232.401682</td>\n      <td>4.589567e-04</td>\n      <td>1.974842e-06</td>\n      <td>32.5</td>\n      <td>143.080695</td>\n      <td>5.731038</td>\n      <td>2.097971e-03</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2023-11-09T16:43:12</td>\n      <td>GPT-2 Prediction</td>\n      <td>eaea4258-44c7-45a4-8287-e9f593ca8dd6</td>\n      <td>0.464987</td>\n      <td>3.851254e-07</td>\n      <td>8.282493e-07</td>\n      <td>32.5</td>\n      <td>37.467017</td>\n      <td>5.731038</td>\n      <td>4.192488e-06</td>\n      <td>...</td>\n      <td>12</td>\n      <td>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</td>\n      <td>1</td>\n      <td>1 x NVIDIA GeForce RTX 3060</td>\n      <td>-79.3999</td>\n      <td>43.6638</td>\n      <td>15.282768</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>22 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('emissions.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T21:43:12.701081063Z",
     "start_time": "2023-11-09T21:43:12.653958797Z"
    }
   },
   "id": "ba6b1b6db56ea7e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For reference, one estimate of the carbon cost to train GPT-4 is around 12,500 metric tons of CO2. This is based on the assumption that the model is trained in California, using about 25,000 NVIDIA A100 GPUs. This is the equivalent of the anual emissions of 2,700 cars. This is a lot of carbon, but it's important to remember that this is a one-time cost. Once the model is trained, it can be used by many people, with a much lower carbon cost per user."
   ],
   "metadata": {
    "collapsed": false,
    "id": "e73f4f228f98c9b7"
   },
   "id": "e73f4f228f98c9b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, we have explored the carbon cost of training a machine learning model. We have seen that the cost of training a large language model is substantially higher than a traditional machine learning algorithm. There are a number of ways that we can try to reduce carbon emissions in machine learning:\n",
    "\n",
    "- **Use more efficient hardware**: The hardware we use to train our models has a significant impact on the carbon footprint of our code. Using more efficient hardware, such as GPUs, can reduce the carbon footprint of our code.\n",
    "- **Use more efficient algorithms**: Some algorithms are more efficient than others. For example, linear regression is much more efficient than a large language model.\n",
    "- **Run code in regions with renewable energy**: The location of the computing resources powering our code has a significant impact on the carbon footprint of our code. Running our code in regions with a high proportion of renewable energy can reduce the carbon footprint of our code.\n",
    "- **Train less often**: Training a model has a much higher carbon cost than using it. If we are careful about how often we train our models, we can spread the carbon cost over a longer period of time.\n",
    "- **Use smaller models**: Large language models are very powerful, but they also have a high carbon cost. If we can use a smaller model, we can reduce the carbon footprint of our code."
   ],
   "metadata": {
    "collapsed": false,
    "id": "6ae6664da985350d"
   },
   "id": "6ae6664da985350d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
