{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CARTE-Enbridge Bootcamp\n",
    "#### Lab 5-0\n",
    "\n",
    "# Building a GPT model from scratch"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a4cba22cdd1889e"
   },
   "id": "a4cba22cdd1889e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we are going to build a very simple version of GPT. Our GPT will have a small vocabulary and a small number of layers. Let's begin by importing the necessary libraries."
   ],
   "metadata": {
    "collapsed": false,
    "id": "b1108de7761cb146"
   },
   "id": "b1108de7761cb146"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:34:32.579881214Z",
     "start_time": "2023-11-13T14:34:30.702011989Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 09:34:30.855114: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-13 09:34:30.887011: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 09:34:38.571521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.704738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.704903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.768423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.768555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.768633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.768697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 10392 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "2023-11-13 09:34:38.769827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.769972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.770044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.770137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.770227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:34:38.770280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 10392 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Check that we are using a GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    raise SystemError('GPU device not found! Enable GPU by going to Runtime > Change runtime type > GPU')"
   ],
   "metadata": {
    "id": "109498fd48e49b4e",
    "outputId": "cb42e057-471f-4a0c-80f1-978f5ea11e9e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:34:38.775889372Z",
     "start_time": "2023-11-13T14:34:38.770933707Z"
    }
   },
   "id": "109498fd48e49b4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "At its core, GPT is a model to predict the next word in a sequence. In order to be able to learn, we need to first convert words into values that can be fed into the model.\n",
    "\n",
    "We are going to load a dataset of samples from [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page). This is a version of Wikipedia that aims to cover the same content, but using a reduced vocabulary and simpler grammar. This makes it easier for language learners to understand. We will use this dataset to train our model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "de23c7596a50c73"
   },
   "id": "de23c7596a50c73"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.9M/11.9M [00:00<00:00, 42.7MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April\n",
      "April is the fourth month of the year with 30 days. The name April comes from that Latin word \"aperire\" which means \"to open\". This probably refers to growing plants in spring. April begins on the same day of week as \"July\" in all years and also \"January\" in leap years.\n",
      "April's flower is the Sweet Pea and its birthstone is the Diamond. The meaning of the Diamond is Innocence.\n",
      "April in poetry.\n",
      "Poets use \"April\" to mean the end of winter. For example: \"April showers bring May flowers.\"\n",
      "\n",
      "August\n",
      "August is the eighth month of the year. It has 31 days.\n",
      "This month was first called \"Sextilis\" in Latin, because it was the sixth month in the old Roman calendar. The Roman calendar began in March about 735 BC with Romulus. It was the eighth month when January or February were added to the start of the year by King Numa Pompilius about 700 BC. Or, when those two months were moved from the end to the beginning of the year by the decemvirs about 450 BC (Roman writers disagree).\n",
      "August is named \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/alexwolson/carte_workshop_datasets/main/corpus.txt.zip\"\n",
    "\n",
    "# Stream the download so we can track its progress\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# Total size in bytes.\n",
    "total_size = int(response.headers.get('content-length', 0))\n",
    "block_size = 1024  # 1KB\n",
    "progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "with open('corpus.txt.zip', 'wb') as file:\n",
    "    for data in response.iter_content(block_size):\n",
    "        progress_bar.update(len(data))\n",
    "        file.write(data)\n",
    "progress_bar.close()\n",
    "\n",
    "if total_size != 0 and progress_bar.n != total_size:\n",
    "    print(\"ERROR, something went wrong\")\n",
    "\n",
    "# Now we will extract the zip file\n",
    "z = zipfile.ZipFile('corpus.txt.zip')\n",
    "z.extractall()\n",
    "\n",
    "# Read the first 1000 characters from the corpus\n",
    "with open('corpus.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(corpus[:1000])"
   ],
   "metadata": {
    "id": "6bea0f8983a1dc85",
    "outputId": "ad740ffa-a810-4826-8f45-eb5b7cd2aebd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:35:44.987497002Z",
     "start_time": "2023-11-13T14:35:44.182886884Z"
    }
   },
   "id": "6bea0f8983a1dc85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we want our model to be very simple, we are going to determine the 500 most common words, and use those as our vocabulary."
   ],
   "metadata": {
    "collapsed": false,
    "id": "79f68b6893648c0"
   },
   "id": "79f68b6893648c0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'in', 'a', 'and', 'is', 'to', 'was', 'it', 'that', 'for', 'are', 'as', 'he', 'on', 'by', 'with', 'or', 'from', 'they', 'an', 'this', 'at', 'be', 'his', 'people', 'also', 'has', 'not', 'were', 'which', 'have', 'one', 'river', 'but', 'can', 'many', 'called', 'other', 'there', 'city', 'their', 'when', 'first', 'who', 'some', 'used', 'its', 'about', 'had', 'most', 'found', 'into', 'after', 'made', 'united', 'very', 'states', 'she', 'more', 'all', 'time', 'because', 'two', 'france', 'new', 'like', 'part', 'her', 'been', 'music', 'region', 'only', 'world', 'known', 'these', 'means', 'north', 'name', 'commune', 'them', 'than', 'became', 'may', 'years', 'such', 'often', 'so', 'up', 'different', 'where', 'department', 'born', 'during', 's', 'between', 'over', 'if', 'him', 'then', 'th', 'use', 'will', 'make', 'usually', 'war', 'out', 'do', 'state', 'south', 'would', 'american', 'later', 'area', 'no', 'famous', 'each', 'same', 'before', 'small', 'year', 'three', 'east', 'english', 'number', 'located', 'sometimes', 'romania', 'important', 'won', 'district', 'well', 'town', 'around', 'work', 'country', 'main', 'being', 'way', 'person', 'life', 'century', 'named', 'language', 'now', 'population', 'history', 'what', 'group', 'did', 'county', 'government', 'live', 'countries', 'second', 'started', 'system', 'flows', 'through', 'west', 'long', 'another', 'british', 'things', 'de', 'both', 'large', 'until', 'since', 'example', 'national', 'while', 'word', 'could', 'much', 'however', 'water', 'tributary', 'series', 'place', 'family', 'band', 'played', 'any', 'province', 'said', 'old', 'capital', 'movie', 'how', 'germany', 'several', 'game', 'album', 'popular', 'get', 'died', 'football', 'season', 'even', 'early', 'king', 'still', 'good', 'england', 'day', 'released', 'iowa', 'wrote', 'german', 'great', 'book', 'back', 'team', 'under', 'show', 'i', 'asteroid', 'september', 'including', 'written', 'us', 'near', 'high', 'school', 'form', 'four', 'end', 'see', 'best', 'church', 'president', 'la', 'big', 'against', 'university', 'together', 'french', 'power', 'become', 'just', 'league', 'built', 'times', 'lot', 'common', 'championship', 'island', 'august', 'parts', 'today', 'body', 'does', 'own', 'play', 'go', 'came', 'song', 'using', 'created', 'games', 'october', 'last', 'john', 'went', 'television', 'began', 'player', 'america', 'largest', 'death', 'you', 'money', 'given', 'march', 'we', 'tropical', 'london', 'sea', 'left', 'europe', 'rock', 'pakistan', 'india', 'party', 'home', 'land', 'based', 'teams', 'took', 'thought', 'hurricane', 'million', 'kingdom', 'man', 'include', 'january', 'down', 'july', 'again', 'help', 'every', 'children', 'those', 'november', 'major', 'municipality', 'god', 'modern', 'st', 'songs', 'december', 'company', 'members', 'southern', 'although', 'union', 'storm', 'type', 'story', 'april', 'june', 'making', 'house', 'food', 'few', 'comes', 'ii', 'take', 'western', 'period', 'makes', 'special', 'animals', 'computer', 'performed', 'set', 'something', 'roman', 'greek', 'republic', 'white', 'moved', 'words', 'father', 'term', 'central', 'human', 'young', 'living', 'put', 'line', 'japan', 'international', 'law', 'northwest', 'similar', 'northern', 'say', 'without', 'next', 'australia', 'order', 'red', 'general', 'others', 'age', 'works', 'come', 'lived', 'black', 'species', 'got', 'off', 'empire', 'february', 'should', 'switzerland', 'change', 'member', 'light', 'third', 'away', 'days', 'single', 'york', 'men', 'along', 'list', 'energy', 'changed', 'books', 'video', 'five', 'though', 'less', 'ancient', 'middle', 'must', 'army', 'seen', 'c', 'short', 'areas', 'little', 'film', 'meaning', 'uses', 'kentucky', 'top', 'air', 'japanese', 'km', 'too', 'china', 'killed', 'right', 'son', 'formed', 'groups', 'cities', 'think', 'florida', 'languages', 'earth', 'control', 'european', 'types', 'eastern', 'public', 'side', 'class', 'head', 'wanted', 'free', 'political', 'almost', 'always', 'worked', 'married', 'places', 'islands', 'former', 'instead', 'center', 'done', 'kind', 'women', 'better', 'version', 'overview', 'canton', 'find', 'gave', 'local', 'love', 'never', 'italy', 'playing', 'held', 'valea', 'hard', 'current', 'point', 'plays', 'late', 'players', 'mostly', 'once', 'park', 'département', 'building', 'know', 'shows', 'give', 'movies', 'lost', 'force', 'metal', 'mother', 'someone', 'bad', 'division', 'original', 'opera', 'lake', 'founded', 'especially', 'sun', 'study', 'royal', 'coast', 'illinois', 'considered', 'certain', 'character', 'want', 'sound', 'art', 'taken']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from re import sub\n",
    "\n",
    "# Strip out all punctuation and numbers\n",
    "corpus = sub(r'[^\\w\\s]', '', corpus)\n",
    "corpus = sub('\\n', ' ', corpus)\n",
    "corpus = sub(r'\\d+', '', corpus)\n",
    "corpus = sub('  ', ' ', corpus)\n",
    "\n",
    "words = corpus.lower().split(' ')\n",
    "word_counts = Counter(words)\n",
    "\n",
    "vocab_size = 500\n",
    "most_common_words = word_counts.most_common(vocab_size+1)\n",
    "most_common_words = [word for word, count in most_common_words if word != '']\n",
    "print(most_common_words)"
   ],
   "metadata": {
    "id": "4a2455193456af50",
    "outputId": "44d59159-f246-494b-9bee-ca176a564312",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:35:48.873632057Z",
     "start_time": "2023-11-13T14:35:47.346283106Z"
    }
   },
   "id": "4a2455193456af50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our reduced vocabulary, we will now take our dataset and strip out all words that are not in our vocabulary. Because we are dropping a LOT of words, we are going to keep only segments that are at least 6 words long."
   ],
   "metadata": {
    "collapsed": false,
    "id": "514d75e42b331ef4"
   },
   "id": "514d75e42b331ef4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5411292/5411292 [00:09<00:00, 587574.55it/s]\n"
     ]
    }
   ],
   "source": [
    "context_length = 5\n",
    "new_corpus = []\n",
    "phrase = []\n",
    "for word in tqdm(words):\n",
    "    if word in most_common_words:\n",
    "        phrase.append(word)\n",
    "    elif len(phrase) >= context_length+1:\n",
    "        new_corpus.append(' '.join(phrase))\n",
    "        phrase = []\n",
    "    else:\n",
    "        phrase = []\n",
    "\n",
    "    if len(phrase) >= context_length+1:\n",
    "        new_corpus.append(' '.join(phrase))\n",
    "        phrase = []"
   ],
   "metadata": {
    "id": "bb5a1f3e60dd3b2d",
    "outputId": "fb9c132b-6aa1-4394-e17d-383bcc1666e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:35:58.128555147Z",
     "start_time": "2023-11-13T14:35:48.875547666Z"
    }
   },
   "id": "bb5a1f3e60dd3b2d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "new_corpus = list(set(new_corpus))"
   ],
   "metadata": {
    "id": "d4add13895c71355",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:00.326361721Z",
     "start_time": "2023-11-13T14:36:00.319350839Z"
    }
   },
   "id": "d4add13895c71355"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but this is only part of', 'war illinois illinois is a state', 'women and young children have more', 'of all time by the american', 'west of england it was created', 'good and they will all want', 'it is a very popular place', 'switzerland in in the city of', 'formed only two groups in the', 'a few years later but in']\n"
     ]
    }
   ],
   "source": [
    "print(new_corpus[:10])"
   ],
   "metadata": {
    "id": "54ef06ded893198e",
    "outputId": "7aa811d9-2aff-47c7-9eb5-94e2a43ccf8d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:00.520673781Z",
     "start_time": "2023-11-13T14:36:00.520059081Z"
    }
   },
   "id": "54ef06ded893198e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fantastic! We now have a dataset of grammatical six-word phrases. Next, we need to encode our words into values, so that we can feed them into our model:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c8755a12484db33a"
   },
   "id": "c8755a12484db33a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "words_to_int = {word: i for i, word in enumerate(most_common_words)}\n",
    "int_to_words = {i: word for i, word in enumerate(most_common_words)}"
   ],
   "metadata": {
    "id": "4068aa9e80c52455",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:01.321519293Z",
     "start_time": "2023-11-13T14:36:01.319357342Z"
    }
   },
   "id": "4068aa9e80c52455"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can encode any sentence (as long as it's made up of words in our vocabulary) into a sequence of integers:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c871e261116f2054"
   },
   "id": "c871e261116f2054"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 1, 0, 25]\n",
      "all of the people\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence):\n",
    "    return [words_to_int[word] for word in sentence.split(' ')]\n",
    "\n",
    "def encode_one_hot(word):\n",
    "    return [1 if i == words_to_int[word] else 0 for i in range(vocab_size)]\n",
    "\n",
    "def decode(sequence):\n",
    "    return ' '.join([int_to_words[i] for i in sequence])\n",
    "\n",
    "def decode_one_hot(word):\n",
    "    return int_to_words[np.argmax(word)]\n",
    "\n",
    "encoded = encode('all of the people')\n",
    "print(encoded)\n",
    "print(decode(encoded))"
   ],
   "metadata": {
    "id": "b6e34b9b06fd0b8d",
    "outputId": "d5443e05-95be-4990-bffc-c9198fa6bcd8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:02.420186737Z",
     "start_time": "2023-11-13T14:36:02.414862327Z"
    }
   },
   "id": "b6e34b9b06fd0b8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a way to convert words into integers, we can create our training data. We will use the first 5 words in a sequence to predict the 6th word. For example, given the sequence \"all of the people in the\", we will use \"all of the people in\" to predict \"the\". We will do this for every sequence in our dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "3849592cbcfba197"
   },
   "id": "3849592cbcfba197"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under a rock and is -> not\n",
      "life is that life is -> not\n",
      "with the red sea through -> the\n",
      "of the first people to -> make\n",
      "as well as the history -> of\n",
      "television series from the united -> kingdom\n",
      "of different countries living together -> since\n",
      "country with its own government -> but\n",
      "to all parts of life -> and\n",
      "a person using it can -> change\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence in new_corpus:\n",
    "    words = sentence.split(' ')\n",
    "    for i in range(len(words)-context_length):\n",
    "        X.append(encode(' '.join(words[i:i+context_length])))\n",
    "        y.append(encode_one_hot(words[i+context_length]))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "for i in range(10):\n",
    "    print(decode(X_train[i]), '->', decode_one_hot(y_train[i]))"
   ],
   "metadata": {
    "id": "4b33800e48cab7b5",
    "outputId": "5c3adcb7-5069-4150-cc68-5c9f084d00e2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:08.377799732Z",
     "start_time": "2023-11-13T14:36:03.991717210Z"
    }
   },
   "id": "4b33800e48cab7b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's use Keras to build our model. At its simplest, GPT has the following structure:\n",
    "\n",
    "1. An embedding layer that converts each word into a vector\n",
    "2. A transformer block\n",
    "3. A linear layer that converts the output of the transformer blocks into a vector of probabilities for each word in the vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "_Optional math:_\n",
    "\n",
    "The transformer is a key concept in GPT. At its core, you can think of a single transformer block as an equivalent to a layer of neurons, but with a more complex architecture. The transformer block is made up of two parts:\n",
    "\n",
    "1. Multi-head attention\n",
    "2. A standard fully-connected layer\n",
    "\n",
    "Multi-head attention is a way of combining information from different parts of the input. \"Multi-head\" really just means that we do this multiple times and combine the results. Attention can be thought of as a replacement for a standard neuron - instead of taking in all the inputs and combining them based on a single set of weights, we instead learn three different sets of weights and combine them in a more complex way. So instead of our neuron working like this:\n",
    "\n",
    "$$ y = activation(WX) $$\n",
    "\n",
    "It works like this:\n",
    "\n",
    "$$ y = activation(\\frac{W_1X*W_2X}{\\sqrt{size(X)}}) * W_3X $$\n",
    "\n",
    "If that seems confusing, don't worry - it's a very new concept in deep learning and we aren't explaining it in much detail. The real takeaway is that we are replacing our standard neuron with its one set of parameters, with a more complex neuron that has three sets of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "Each word in our vocabulary will be transformed into a vector of size `8` that will be learned by the model. We will use two transformer blocks, each with two heads. The feedforward layer will have 32 neurons. The output of the feedforward layer will be flattened, and then fed into a linear layer that will output a vector of size `vocab_size`. This vector will be a probability distribution over the words in our vocabulary. We will use the `softmax` activation function to ensure that the output is a valid probability distribution."
   ],
   "metadata": {
    "collapsed": false,
    "id": "4d9337a15522ef33"
   },
   "id": "4d9337a15522ef33"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 5)]                  0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 5, 8)                 4000      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 5, 8)                 568       ['embedding[0][0]',           \n",
      " iHeadAttention)                                                     'embedding[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 5, 8)                 568       ['multi_head_attention[0][0]',\n",
      " ltiHeadAttention)                                                   'multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 5, 32)                288       ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 160)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 500)                  80500     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 85924 (335.64 KB)\n",
      "Trainable params: 85924 (335.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 09:36:10.818218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.818467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.818563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.818813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.818901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.818976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.819093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.819175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 09:36:10.819243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10392 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words_to_int)\n",
    "embedding_size = 8\n",
    "num_heads = 2\n",
    "num_transformer_blocks = 2\n",
    "feedforward_dim = 32\n",
    "\n",
    "inputs = keras.layers.Input(shape=(context_length,)) # Take in three words\n",
    "embedding_layer = keras.layers.Embedding(vocab_size, embedding_size)(inputs) # Convert each word to a vector\n",
    "transformer_block = keras.layers.MultiHeadAttention(num_heads, embedding_size)(embedding_layer, embedding_layer) # Apply multi-head attention, aka transformer block\n",
    "transformer_block = keras.layers.MultiHeadAttention(num_heads, embedding_size)(transformer_block, transformer_block) # Apply multi-head attention again\n",
    "transformer_block = keras.layers.Dense(feedforward_dim, activation='relu')(transformer_block) # Feedforward layer\n",
    "transformer_flattened = keras.layers.Flatten()(transformer_block) # Flatten the output - we currently get 3 vectors because we have 3 words\n",
    "outputs = keras.layers.Dense(vocab_size, activation='softmax')(transformer_flattened) # Output probabilities for each word in the vocabulary\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "9ea317afb44f1e21",
    "outputId": "b67f8119-a3db-42f2-f7be-e83a6d87201c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:36:11.112069658Z",
     "start_time": "2023-11-13T14:36:10.809135457Z"
    }
   },
   "id": "9ea317afb44f1e21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is our own microscopic GPT! Our model has 85,924 trainable parameters - GPT 3.5 has 154 billion. Let's train our model on our dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9b6b32ae8b6a7571"
   },
   "id": "9b6b32ae8b6a7571"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 09:36:13.802251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-11-13 09:36:13.863661: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5573471203a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-13 09:36:13.863686: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-11-13 09:36:13.866859: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-13 09:36:13.879954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800\n",
      "2023-11-13 09:36:13.922365: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-11-13 09:36:13.976238: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 5s 37ms/step - loss: 5.5922 - accuracy: 0.1143 - val_loss: 5.1040 - val_accuracy: 0.1196\n",
      "Epoch 2/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 5.0901 - accuracy: 0.1170 - val_loss: 5.0843 - val_accuracy: 0.1196\n",
      "Epoch 3/1000\n",
      "82/82 [==============================] - 1s 13ms/step - loss: 5.0659 - accuracy: 0.1170 - val_loss: 5.0636 - val_accuracy: 0.1196\n",
      "Epoch 4/1000\n",
      "82/82 [==============================] - 1s 13ms/step - loss: 5.0504 - accuracy: 0.1170 - val_loss: 5.0590 - val_accuracy: 0.1196\n",
      "Epoch 5/1000\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 5.0376 - accuracy: 0.1170 - val_loss: 5.0334 - val_accuracy: 0.1196\n",
      "Epoch 6/1000\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 4.9899 - accuracy: 0.1181 - val_loss: 4.9650 - val_accuracy: 0.1313\n",
      "Epoch 7/1000\n",
      "82/82 [==============================] - 1s 12ms/step - loss: 4.8536 - accuracy: 0.1406 - val_loss: 4.8106 - val_accuracy: 0.1451\n",
      "Epoch 8/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.7533 - accuracy: 0.1464 - val_loss: 4.7655 - val_accuracy: 0.1483\n",
      "Epoch 9/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.7101 - accuracy: 0.1497 - val_loss: 4.7350 - val_accuracy: 0.1496\n",
      "Epoch 10/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.6801 - accuracy: 0.1512 - val_loss: 4.7149 - val_accuracy: 0.1505\n",
      "Epoch 11/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.6527 - accuracy: 0.1509 - val_loss: 4.6894 - val_accuracy: 0.1506\n",
      "Epoch 12/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.6234 - accuracy: 0.1522 - val_loss: 4.6657 - val_accuracy: 0.1520\n",
      "Epoch 13/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.5901 - accuracy: 0.1532 - val_loss: 4.6371 - val_accuracy: 0.1524\n",
      "Epoch 14/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.5606 - accuracy: 0.1543 - val_loss: 4.6167 - val_accuracy: 0.1538\n",
      "Epoch 15/1000\n",
      "82/82 [==============================] - 1s 13ms/step - loss: 4.5333 - accuracy: 0.1556 - val_loss: 4.5937 - val_accuracy: 0.1555\n",
      "Epoch 16/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.5100 - accuracy: 0.1570 - val_loss: 4.5768 - val_accuracy: 0.1563\n",
      "Epoch 17/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.4905 - accuracy: 0.1575 - val_loss: 4.5667 - val_accuracy: 0.1566\n",
      "Epoch 18/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.4723 - accuracy: 0.1591 - val_loss: 4.5516 - val_accuracy: 0.1586\n",
      "Epoch 19/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.4569 - accuracy: 0.1596 - val_loss: 4.5402 - val_accuracy: 0.1591\n",
      "Epoch 20/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.4401 - accuracy: 0.1607 - val_loss: 4.5334 - val_accuracy: 0.1593\n",
      "Epoch 21/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.4279 - accuracy: 0.1621 - val_loss: 4.5227 - val_accuracy: 0.1606\n",
      "Epoch 22/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.4117 - accuracy: 0.1632 - val_loss: 4.5175 - val_accuracy: 0.1621\n",
      "Epoch 23/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.3967 - accuracy: 0.1643 - val_loss: 4.5022 - val_accuracy: 0.1631\n",
      "Epoch 24/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.3812 - accuracy: 0.1657 - val_loss: 4.4947 - val_accuracy: 0.1636\n",
      "Epoch 25/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.3657 - accuracy: 0.1672 - val_loss: 4.4881 - val_accuracy: 0.1640\n",
      "Epoch 26/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.3498 - accuracy: 0.1689 - val_loss: 4.4785 - val_accuracy: 0.1656\n",
      "Epoch 27/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.3335 - accuracy: 0.1701 - val_loss: 4.4717 - val_accuracy: 0.1656\n",
      "Epoch 28/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.3179 - accuracy: 0.1711 - val_loss: 4.4594 - val_accuracy: 0.1665\n",
      "Epoch 29/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.3027 - accuracy: 0.1733 - val_loss: 4.4528 - val_accuracy: 0.1665\n",
      "Epoch 30/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.2872 - accuracy: 0.1744 - val_loss: 4.4459 - val_accuracy: 0.1684\n",
      "Epoch 31/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.2730 - accuracy: 0.1765 - val_loss: 4.4395 - val_accuracy: 0.1691\n",
      "Epoch 32/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.2609 - accuracy: 0.1768 - val_loss: 4.4291 - val_accuracy: 0.1706\n",
      "Epoch 33/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.2454 - accuracy: 0.1783 - val_loss: 4.4236 - val_accuracy: 0.1715\n",
      "Epoch 34/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.2359 - accuracy: 0.1794 - val_loss: 4.4190 - val_accuracy: 0.1720\n",
      "Epoch 35/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.2247 - accuracy: 0.1804 - val_loss: 4.4159 - val_accuracy: 0.1736\n",
      "Epoch 36/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.2121 - accuracy: 0.1817 - val_loss: 4.4110 - val_accuracy: 0.1720\n",
      "Epoch 37/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.2025 - accuracy: 0.1829 - val_loss: 4.4069 - val_accuracy: 0.1746\n",
      "Epoch 38/1000\n",
      "82/82 [==============================] - 1s 12ms/step - loss: 4.1927 - accuracy: 0.1840 - val_loss: 4.4039 - val_accuracy: 0.1745\n",
      "Epoch 39/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.1817 - accuracy: 0.1853 - val_loss: 4.3964 - val_accuracy: 0.1752\n",
      "Epoch 40/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.1729 - accuracy: 0.1853 - val_loss: 4.3974 - val_accuracy: 0.1765\n",
      "Epoch 41/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.1616 - accuracy: 0.1862 - val_loss: 4.3894 - val_accuracy: 0.1767\n",
      "Epoch 42/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.1527 - accuracy: 0.1871 - val_loss: 4.3894 - val_accuracy: 0.1760\n",
      "Epoch 43/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.1437 - accuracy: 0.1872 - val_loss: 4.3851 - val_accuracy: 0.1786\n",
      "Epoch 44/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.1341 - accuracy: 0.1884 - val_loss: 4.3833 - val_accuracy: 0.1779\n",
      "Epoch 45/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.1253 - accuracy: 0.1888 - val_loss: 4.3759 - val_accuracy: 0.1799\n",
      "Epoch 46/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.1152 - accuracy: 0.1902 - val_loss: 4.3764 - val_accuracy: 0.1794\n",
      "Epoch 47/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.1062 - accuracy: 0.1916 - val_loss: 4.3726 - val_accuracy: 0.1805\n",
      "Epoch 48/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.0956 - accuracy: 0.1924 - val_loss: 4.3602 - val_accuracy: 0.1822\n",
      "Epoch 49/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.0852 - accuracy: 0.1937 - val_loss: 4.3581 - val_accuracy: 0.1822\n",
      "Epoch 50/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.0758 - accuracy: 0.1937 - val_loss: 4.3526 - val_accuracy: 0.1825\n",
      "Epoch 51/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.0663 - accuracy: 0.1946 - val_loss: 4.3470 - val_accuracy: 0.1854\n",
      "Epoch 52/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.0568 - accuracy: 0.1957 - val_loss: 4.3469 - val_accuracy: 0.1859\n",
      "Epoch 53/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.0470 - accuracy: 0.1968 - val_loss: 4.3463 - val_accuracy: 0.1866\n",
      "Epoch 54/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.0364 - accuracy: 0.1984 - val_loss: 4.3430 - val_accuracy: 0.1866\n",
      "Epoch 55/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.0272 - accuracy: 0.2000 - val_loss: 4.3353 - val_accuracy: 0.1864\n",
      "Epoch 56/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 4.0182 - accuracy: 0.1997 - val_loss: 4.3344 - val_accuracy: 0.1889\n",
      "Epoch 57/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.0086 - accuracy: 0.2011 - val_loss: 4.3269 - val_accuracy: 0.1887\n",
      "Epoch 58/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 3.9998 - accuracy: 0.2018 - val_loss: 4.3299 - val_accuracy: 0.1892\n",
      "Epoch 59/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9917 - accuracy: 0.2028 - val_loss: 4.3237 - val_accuracy: 0.1894\n",
      "Epoch 60/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9824 - accuracy: 0.2040 - val_loss: 4.3277 - val_accuracy: 0.1909\n",
      "Epoch 61/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9750 - accuracy: 0.2042 - val_loss: 4.3196 - val_accuracy: 0.1908\n",
      "Epoch 62/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9679 - accuracy: 0.2048 - val_loss: 4.3222 - val_accuracy: 0.1926\n",
      "Epoch 63/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9601 - accuracy: 0.2063 - val_loss: 4.3219 - val_accuracy: 0.1923\n",
      "Epoch 64/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9521 - accuracy: 0.2074 - val_loss: 4.3185 - val_accuracy: 0.1940\n",
      "Epoch 65/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9450 - accuracy: 0.2083 - val_loss: 4.3151 - val_accuracy: 0.1944\n",
      "Epoch 66/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9373 - accuracy: 0.2097 - val_loss: 4.3117 - val_accuracy: 0.1934\n",
      "Epoch 67/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9299 - accuracy: 0.2104 - val_loss: 4.3114 - val_accuracy: 0.1926\n",
      "Epoch 68/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9229 - accuracy: 0.2107 - val_loss: 4.3161 - val_accuracy: 0.1959\n",
      "Epoch 69/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9168 - accuracy: 0.2116 - val_loss: 4.3128 - val_accuracy: 0.1957\n",
      "Epoch 70/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9096 - accuracy: 0.2119 - val_loss: 4.3124 - val_accuracy: 0.1966\n",
      "Epoch 71/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9035 - accuracy: 0.2119 - val_loss: 4.3083 - val_accuracy: 0.1968\n",
      "Epoch 72/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 3.8981 - accuracy: 0.2129 - val_loss: 4.3177 - val_accuracy: 0.1974\n",
      "Epoch 73/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8912 - accuracy: 0.2141 - val_loss: 4.3163 - val_accuracy: 0.1961\n",
      "Epoch 74/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8846 - accuracy: 0.2149 - val_loss: 4.3183 - val_accuracy: 0.1962\n",
      "Epoch 75/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.8794 - accuracy: 0.2153 - val_loss: 4.3136 - val_accuracy: 0.1975\n",
      "Epoch 76/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8732 - accuracy: 0.2155 - val_loss: 4.3128 - val_accuracy: 0.1969\n",
      "Epoch 77/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8676 - accuracy: 0.2162 - val_loss: 4.3141 - val_accuracy: 0.1968\n",
      "Epoch 78/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8616 - accuracy: 0.2166 - val_loss: 4.3161 - val_accuracy: 0.1980\n",
      "Epoch 79/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8548 - accuracy: 0.2169 - val_loss: 4.3120 - val_accuracy: 0.1972\n",
      "Epoch 80/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8503 - accuracy: 0.2176 - val_loss: 4.3173 - val_accuracy: 0.1973\n",
      "Epoch 81/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.8438 - accuracy: 0.2178 - val_loss: 4.3147 - val_accuracy: 0.1991\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x7fb1e0156d10>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "fed21afb192e7120",
    "outputId": "7167c912-94d4-4298-8bc7-4850d09724f0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:24.944730066Z",
     "start_time": "2023-11-13T14:36:12.320453238Z"
    }
   },
   "id": "fed21afb192e7120"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, our accuracy is not very good. It picks the right word something like 1 in 5 times. That's still a lot better than random, which would be 1 in 500, but it's not enough to be useful in practice. This is because our model is very small, and so is our dataset. However, the principles we are using here are exactly the same as in GPT - just on a smaller scale. Let's see how our model performs on some sample sentence:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "21ba53e8835e1b32"
   },
   "id": "21ba53e8835e1b32"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 55, 57, 5, 32]\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = 'the united states is one'\n",
    "sample_sentence_encoded = encode(sample_sentence)\n",
    "print(sample_sentence_encoded)\n",
    "predictions = model.predict(np.array([sample_sentence_encoded]))\n",
    "print(decode_one_hot(predictions[0]))"
   ],
   "metadata": {
    "id": "5cf1c9ce7ceb0bdd",
    "outputId": "a8720769-10dd-44e1-9a0f-63d15b45dc01",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:25.108940188Z",
     "start_time": "2023-11-13T14:37:24.944218656Z"
    }
   },
   "id": "5cf1c9ce7ceb0bdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we talk about models like GPT-3.5, we talk about the 'context window'. This is the number of tokens we feed into the model to get one word out. In our case, our context window is 5. In GPT-3.5, the context window is 4096, or 16384, depending on the model. The latest version of GPT-4 supports a context window of up to 128,000 tokens - as much as 300 pages of text. No matter what the context is, we are only getting one word out - if we want to produce a larger sequence, we have to successively feed the output back into the model. This is called 'autoregressive generation'. We can use our model to generate a sequence of words like this:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "e220f26dfb10b515"
   },
   "id": "e220f26dfb10b515"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "in the way of the british empire the first union the united states the storm\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, context, length):\n",
    "    result = context\n",
    "    for i in range(length):\n",
    "        predictions = model.predict(np.array([context]))\n",
    "        context = np.append(context, np.argmax(predictions[0]))\n",
    "        result = np.append(result, np.argmax(predictions[0]))\n",
    "        context = context[1:]\n",
    "    return result\n",
    "\n",
    "print(decode(generate_sequence(model, encode('in the way of the'), 10)))"
   ],
   "metadata": {
    "id": "4c386e6f93a7434b",
    "outputId": "472d6232-2513-4eac-d6e6-e6f6221d45b8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:25.526186536Z",
     "start_time": "2023-11-13T14:37:25.067639878Z"
    }
   },
   "id": "4c386e6f93a7434b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is precisely how models like ChatGPT generate text. They take in the context (which, as we discussed, is typically much longer than 5 words) and generate the next word. However, unlike our case, where we choose a fixed number of words to generate, ChatGPT keeps generating words until it reaches a special token that marks the end of a sequence (often `<end>`). This is how it can generate text of arbitrary length.\n",
    "\n",
    "Now we have created our very own GPT model. But this is not the same as ChatGPT. Models like ChatGPT go one step further, to make the model more useful for conversation. This is done using a technique called Reinforcement Learning from Human Feedback (RLHF).\n",
    "\n",
    "RLHF expands on the training process we've seen above by adding a second model, called the discriminator or the adversary. The role of the adversary is to rate the quality of a response based on some conditions that we care about. In the case of ChatGPT, the adversary is looking for things like whether the response fits the conversational style, and whether it avoids sensitive topics. The adversary is trained using _human feedback_ - humans rate the quality of responses, and the adversary learns to predict the human rating. The adversary is then used to train the generator (the GPT model) - the generator is rewarded for producing responses that the adversary rates highly. This is called adversarial training, and it is a very powerful technique for training models.\n",
    "\n",
    "We are going to make our own extremely simple adversary. Our adversary will assign a score to the response based on how many times the letter 'e' appears in the response. We will then use this score to train our generator. This is a very simple example, but it demonstrates the principle of adversarial training.\n",
    "\n",
    "Because we can directly calculate how many 'e's appear in each of our vocabulary words, we don't need to 'train' our adversary - we can just use it directly. We will use the following function to calculate the adversary score of a word:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "93868f6cfe2e9955"
   },
   "id": "93868f6cfe2e9955"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "adversary_scores = [word.count('e') for word in most_common_words]\n",
    "def adversary_score(y_true, y_pred):\n",
    "    return tf.reduce_sum(y_pred * adversary_scores, axis=-1)"
   ],
   "metadata": {
    "id": "edeae9d23913c798",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:25.526447543Z",
     "start_time": "2023-11-13T14:37:25.518097820Z"
    }
   },
   "id": "edeae9d23913c798"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "the\n",
      "tf.Tensor(0.6908647981617653, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"in the same way i\"\n",
    "sample_sentence_encoded = encode(sample_sentence)\n",
    "prediction = model.predict(np.array([sample_sentence_encoded]))[0]\n",
    "print(decode_one_hot(prediction))\n",
    "print(adversary_score(None, prediction))"
   ],
   "metadata": {
    "id": "ad51ae2e0a7fb1e2",
    "outputId": "448acb96-48cf-4747-96ff-0227aeb31b14",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:25.589727506Z",
     "start_time": "2023-11-13T14:37:25.525479461Z"
    }
   },
   "id": "ad51ae2e0a7fb1e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you should be able to see, the adversary assigns a score greater than zero even if the predicted word doesn't have any 'e's in it. This is because the predicted word is not a one-hot vector - it is a probability distribution. The adversary is assigning a score to the entire distribution, not just the most likely word. This is valuable because we typically avoid methods which can produce zero as an error - in a nutshell, if the error is zero, the model doesn't know how to change things in order to improve. Our approach instead will incentivize the model to consider all words containing 'e's more strongly."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c7c08535fa523049"
   },
   "id": "c7c08535fa523049"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Top 10 most likely words\n",
      "Word       | Chance \t| e count | Adversary score\n",
      "the        | 19% \t| 1       | 0.1918\n",
      "be         | 7% \t| 1       | 0.0723\n",
      "a          | 4% \t| 0       | 0.0000\n",
      "people     | 3% \t| 2       | 0.0625\n",
      "have       | 2% \t| 1       | 0.0240\n",
      "do         | 2% \t| 0       | 0.0000\n",
      "things     | 2% \t| 0       | 0.0000\n",
      "their      | 2% \t| 1       | 0.0186\n",
      "get        | 2% \t| 1       | 0.0182\n",
      "make       | 2% \t| 1       | 0.0167\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"in the same way i\"\n",
    "sample_sentence_encoded = encode(sample_sentence)\n",
    "prediction = model.predict(np.array([sample_sentence_encoded]))[0]\n",
    "print(\"Top 10 most likely words\")\n",
    "print(\"Word       | Chance \\t| e count | Adversary score\")\n",
    "for i in sorted(zip(prediction, most_common_words), reverse=True)[:10]:\n",
    "    print(f'{i[1]:10} | {i[0]*100:.0f}% \\t| {i[1].count(\"e\")}       | {i[0]*i[1].count(\"e\"):.4f}')"
   ],
   "metadata": {
    "id": "e6c725e058889c45",
    "outputId": "f55454a2-ca26-4379-dc69-0d06167c1265",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:25.684277882Z",
     "start_time": "2023-11-13T14:37:25.592486875Z"
    }
   },
   "id": "e6c725e058889c45"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "    adversary_weight = 0.75 # Modify this to increase or decrease the influence of the adversary\n",
    "    return tf.losses.categorical_crossentropy(y_true, y_pred) - adversary_weight * adversary_score(y_true, y_pred)\n",
    "\n",
    "# Duplicate the model\n",
    "model_adversary = keras.models.clone_model(model)\n",
    "\n",
    "model_adversary.compile(optimizer='adam',\n",
    "              loss=combined_loss,\n",
    "              metrics=['accuracy', adversary_score])"
   ],
   "metadata": {
    "id": "38f41c757aab2c61",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:25.754946959Z",
     "start_time": "2023-11-13T14:37:25.684046345Z"
    }
   },
   "id": "38f41c757aab2c61"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "82/82 [==============================] - 5s 39ms/step - loss: 4.9966 - accuracy: 0.0434 - adversary_score: 1.0627 - val_loss: 4.4295 - val_accuracy: 2.3948e-04 - val_adversary_score: 1.6763\n",
      "Epoch 2/1000\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 4.4128 - accuracy: 5.5080e-04 - adversary_score: 1.6666 - val_loss: 4.4095 - val_accuracy: 2.3948e-04 - val_adversary_score: 1.6790\n",
      "Epoch 3/1000\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 4.4007 - accuracy: 7.7830e-04 - adversary_score: 1.6462 - val_loss: 4.4039 - val_accuracy: 0.0011 - val_adversary_score: 1.6714\n",
      "Epoch 4/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.3947 - accuracy: 9.6989e-04 - adversary_score: 1.6226 - val_loss: 4.4009 - val_accuracy: 0.0011 - val_adversary_score: 1.5776\n",
      "Epoch 5/1000\n",
      "82/82 [==============================] - 1s 13ms/step - loss: 4.3924 - accuracy: 9.6989e-04 - adversary_score: 1.6084 - val_loss: 4.4001 - val_accuracy: 0.0011 - val_adversary_score: 1.6005\n",
      "Epoch 6/1000\n",
      "82/82 [==============================] - 1s 13ms/step - loss: 4.3903 - accuracy: 9.6989e-04 - adversary_score: 1.6042 - val_loss: 4.3993 - val_accuracy: 0.0011 - val_adversary_score: 1.5778\n",
      "Epoch 7/1000\n",
      "82/82 [==============================] - 1s 12ms/step - loss: 4.3874 - accuracy: 9.6989e-04 - adversary_score: 1.6056 - val_loss: 4.3937 - val_accuracy: 0.0011 - val_adversary_score: 1.6054\n",
      "Epoch 8/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.3756 - accuracy: 0.0035 - adversary_score: 1.6284 - val_loss: 4.3668 - val_accuracy: 0.0207 - val_adversary_score: 1.6880\n",
      "Epoch 9/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.3074 - accuracy: 0.0396 - adversary_score: 1.6676 - val_loss: 4.2235 - val_accuracy: 0.0149 - val_adversary_score: 1.7003\n",
      "Epoch 10/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.1362 - accuracy: 0.0285 - adversary_score: 1.6728 - val_loss: 4.1248 - val_accuracy: 0.0102 - val_adversary_score: 1.6845\n",
      "Epoch 11/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.0690 - accuracy: 0.0159 - adversary_score: 1.6787 - val_loss: 4.0915 - val_accuracy: 0.0169 - val_adversary_score: 1.6550\n",
      "Epoch 12/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 4.0376 - accuracy: 0.0166 - adversary_score: 1.6758 - val_loss: 4.0673 - val_accuracy: 0.0226 - val_adversary_score: 1.6764\n",
      "Epoch 13/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 4.0128 - accuracy: 0.0264 - adversary_score: 1.6758 - val_loss: 4.0507 - val_accuracy: 0.0298 - val_adversary_score: 1.6903\n",
      "Epoch 14/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9939 - accuracy: 0.0449 - adversary_score: 1.6736 - val_loss: 4.0343 - val_accuracy: 0.0490 - val_adversary_score: 1.6880\n",
      "Epoch 15/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9756 - accuracy: 0.0537 - adversary_score: 1.6694 - val_loss: 4.0212 - val_accuracy: 0.0620 - val_adversary_score: 1.7013\n",
      "Epoch 16/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9582 - accuracy: 0.0569 - adversary_score: 1.6660 - val_loss: 4.0123 - val_accuracy: 0.0683 - val_adversary_score: 1.6228\n",
      "Epoch 17/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 3.9431 - accuracy: 0.0569 - adversary_score: 1.6620 - val_loss: 3.9978 - val_accuracy: 0.0722 - val_adversary_score: 1.6191\n",
      "Epoch 18/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.9274 - accuracy: 0.0558 - adversary_score: 1.6586 - val_loss: 3.9819 - val_accuracy: 0.0532 - val_adversary_score: 1.6603\n",
      "Epoch 19/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.9073 - accuracy: 0.0534 - adversary_score: 1.6584 - val_loss: 3.9583 - val_accuracy: 0.0705 - val_adversary_score: 1.6647\n",
      "Epoch 20/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.8777 - accuracy: 0.0622 - adversary_score: 1.6566 - val_loss: 3.9346 - val_accuracy: 0.0680 - val_adversary_score: 1.6840\n",
      "Epoch 21/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.8527 - accuracy: 0.0631 - adversary_score: 1.6563 - val_loss: 3.9165 - val_accuracy: 0.0727 - val_adversary_score: 1.6421\n",
      "Epoch 22/1000\n",
      "82/82 [==============================] - 1s 12ms/step - loss: 3.8318 - accuracy: 0.0645 - adversary_score: 1.6546 - val_loss: 3.9002 - val_accuracy: 0.0600 - val_adversary_score: 1.6588\n",
      "Epoch 23/1000\n",
      "82/82 [==============================] - 1s 12ms/step - loss: 3.8138 - accuracy: 0.0640 - adversary_score: 1.6592 - val_loss: 3.8871 - val_accuracy: 0.0443 - val_adversary_score: 1.6645\n",
      "Epoch 24/1000\n",
      "82/82 [==============================] - 1s 13ms/step - loss: 3.7976 - accuracy: 0.0609 - adversary_score: 1.6605 - val_loss: 3.8767 - val_accuracy: 0.0649 - val_adversary_score: 1.6487\n",
      "Epoch 25/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 3.7839 - accuracy: 0.0558 - adversary_score: 1.6642 - val_loss: 3.8650 - val_accuracy: 0.0644 - val_adversary_score: 1.6649\n",
      "Epoch 26/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.7690 - accuracy: 0.0560 - adversary_score: 1.6642 - val_loss: 3.8579 - val_accuracy: 0.0549 - val_adversary_score: 1.6863\n",
      "Epoch 27/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.7570 - accuracy: 0.0584 - adversary_score: 1.6639 - val_loss: 3.8481 - val_accuracy: 0.0562 - val_adversary_score: 1.6436\n",
      "Epoch 28/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.7439 - accuracy: 0.0569 - adversary_score: 1.6672 - val_loss: 3.8407 - val_accuracy: 0.0694 - val_adversary_score: 1.6495\n",
      "Epoch 29/1000\n",
      "82/82 [==============================] - 1s 10ms/step - loss: 3.7333 - accuracy: 0.0574 - adversary_score: 1.6647 - val_loss: 3.8287 - val_accuracy: 0.0590 - val_adversary_score: 1.6458\n",
      "Epoch 30/1000\n",
      "82/82 [==============================] - 1s 9ms/step - loss: 3.7205 - accuracy: 0.0580 - adversary_score: 1.6651 - val_loss: 3.8230 - val_accuracy: 0.0585 - val_adversary_score: 1.6436\n",
      "Epoch 31/1000\n",
      "82/82 [==============================] - 1s 11ms/step - loss: 3.7083 - accuracy: 0.0586 - adversary_score: 1.6625 - val_loss: 3.8111 - val_accuracy: 0.0518 - val_adversary_score: 1.6917\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x7fb21778a790>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adversary.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "f11a8845dadb63d1",
    "outputId": "278e8131-f00c-4bc5-9050-9a72ff543c57",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:58.338045958Z",
     "start_time": "2023-11-13T14:37:25.757348219Z"
    }
   },
   "id": "f11a8845dadb63d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compare the predictions of our original model and our adversary model:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "74d84484ed9cb21c"
   },
   "id": "74d84484ed9cb21c"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 1s 1ms/step\n",
      "653/653 [==============================] - 1s 1ms/step\n",
      "Adversary score for original model: 0.5101372502659014\n",
      "Adversary score for adversary model: 1.6916764371761261\n",
      "of         | of         | between   \n",
      "march      | the        | the       \n",
      "the        | the        | the       \n",
      "for        | to         | september \n",
      "this       | the        | the       \n",
      "word       | of         | between   \n",
      "make       | be         | released  \n",
      "and        | the        | the       \n",
      "time       | are        | released  \n",
      "this       | to         | between   \n"
     ]
    }
   ],
   "source": [
    "original_predictions = model.predict(X_test)\n",
    "adversary_predictions = model_adversary.predict(X_test)\n",
    "\n",
    "print(f'Adversary score for original model: {np.mean(adversary_score(None, original_predictions))}')\n",
    "print(f'Adversary score for adversary model: {np.mean(adversary_score(None, adversary_predictions))}')\n",
    "\n",
    "for i in range(10):\n",
    "    true_word = decode_one_hot(y_test[i])\n",
    "    original_word = decode_one_hot(original_predictions[i])\n",
    "    adversary_word = decode_one_hot(adversary_predictions[i])\n",
    "    print(f'{true_word:10} | {original_word:10} | {adversary_word:10}')"
   ],
   "metadata": {
    "id": "378821759b649a5d",
    "outputId": "33cba925-1c85-4890-d8ba-21d1b57b1936",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-11-13T14:38:00.762024374Z",
     "start_time": "2023-11-13T14:37:58.336567532Z"
    }
   },
   "id": "378821759b649a5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, introducing the adversary has dramatically increased the model's likelihood to choose words with lots of 'e's in them. Of course, in this setting, that's at a cost to the model's accuracy. However, in a real-world setting, we would use a more sophisticated adversary, and we would use a more sophisticated metric than just accuracy.\n",
    "\n",
    "So there you have it! We have built our own GPT model, and we have seen how we can use an adversary to obtain specific behaviour. Of course, there are many more details that go into building a model like ChatGPT, but this is the core of it. I hope you enjoyed this tutorial!"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6584ddbdaa8b325f"
   },
   "id": "6584ddbdaa8b325f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
